[前のissueへのリンク](https://github.com/kyotovision/individual-meeting/issues/2529)

# 教師なしでの絶対深度推定

## 骨子

1. $\boxed{\large \quad A \quad }$ is important.
   - Self-supervised monocular absolute depth estimation is important task for ADAS, autonomous driving, and robots.

2. Past works have done $\boxed{\large \quad B \quad }$ for $\boxed{\large \quad A \quad }$. Their method, however, does not really solve $\boxed{\large \quad A \quad }$ because ...  That is, the key of $\boxed{\large \quad A \quad }$ is $\boxed{\large \quad A_1 \quad }$ and $\boxed{\large \quad A_2 \quad }$.
   - 先行研究では，カメラ高さや速度を教師として学習することで，スケールを理解した単眼深度推定を実現している．しかし，以下のデメリットを持つ
      1. 学習データの作成時にカメラ以外のセンサーが必要
      2. 車内にカメラを簡易に設置し撮影したBDD100Kデータセットのように，カメラ高さの測定が困難な条件下で撮影された動画や，自然動画を学習データとして使うことができない
   - 鍵はあらゆるGTデータを使わず教師なし学習を実現させること

3. $\boxed{\large \quad A_1 \quad}$ and $\boxed{\large \quad A_2 \quad }$ remain challenging because $\boxed{\large \quad C_1 \quad }$, $\boxed{\large \quad C_2 \quad }$, $\dots$
    - 単眼カメラで撮影した画像のみだけではスケールがわからない．
        - 道路環境下では，大きさが統一的なものなどのスケールを決定付ける指標が存在しない．

4. We solve $\boxed{\large \quad C_1 \quad }$, $\boxed{\large \quad C_2 \quad }$, $\dots$ by $\boxed{\large \quad D_1 \quad }$, $\boxed{\large \quad D_2 \quad }$, $\dots$ The key idea is $\boxed{\large \quad E \quad }$.
    - 人間同様に，物体の高さに対する事前知識からスケールを理解する

5. For each $\boxed{\large \quad C_i \quad }$, we do $\boxed{\large \quad D_i \quad }$.
    - **教師なし学習モデルの推定結果を，物体高さの事前知識を用いてスケーリングし，pseudo depth labels を作る**
        - 物体の高さを $H$，物体のピクセル高さを $h$ としたとき，深度 $D$ は $\frac{f_y\cdot H}{h}$ と表せる．スケールファクタは，推定した物体の最近傍点の深度 $\hat{D}$ を用いて $\frac{D}{\hat{D}}$ と表せる．
            - 最近傍点の深度を用いたのは，画像平面にreprojectした3D BBoxの鉛直線のうち，光学中心に対する最近傍の線（物体上面が見えていない時，2D BBoxの高さに相当）おける深度と近いため
            
            <img src="https://user-images.githubusercontent.com/54442538/230831334-9f4e09d5-c816-4a7a-a8bb-02008f2db424.png" width=300px>

            - 物体領域は，学習済みのInstanceSegmentationモデルで推定

        - **道路平面に対するカメラ高さが一定であると仮定し，この尤度を最大化するように，各フレームのスケールファクタを最適化**
            - 深度の推定結果・深度-高さの幾何的な関係式は完璧ではない．物体が登場しないフレームも存在→各フレームで独立にスケールファクタを決定すべきではない
            - 深度推定結果におけるフレーム間のスケールは，基本的にバラバラ→フレーム間でほぼ統一の取れたカメラ高さを利用
            - カメラ高さを $H_c$ , フレーム $1\sim F$ のスケールファクタを $s_1\sim s_F$ とすると， $$p(H_c|s_1, \cdots, s_F)\propto p(H_c|s_1)p(H_c|s_2)\cdots p(H_c|s_F)$$ と表せる．また $p(H_c|s_f)$ はフレーム中に登場する各インスタンスの幾何的関係式から計算可能
                - semantic segmentationにより道路領域を特定し，その領域の深度推定結果から道路平面の方程式を計算．これによりカメラ高さが計算可能．
                - カテゴリーごとの物体高さは平均値，分散を既知とする（3D BBoxから計算）

        - **cross-ratioを用いた幾何的関係が成立しない物体の検出**
        
            画像外に見切れた物体，遮蔽を含む物体，クラス分類・Segmentation領域を誤った物体（以下，異常な物体）は，深度から高さへの幾何的な関係式が成立しない．そのような物体はスケールファクタの計算時に利用すべきでない．
                - 特に画像の左右下で見切れた物体は距離も近いため，最尤推定の際にかなり邪魔になる．
            <img src="https://user-images.githubusercontent.com/54442538/215378355-9a775e97-4337-400b-a0de-0a4bfcc76ca8.png" width=400px>
            1. 道路平面の法線ベクトルからhorizonを計算．
            2. 2つで1組の物体ペアについて，cross ratioの式に基づき，片方の物体高さ（カテゴリーの高さ平均値で代用）が正しいと仮定して，もう片方の物体の高さを計算．
            3. 計算した高さが，その物体のカテゴリー高さの平均値との乖離が大きければ，物体ペアのいずれか，または両方が異常な物体と考えられる．
            4. 複数フレームに渡って物体ペアを作成し，「異常な物体」と見なされた回数が多い物体は「異常な物体」と断定して，スケールファクタ計算から除外する．
                - 異なるフレーム間でペアを作る場合には，片方の道路法線ベクトルをもう片方の法線ベクトルに合うように回転してから，cross-ratioの式でを計算する

## これから考えていきたいこと
- 深度-高さの幾何的な関係式を計算する際に，道路平面に対するピッチ・ロールを考慮して補正
- 教師作成後の学習時にできる工夫があるか
    - self-distillation周りの論文を読んでいる
    - [Exploiting pseudo labels in a self-supervised learning framework for improved monocular depth estimation](https://openaccess.thecvf.com/content/CVPR2022/papers/Petrovai_Exploiting_Pseudo_Labels_in_a_Self-Supervised_Learning_Framework_for_Improved_CVPR_2022_paper.pdf) (CVPR2022)
    - [Adaptive co-teaching for unsupervised monocular depth estimation](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610089.pdf) (ECCV2022)
    - [Self-supervised deep monocular depth estimation with ambiguity boosting](https://ieeexplore.ieee.org/document/9599401) (TPAMI2022)
- 上記のやり方では，道路平面の法線ベクトルを正確に出すことが非常に鍵となっている．monodepth2の推定結果を見る限り，道路は平らな平面にはなっているが，ピッチ・ロールはあまり考えられていない（特にロール）
    - 最初の学習時，ターゲット画像の深度推定と並行して，ターゲット画像をロールさせたものを入力として深度推定．これらオリジナルの画像の深度推定結果をロールさせたものと，画像をロールしてから入力したものの深度が一致するように制約を加えるとか

| O | x | x |
| - | - | - |
| ![image](https://user-images.githubusercontent.com/54442538/230826278-dc0c3e4b-93de-455a-a3b5-ddf21e591df4.png) | ![image](https://user-images.githubusercontent.com/54442538/230826362-eb2d1613-72e6-4b8b-80e5-96ab925bbd2e.png) | ![image](https://user-images.githubusercontent.com/54442538/230826571-64ab1125-4ad4-4281-9792-5584b008697b.png) |

関連研究
- [Probabilistic global scale estimation for MonoSLAM based on generic object detection](https://openaccess.thecvf.com/content_cvpr_2017_workshops/w10/papers/Sucar_Probabilistic_Global_Scale_CVPR_2017_paper.pdf) (CVPR2017 workshop)
    - SLAMで物体高さの事前知識とObject Detectionを利用して実スケール化．細長い物体に限られる．

取り組んでいること
<!-- - ２ステージでの学習 -->
  <!-- 1. 自己教師で学習 -->
  <!-- 2. 上記モデルの推定結果を，スケーリングさせて教師として推定する -->
<!-- - スケーリングの手法 -->
  <!-- - Instance Segmentationした物体領域の深度の最近傍点をその物体の深度と -->
    <!-- - image planeに射影した3d bboxの鉛直線のうち光学中心に対して最近傍のものを見つけたいイメージ -->
  <!-- - 幾何的な関係式は完璧ではなく（以下に具体的に例示），教師なしモデルの推定結果も完璧ではないので，各フレームを独立にスケールファクタを考えるは不適．しかし，フレームごとにスケール係数がバラバラ．→地面からのカメラ高さが一定であると仮定し，カメラ高さの最尤推定を行う． -->
<!-- - 道路平面の法線に垂直になるようにカメラを補正してから深度を計算 -->
<!-- - スケールを阻害する物体(見切れ・オクルージョン)の検知 -->


<!-- これからやりたいこと -->
<!-- - GT depthと推定結果を比較してどういうところに問題があるのかちゃんと見る -->
<!-- - 回転によるオーグメンテーション -->

-----------------------------------------------------------
#   04-10
-----------------------------------------------------------

[link to 2023-04-10 individual-meeting](https://github.com/kyotovision/individual-meeting/issues/2570)

# 教師なしでの絶対深度推定
先週のミーティングで話した骨子

1. $\boxed{\large \quad A \quad }$ is important.
   - Self-supervised monocular **absolute** depth estimation is important task for ADAS, autonomous driving, and robots.

2. Past works have done $\boxed{\large \quad B \quad }$ for $\boxed{\large \quad A \quad }$. Their method, however, does not really solve $\boxed{\large \quad A \quad }$ because ...  That is, the key of $\boxed{\large \quad A \quad }$ is $\boxed{\large \quad A_1 \quad }$ and $\boxed{\large \quad A_2 \quad }$.
   - 先行研究では，カメラ高さや速度を教師として学習することで，スケールを理解した単眼深度推定を実現している．しかし，以下のデメリットを持つ
      1. 学習データの作成時にカメラ以外のセンサーが必要
      2. 車内にカメラを簡易に設置し撮影したBDD100Kデータセットのように，カメラ高さの測定が困難な条件下で撮影された動画や，自然動画を学習データとして使うことができない
   - 鍵はあらゆるGTデータを使わず教師なし学習を実現させること

3. $\boxed{\large \quad A_1 \quad}$ and $\boxed{\large \quad A_2 \quad }$ remain challenging because $\boxed{\large \quad C_1 \quad }$, $\boxed{\large \quad C_2 \quad }$, $\dots$
    - 単眼カメラで撮影した画像のみだけではスケールがわからない．
        - 道路環境下では，大きさが統一的なものなどのスケールを決定付ける指標が存在しない．

4. We solve $\boxed{\large \quad C_1 \quad }$, $\boxed{\large \quad C_2 \quad }$, $\dots$ by $\boxed{\large \quad D_1 \quad }$, $\boxed{\large \quad D_2 \quad }$, $\dots$ The key idea is $\boxed{\large \quad E \quad }$.
    - 人間同様に，物体の高さに対する事前知識からスケールを理解する

5. For each $\boxed{\large \quad C_i \quad }$, we do $\boxed{\large \quad D_i \quad }$.
    - **教師なし学習モデルの推定結果を，物体高さの事前知識を用いてスケーリングし，pseudo depth labels を作る**
        - 物体の高さを $H$，物体のピクセル高さを $h$ としたとき，深度 $D$ は $\frac{f_y\cdot H}{h}$ と表せる．スケールファクタは，推定した物体の最近傍点の深度 $\hat{D}$ を用いて $\frac{D}{\hat{D}}$ と表せる．
            - 最近傍点の深度を用いたのは，画像平面にreprojectした3D BBoxの鉛直線のうち，カメラ座標の原点に対する最近傍の線（物体上面が見えていない時，2D BBoxの高さに相当）おける深度と近いため
            
            <img src="https://user-images.githubusercontent.com/54442538/230831334-9f4e09d5-c816-4a7a-a8bb-02008f2db424.png" width=300px>

            - 物体領域は，学習済みのInstanceSegmentationモデルで推定

        - **道路平面に対するカメラ高さが一定であると仮定．この尤度を最大化するように，各フレームの深度スケールファクタを計算＆スケール一貫性担保**
            - 深度推定結果・深度-高さの幾何的な関係式は完璧ではない．物体が登場しないフレームも存在→各フレームで独立にスケールファクタを決定すべきではない
            - 深度推定結果におけるフレーム間のスケールは，基本的にバラバラ→フレーム間でほぼ統一の取れたカメラ高さを利用
            - カメラ高さを $H_c$ , フレーム $1\sim F$ のスケールファクタを $s_1\sim s_F$ とすると， $$p(H_c|s_1, \cdots, s_F)\propto p(H_c|s_1)p(H_c|s_2)\cdots p(H_c|s_F)$$ と表せる．また $p(H_c|s_f)$ はフレーム中に登場する各インスタンスの幾何的関係式から計算可能
                - semantic segmentationにより道路領域を特定し，その領域の深度推定結果から道路平面の方程式を計算．これによりカメラ高さが計算可能．
                - カテゴリーごとの物体高さは平均値，分散を既知とする（3D BBoxから計算）

        - **cross-ratioを用いた幾何的関係が成立しない物体の検出**
        
            画像外に見切れた物体，遮蔽を含む物体，クラス分類・Segmentation領域を誤った物体（以下，異常な物体）は，深度から高さへの幾何的な関係式が成立しない．そのような物体はスケールファクタの計算時に利用すべきでない．
                - 特に画像の左右下で見切れた物体は距離も近いため，最尤推定の際にかなり邪魔になる．
            <img src="https://user-images.githubusercontent.com/54442538/215378355-9a775e97-4337-400b-a0de-0a4bfcc76ca8.png" width=400px>
            1. 道路平面の法線ベクトルからhorizonを計算．
            2. 2つで1組の物体ペアについて，cross ratioの式に基づき，片方の物体高さ（カテゴリーの高さ平均値で代用）が正しいと仮定して，もう片方の物体の高さを計算．
            3. 計算した高さが，その物体のカテゴリー高さの平均値との乖離が大きければ，物体ペアのいずれか，または両方が異常な物体と考えられる．
            4. 複数フレームに渡って物体ペアを作成し，「異常な物体」と見なされた回数が多い物体は「異常な物体」と断定して，スケールファクタ計算から除外する．
                - 異なるフレーム間でペアを作る場合には，片方の道路法線ベクトルをもう片方の法線ベクトルに合うように回転してから，cross-ratioの式でを計算する


# O-1: 手法の改善提案
特に「学習済みモデルの推定結果でpseudo depth labelを作る」部分のインパクトが弱いので，もう少しうまい方法を考える

## KR-1: 来週までに改善手法を考える

### 経過

以下の欠点を補う手法を考える
- 物体領域の深度推定結果の信頼性は低い．
    - monodepth2の推定結果をPointCloud化して見た限り，特に遠くの物体領域についての物体形状は不正確．そのうちの１点を取り出し，それを元にスケールを決定するのはロバスト性に欠ける．
    <video src="https://user-images.githubusercontent.com/54442538/232192628-8745023d-55b2-4efe-a3ee-6f53a846f6a3.mp4" width="400"></video>
- １ステージ目の教師なし学習時の工夫が無く，ハードな教師を作っているだけ．深層学習の柔軟性を活かせていない．
    - ２ステージ目のpseudo labelを使った学習がうまく行くかどうかは，pseudo labelの正確性（特にスケール）にひどく依存してしまう．

### 結果

#### 提案:  (i)物体領域の平均深度と高さの幾何的な関係式をLoss，(ii)オンラインで作成するカメラ高さに対する教師をとして採用し，１ステージで学習．

**(i):  物体領域の平均深度と高さの幾何的な関係式をLossに**
- 詳細
    カテゴリーごとの物体高さが，期待値 $\overline{H}$, 分散 $\sigma^2$ のガウス分布 $\mathcal{N}$ に従うと仮定．物体の2D BBoxのピクセル高さを $h$, y軸方向の焦点距離を $f_y$, 物体深度を $D$ としたとき，物体の高さ $H$ は $\frac{f_y\cdot H}{h}$ と表せる．$\mathcal{N}$ に従う $H$ の負の対数尤度を損失として設定．物体深度 $D$ は，物体領域の平均深度で代表．
- 目的
    - 物体領域の深度が，ある程度の領域に収まるようになり，結果としてPoint Cloud化した際の物体形状も向上
        - 凹んだり尖ったりさせない
    - 学習初期だと推定結果が無茶苦茶であり，(ii)で作成するこれを元にしたpseudo labelsもかなり不正確．(i)によりラフなスケールをモデルに学習させていき，スケール学習の安定化につなげる．

**(ii):  カメラ高さに対する教師をオンラインに作成** 
- 詳細
    １エポックごとにメトリックなカメラ高さに対する教師を作成していき，次エポックの学習に使用．教師作成手順は以下の通り．
    1. 推定した各フレームのDepth mapを，高さ-深度の変換式を用いてスケール
        - 高さ-深度の変換式を計算する際，深度として物体領域内のカメラ座標の原点の**最近傍点**を採用
        <img src="https://user-images.githubusercontent.com/54442538/230831334-9f4e09d5-c816-4a7a-a8bb-02008f2db424.png" width=200px>

    2. スケールしたdepth mapの道路セグメントから道路平面の方程式を計算（＝カメラ高さを計算）
        - 道路セグメントは学習済みモデルで事前に推定

    3. 「シーン内でカメラ高さは不変」という仮定の下，計算した各フレームのカメラ高さから最尤推定し，カメラ高さを決定．

    4. 次エポックの学習時，各フレームで推定したDepth mapから同様にカメラ高さを計算し，これが前エポックで最尤推定したカメラ高さにどれだけ近いかをlossにする
- 目的
    - (i)よりもロバスト性に欠けるが，より正確な教師
        - ３次元上で「物体の高さ」と定義される部分の深度を用いてスケールを決定
    - 物体が登場しないフレームでもスケールに関する教師を与えれる

**学習の初期は(i)の損失に重みを置き，徐々に(ii)の損失の重みを大きくしていく．**

#### その他の工夫
**遮蔽や画像外への見切れ・クラス誤分類された物体（異常物体）についての深度-高さ変換式は不正確．これらの物体を特定し，(i),(ii)で使用しないようにする．**

手順は以下の通り
1. １組の物体ペアについて，cross ratioの式に基づき消失点を計算
    - 両物体の高さは，各所属カテゴリーの平均高さを使用
    - 物体の高さを定義する部分として，2D BBoxの中間を使用（＝道路平面に対するrollを無視）
        <img src="https://user-images.githubusercontent.com/54442538/232188731-a17dc92c-525f-4e8d-9cb4-64ab40ac287b.png" width=400px>

2. 複数ペアから計算した消失点を通る直線をRANSACにより求める
    - 物体ペアは複数フレームに渡って作成し，異なるフレーム内に登場する同一物体は別の物体とみなす
        <img src="https://user-images.githubusercontent.com/54442538/232190129-1f2379b4-0457-477d-a568-98079df925d0.png" width=600px>

3. RANSACにより外れ値とみなされた消失点を生成した物体ペアのうちのいずれか，または両方は異常物体であると考えられる．あるフレームのある舞台について，消失点の計算に使用された回数のうち，そのペアがRANSACにより外れ値と判断された割合が閾値を超えた場合，それを異常な物体として決定する．

| | |
|-|-|
| <video src="https://user-images.githubusercontent.com/54442538/212581783-48927261-0165-426e-a0d0-8a1367eb6331.mp4"></video> | <video src="https://user-images.githubusercontent.com/54442538/212582053-dcedc9b9-ec3e-4493-a123-acd172307bd5.mp4"></video> |
- 道路平面に対するrollを無視している影響が大きい可能性がある．

# O-2: 就職先の決定

## KR-2: 来週中には就職先を決断する

### 経過・結果
全内定先企業との面談をした．リクルートの機械学習エンジニア職とソニーの車載事業部（early sensor fusionの研究開発）で迷い中．




-----------------------------------------------------------------------------------------
# 04-10-modified
-----------------------------------------------------------------------------------------
# 教師なしでの絶対深度推定

1. $\boxed{\large \quad A \quad }$ is important.
   - $A$: Self-supervised monocular **absolute** depth estimation is important task for ADAS, autonomous driving, and robots.

2. Past works have done $\boxed{\large \quad B \quad }$ for $\boxed{\large \quad A \quad }$. Their method, however, does not really solve $\boxed{\large \quad A \quad }$
    - $B$: DepthのGT，カメラ高さ，速度などを教師として学習
    - because ...
      1. 学習データの作成時にカメラ以外のセンサーが必要
      2. 車内にカメラを簡易に設置し撮影したBDD100Kデータセットのように，カメラ高さの測定が困難な条件下で撮影された動画や，自然動画を学習データとして使うことができない

    That is, the key of $\boxed{\large \quad A \quad }$ is $\boxed{\large \quad A' \quad }$.
    <!-- - $A'$: 物体高さの事前知識を用いて，高さ-深度の幾何的制約から推定する深度マップを実スケール化 -->
    - $A'$: 物体高さの事前知識を用いて，高さ-深度の幾何的制約からスケールを理解させること
    - (or 人間同様に，物体の高さに対する事前知識からスケールを理解する)
        - 物体の高さ $H$, 物体の2D BBoxのピクセル高さ $h$, 物体の深度 $D$, y軸方向の焦点距離 $f_y$ の間に成立する以下の関係式を利用
        $$H=\frac{D \cdot h}{f_y}$$
        - 厳密には，物体上面が見えていない（光学中心の高さが物体上部よりも低い）・道路平面に対するカメラのロールが0を仮定した時に成立

3. $\boxed{\large \quad A' \quad}$ remains challenging because $\boxed{\large \quad C_1 \quad }$, $\boxed{\large \quad C_2 \quad }$, $\dots$
    - $C_1$: 高さ-深度の幾何的制約の計算時に，代表すべき深度の値を決定するのが困難
        <details>
            <summary>深度の代表点を見つける難しさ</summary>

        <img src="https://user-images.githubusercontent.com/54442538/233389995-bceb34c7-e98d-4db9-af8a-a0a5dd7afebb.png" width=60%>
        </details>

    - $C_2$: 仮に代表する深度を決定でき，それを元に高さ-深度の幾何制約から深度の教師を作れても，それは１ピクセルに対する教師でしかなく，画像全体の深度スケールを学習するのは困難．

    - $C_3$: 車や歩行者などの物体が登場しないフレームではスケールに対する教師が作れない

4. We solve $\boxed{\large \quad C_1 \quad }$, $\boxed{\large \quad C_2 \quad }$, $\dots$ by $\boxed{\large \quad D_1 \quad }$, $\boxed{\large \quad D_2 \quad }$, $\dots$ The key idea is $\boxed{\large \quad E \quad }$.
    - $E_1$: **カメラ高さが不変であること**を利用してシーン全体に渡るスケールの教師を作成
    - $E_2$: cross-ratioによる画像外に見切れた物体，遮蔽を含む物体，クラス分類・Segmentation領域を誤った物体の検出

5. For each $\boxed{\large \quad C_i \quad }$, we do $\boxed{\large \quad D_i \quad }$.
    - $D_1$($C_1, C_2, C_3$ に対する策): 物体高さの事前知識とカメラ高さの不変性を利用し，カメラ高さを確率的に計算．カメラ高さは毎エポックごとに計算し，次エポック学習時のスケールに対する教師として使用．

        - **手続き**
            1. 学習時の毎エポックで推論する深度マップを保存

            2. 1エポック終了後，推定した深度マップを用いて，幾何制約式から1フレーム中の各物体ごとにスケールファクタを計算
                - 画像外への見切れや遮蔽がない物体であれば，幾何制約式で代表すべき深度は，基本的にカメラ座標の原点に最近傍の点の深度．逆に言うと，深度の値を代表すべき点を決定するのを難しくしているのはこういった物体．
                - 上記のような物体は $D_3$ で除外

            3. 各フレームごとに，そのフレームに現れた物体数分だけスケールファクタが計算される．これらをもとにスケールファクタ(が従うガウス分布のパラメータ)を計算

            4. 事前に推論していた道路セグメントと c.で計算したスケールファクタから，道路平面の方程式を計算．これにより，今注目しているフレームのカメラ高さが得られる．

            5. 全フレームについて計算したカメラ高さから，カメラ高さが一定であると仮定し，最終的なカメラ高さを計算する

            6. 計算したカメラ高さを，次エポック学習時の教師とする（道路平面の方程式を計算し，前エポックで学習したカメラ高さとどれだけ乖離しているかを損失とする）

        - <details><summary>数式</summary>

            - フレーム $f$ 中の物体 $i$ (カテゴリーは $c(i)$, 物体数は $n^f$, フレーム数は $F$)
            -  物体の高さはガウス分布に従うと仮定．
            - 物体の高さ: $H^f_i\sim \mathcal{N}(\mu_{c(i)}, \sigma^2_{c(i)})$
            - 物体の代表深度（推定値）: $\tilde{D}^f_i$
            - 物体の代表深度（理想）: $D^f_i=\frac{f_y \cdot H^f_i}{h^f_i} \sim \mathcal{N}(\frac{f_y \cdot H^f_i}{h^f_i}\mu_{c(i)}, \frac{f_y^2 \cdot {H^f_i}^2}{{h^f_i}^2}\sigma^2_{c(i)})$
            - １物体から計算したスケールファクター: $s^f_i=\frac{D^f_i}{\tilde{D}^f_i} \sim \mathcal{N}(\frac{f_y \cdot H^f_i}{h^f_i \tilde{D}^f_i}\mu_{c(i)}, \frac{f_y^2 \cdot {H^f_i}^2}{{h^f_i}^2 {\tilde{D}^f_i}^2}\sigma^2_{c(i)})$
            - →スケールファクターはガウス分布に従う．フレーム $f$ のスケールファクター $S^f$ を $s^f_i$ の平均で近似する
            - スケールファクター: $S^f = \frac{1}{n^f}\sum_i s^f_i \sim \mathcal{N}(\frac{1}{n^f} \sum_i\frac{f_y \cdot H^f_i}{h^f_i \tilde{D}^f_i}\mu_{c(i)}, \frac{1}{{n^f}^2} \sum_i\frac{f_y^2 \cdot {H^f_i}^2}{{h^f_i}^2 {\tilde{D}^f_i}^2}\sigma^2_{c(i)})$
            - フレーム $f$ のカメラ高さ(w/o scaling): $\tilde{H}^f_c$
            - フレーム $f$ のカメラ高さ(w/ scaling): $H^f_c = S^f\tilde{H}^f_c \sim \mathcal{N}(\frac{\tilde{H}^f_c}{n^f} \sum_i\frac{f_y \cdot H^f_i}{h^f_i \tilde{D}^f_i}\mu_{c(i)}, \frac{{\tilde{H}^f_c}^2}{{n^f}^2} \sum_i\frac{f_y^2 \cdot {H^f_i}^2}{{h^f_i}^2 {\tilde{D}^f_i}^2}\sigma^2_{c(i)})$
            -  →カメラ高さはガウス分布に従う．全フレームで共通するカメラ高さ $H_c$ を $H^f_c$ の平均で近似する．
            - カメラ高さ: $H_c = \frac{1}{F}\sum_f H^f_c \sim \mathcal{N}(\frac{1}{F}\sum_f \frac{\tilde{H}^f_c}{n^f} \sum_i\frac{f_y \cdot H^f_i}{h^f_i \tilde{D}^f_i}\mu_{c(i)}, \frac{1}{F^2}\sum_f\frac{{\tilde{H}^f_c}^2}{{n^f}^2} \sum_i\frac{f_y^2 \cdot {H^f_i}^2}{{h^f_i}^2 {\tilde{D}^f_i}^2}\sigma^2_{c(i)})$
            - この期待値をシーン全体のカメラ高さとして採用する．現時点では分散の情報は何も扱えておらず，ただ平均し続けているだけだが，損失に掛ける係数として使えるかもしれない（教師の信頼度的な要素）．
           </details>

    - $D_2$($C_2$ に対する策と $D_1$ の補助): 物体領域の深度を高さ-深度の幾何的制約における深度の代表値として使用し，この制約を損失として学習
        - **詳細**
            - カテゴリーごとの物体高さが，期待値 $\overline{H}$, 分散 $\sigma^2$ のガウス分布 $\mathcal{N}$ に従うと仮定．物体の高さ $H$ は $\frac{f_y\cdot D}{h}$ と表せる． $\mathcal{N}$ に従う $H$ の負の対数尤度を損失として設定．
            - このとき深度 $D$ として，物体領域の深度を使用
            - **学習の初期は $D_2$ の損失にかける係数を大きくし，徐々に $D_1$ の損失にかける係数を大きくしていく**

        - **目的**
            - monodepth2の推定結果をPointCloud化して見た限り，特に遠くの物体領域についての物体形状は不正確．そのうちの１点を取り出し，それを元にスケールを決定すると $D_1$ の教師の不正確さにつながる．
            <video src="https://user-images.githubusercontent.com/54442538/232192628-8745023d-55b2-4efe-a3ee-6f53a846f6a3.mp4" width="400"></video>
                - こうなってしまう直接的な原因は，おそらくlow texture regionでは深度推定があいまいでも損失が小さくなってしまうことにある
                - 物体領域の深度が，空間的にある程度の範囲に収まり（＝尖ったり道路平面とほぼ平行になったりしない）, $D_1$ の教師の正確性を上げる
                - 副次的に，物体領域を理解した推定が可能となる

    - $D_3$($C_1$ に対する策): cross-ratioを用いた異常物体の除去
        - ここで「**異常物体**」とは，「画像外に見切れた物体，遮蔽を含む物体，クラス分類・Segmentation領域を誤った物体」を指す．
        - 学習前に1度だけ行う，$D_1, D_2$ で高さ-深度の幾何的制約を計算する際に使用すべきでない物体の特定法．

        - **手続き**
            1. １組の物体ペアについて，cross ratioの式に基づき消失点を計算
                - 両物体の高さは，各所属カテゴリーの平均高さを使用
                - 物体の高さを定義する部分として，2D BBoxの中間を使用（＝道路平面に対するrollを無視）
                    <img src="https://user-images.githubusercontent.com/54442538/232188731-a17dc92c-525f-4e8d-9cb4-64ab40ac287b.png" width=400px>

            2. 複数ペアから計算した消失点を通る直線をRANSACにより求める
                - 物体ペアは複数フレームに渡って作成し，異なるフレーム内に登場する同一物体は別の物体とみなす
                    <img src="https://user-images.githubusercontent.com/54442538/232190129-1f2379b4-0457-477d-a568-98079df925d0.png" width=600px>

            3. RANSACにより外れ値とみなされた消失点を生成した物体ペアのうちのいずれか，または両方は異常物体であると考えられる．あるフレームのある舞台について，消失点の計算に使用された回数のうち，そのペアがRANSACにより外れ値と判断された割合が閾値を超えた場合，それを異常な物体として決定する．

# メモ
- **？：2ステージでやることに比べたメリット**
    - 2度学習しなくて良い
    - 1度学習しただけだと，教師としてハードなものになる
        - 「オンラインで教師を作ることで，学習の最終段階では，事前に作成するハードな教師よりも**教師の精度が良くなるように学習が進む**」ことを言えたら良い
        - 「教師の精度も良くなるように学習が進む」の部分については，深度平均化が担う
- **？：そもそもカメラ高さの一定性と物体高さを推定することの新規性が薄い？**
    - スケール合わせするとき，なぜSLAMじゃない？
        - SLAMを使う場合，全フレームでのスケール合わせ→各物体の深度1点からスケール係数を決定→これを全フレームで平均 or 最尤推定→SLAMの問題はフレームをすっ飛ばしてしまうことやけど，この方法で（とばしたフレーム以外で）カメラ高さを計算できるから問題にはならない．
    - 一応考えている新規性は **「物体高さの事前知識とカメラ高さが不変であることを利用し，最尤推定により全フレームについてスケール教師を作成」** と **「cross-ratioによる異常物体の除外」**



# メモ stash
ざっくりまとめると，
- 正確な深度を決定するのが難しい．仮に深度を決定できても，それは１ピクセルの教師しかなく，制約として非常に弱い（これだけではスケール情報が画像全体に渡るように学習することは困難）．
- **？：なぜ正確な深度を決定するのが難しい?**
    - TODO
- **？：この問題はきちんと提案手法で解決できるものなのか? (提案する手法によって解けている問題をCに書く)**
    - $C_1$: 「正確な深度」＝ 各インスタンスから単独でスケールを決定せず，全フレームを考慮した最尤推定により計算したカメラ高さを，ロバストなスケール教師として代用
    - $C_2$: 「点のみの教師」＝ 深度平均化＋カメラ高さの教師
    - $C_3$: 「物体が現れないフレームはスケール教師がない」＝ カメラ高さによるスケーリング係数の決定
        - こう考えると，**鍵は「物体高さの事前知識を用いて高さ-深度の幾何的制約から推定する深度マップを実スケール化」ではない気がする（どっちかというと重要なのはカメラ高さの一貫性を使用したスケーリングな気がする）**
        - まあでも物体高さの事前知識を使うっていうのが重要（カメラ高さの決定時にも使用）．「深度マップを実スケール化」と言い切らない方がいい気がする
- **？：深度平均化の意味は？論理的にはカメラの高さ教師だけでいい気がする**
    - 物体領域の深度が，ある程度の領域に収まるようになり，結果としてpoint Cloud化した際の物体形状も向上
        - 尖るのを防いだり，物体形状が正しくなり，本当に欲しい部分の深度を得られるようにするため
            - 正確には物体形状を正しくするというよりも，スケール教師を作成する際には物体領域の深度のmaxを深度-幾何関係式の教師として利用しているが，このとき，物体形状が異常に尖ったりするのが困るので，ある一定範囲に収めるために深度平均化損失を採用
        - **なぜ物体形状がぐちゃぐちゃになりがち？**
            - low texture
        - 深度平均化により多少推定結果が平面的になるかもしれないが，それでいい．
        - **結局，深度平均化はカメラ高さ教師を正確にするための補助**
            - なので，あくまでもカメラ高さを利用して学習するのがメインの目的であることを明示．これの補助として深度平均化を使うことを主張
    - 副次的に，物体領域を理解した深度推定が可能になる
- **？：2ステージでやることに比べたメリット**
    - 2度学習しなくて良い
    - 1度学習しただけだと，教師としてハードなものになる
        - 「オンラインで教師を作ることで，学習の最終段階では，事前に作成するハードな教師よりも**教師の精度が良くなるように学習が進む**」ことを言えたら良い
        - 「教師の精度も良くなるように学習が進む」の部分については，深度平均化が担う
    - 学習初期だと物体領域の深度は無茶苦茶だからよくないみたいな話は無し．だって良く無さは道路平面も同じやし．
- **？：そもそもカメラ高さの一定性と物体高さを推定することの新規性が薄い？**
    - スケール合わせするとき，なぜSLAMじゃない？
        - SLAMを使う場合，全フレームでのスケール合わせ→各物体の深度1点からスケール係数を決定→これを全フレームで平均 or 最尤推定→SLAMの問題はフレームをすっ飛ばしてしまうことやけど，この方法で（とばしたフレーム以外で）カメラ高さを計算できるから問題にはならない．
    - 一応考えている新規性は **「物体高さの事前知識とカメラ高さが不変であることを利用し，最尤推定により全フレームについてスケール教師を作成」** と **「cross-ratioによる異常物体の除外」**

- 関連して，見切れ・遮蔽・誤分類・セグメンテーション領域の誤りを含む物体では，深度が正確ではない→**これは深度の代表点決定を可能にしているD_1と言える**
- 物体が登場しないフレームでは，スケールに対する教師が存在せず，そのフレームにおける深度の最適解はスケールを無視したものとなる

<!-- old -->
<!-- 3. $\boxed{\large \quad A' \quad}$ remains challenging because $\boxed{\large \quad C_1 \quad }$, $\boxed{\large \quad C_2 \quad }$, $\dots$
    <!-- - $C_1$: 画像上で，各物体の高さが定義される位置が不明 -->
    <!-- - $C_1$: 推定した深度から，物体高さを定義する際に代表すべき深度の値が不明 -->
    - $C_1$: 高さ-深度の幾何的制約の計算時に，代表すべき深度の値を決定するのが困難
        - なぜ難しい？
            - セグメンテーションしたことを前提に書く
            - 画像上の2D BBoxのピクセル高さに対応する，3次元空間の高さを定義する部分が，画像上ではどこに対応しているのかわからない
        - 仮に代表する深度を決定できても，制約として弱い
    - $C_2$: 物体は全フレームには出てこない -->

-----------------------------------------------------------------------------------------
# 04-17
-----------------------------------------------------------------------------------------

[前回のリンク](https://github.com/kyotovision/kinoshita_genki/issues/1)

# O-1: 段階的に学習パイプラインを実装する

## _KR-1a: 深度-高さの幾何的な関係式のみを教師として学習するパイプラインをmonodepth2に実装・結果分析_

以前この部分については，[Insta-DM](https://sites.google.com/site/seokjucv/home/instadm)(AAAI-2021) (物体のモーションも推定するモデル）の学習パイプラインに組み込むように実装したが，以降はmonodepth2に実装し，実験を回していく．理由は以下の通り．
- 最も一般的なモデルであるmonodepth2でうまくいくことを示し，Insta-DMのような少し特殊なモデルで無くてもうまくいくことを見(せ)たい
- Insta-DMは学習に4日程かかり，実験サイクルを回すのに苦労する．

補足：Insta-DMでは平均誤差が3m程度という何とも言えない結果だった．ただし距離の遠近を考慮せず計算した値なので，もしかすると距離の遠い領域で誤差が大きくなっているという可能性もある．

### 経過
- [x] cross ratioを用いて，異常な物体を検出するコードを実装
  - [ ] 上記のコードにより，異常な物体を取り除いたセグメンテーションデータセットを作成
- [x] monodepth2の学習パイプラインに，深度-高さの関係式を組み込む
  - 物体領域の深度は平均深度で代表
- [ ] 精度を分析する
  - [ ] GTとの絶対誤差を比較
    - [ ] 誤差が大きい/小さい領域を分析
  - [ ] シンプルなモデルの推定結果をGTのmedianでスケールした場合と，精度誤差を比較
  - [ ] 絶対誤差以外の指標について，シンプルなモデルと比較

### 結果
- monodepth2の学習パイプラインを完成させた
- Instance Segmentationの後処理（物体の一部が遠く離れたところにも存在すると推定されてしまう，Transformerを使ったセグメンテーションモデルならではのエラーを修正）がうまくできていなかったので処理
    <img src="https://user-images.githubusercontent.com/54442538/233776699-8bd966d0-88f0-4b6e-b209-c49567d145b4.png" width=60%>
- cross ratioを計算するコードは完成．
    - このコードを用いて，Instance Segmentation結果から異常な物体を除外したデータセットを作成中

## _KR-1b: 学習済みmonodepth2が推定したdepth mapからカメラ高さを計算した値が，実際のカメラ高さとどれだけ乖離があるかを確認_

最終的に考えているのは，毎エポックor数エポックごとにカメラ高さの計算を繰り返すこと．
その前段階として，一旦学習し終わったモデルでカメラ高さを計算した場合，どの程度カメラ高さが実際の値を乖離しているのか見る．
<!-- これが全然うまくいっていないようだと，おそらく提案したモデルは正しく学習できないため，方針を変える必要がる．そのための実験． -->

### 経過
- [x] OneFormerによるInstance Segmentation
- [x] OneFormerによる道路領域のSemantic Segmentation
- [x] 道路領域の推定したDepth mapからカメラ高さを計算
- [ ] 全フレームについてカメラ高さを計算し，実際のカメラ高さ(1.65m)と比較．
    - KR-1aで実装した異常な物体を除外したのち，カメラ高さを計算するよう修正

### 結果
カメラ高さを計算するコードの実装は終了．これを元にカメラ高さを計算していく

## _KR-1c: 深度-高さの関係式とカメラ高さの教師を両方組み込んだモデルを実装し，シンプルなmonodepth2・KR-1aの手法よりもあらゆる指標で精度が向上することを示す_

比較する評価指標は，
- absolute relative error
- squared relative error
- RMSE
- RMSE log
- $\delta < 1.25$
- $\delta < 1.25^2$
- $\delta < 1.25^3$
($\delta < threshold$ は， $max(gt/pred, pred/gt)$ が $threshold$ 未満となるピクセル数の割合)

※simpleなmonodepth2は，推定結果をmedian scalingしてから評価指標を計算．

### 経過
...
### 結果
...

-----------------------------------------------------------------------------------------
# 04-24 
-----------------------------------------------------------------------------------------

### 言葉の定義
- 「異常な物体」： 画像外に見切れた物体，遮蔽を含む物体，クラス分類・Segmentation領域を誤った物体
- 「幾何制約式」： 物体の高さ $H$, 物体の2D BBoxのピクセル高さ $h$, 物体の深度 $D$, y軸方向の焦点距離 $f_y$ の間に成立する以下の関係式
        $$H=\frac{D \cdot h}{f_y}$$
# O-1: 段階的に学習パイプラインを実装する

## _KR-1a: 幾何制約式のみを損失に加えて訓練→実装・結果分析（異常な物体を除去せず学習）_
幾何制約式で用いる物体の深度として，物体領域の深度を**平均した深度**を使用．
学習パイプラインは既に完成．現在訓練中．

#### 学習設定
- 異常な物体は**取り除いていない**
- 「物体」としては **「車」と「人間」だけ** を扱うことにしている
    - バスやトラック，自転車などはKITTIの3D BBox labelのクラスとして定義されておらず，これらのクラスのheight priorの信頼度が少し低い．height priorの計算手順は以下の通り．
        - OneFormer(COCOで学習)が推定したセグメントとKITTIの2D BBox labelのIoUが閾値を超えた時，COCOのクラスとその物体の高さを紐付ける．これを繰り返すことでCOCOの各クラスに対して複数の物体高さ情報が紐づく．紐付いた高さから平均・分散を計算し，各COCOクラスのheight priorを算出していた．
- 幾何制約式（損失）に掛ける係数はとりあえず **1** とする．

### 経過
- [x] 学習を完了する
- [ ] 精度を分析する
  - [ ] GTとの絶対誤差を比較
    - [ ] 誤差が大きい/小さい領域を分析
  - [ ] シンプルなモデルの推定結果をGTのmedianでスケールした場合と，精度誤差を比較
  - [ ] 絶対誤差以外の指標について，シンプルなモデルと比較

### 結果
学習が完了したので，現在はテストデータで推論中．

## _KR-1b: 幾何制約式のみを損失に加えて訓練し，実装・結果分析（異常な物体を除去して学習）_

#### 学習設定
- 異常な物体は**取り除く**
- 「物体」としては **「車」と「人間」だけ** を扱うことにしている
- 幾何制約式（損失）に掛ける係数はとりあえず **1** とする．

### 経過
- [x] 異常な物体を取り除いたセグメンテーションデータセットを作成
- [x] 上記のセグメンテーションデータセットを用いて学習を回す
- [ ] 学習を完了する
- [ ] 精度を分析する
  - [ ] GTとの絶対誤差を比較
    - [ ] 誤差が大きい/小さい領域を分析
  - [ ] シンプルなモデルの推定結果をGTのmedianでスケールした場合と，精度誤差を比較
  - [ ] 絶対誤差以外の指標について，シンプルなモデルと比較
  - [ ] 異常な物体を除去せず学習(KR-1a)したときと比べて，精度が向上しているかを確認

### 結果
現在学習中．


## _KR-1c: 学習済みmonodepth2が推定したdepth mapからカメラ高さを計算した値が，実際のカメラ高さとどれだけ乖離があるかを確認（異常な物体を除去して計算）_

最終的に考えているのは，毎エポックor数エポックごとにカメラ高さの計算を繰り返すこと．
その前段階として，一旦学習し終わったモデルでカメラ高さを計算した場合，どの程度カメラ高さが実際の値を乖離しているのか見る．

異常な物体を **除去せず** 計算したカメラ高さの結果は以下の通り．
| 実際のカメラ高さ | 単純に平均 | max(分散) $-$ 分散 で重み付け平均 | 最頻値 |
| :----: | :----: | :----: | :----: |
|1.6500 m|2.1932 m|2.1895 m|1.9114 m|
<img src="https://user-images.githubusercontent.com/54442538/233839651-2e83b3bc-ba06-471e-9cc7-67d943752694.png" width=40%>

### 経過
- [x] 異常な物体以外を用いてカメラ高さを計算．これを実際のカメラ高さと比較．

### 結果

異常な物体を **除いて** 計算したカメラ高さの結果は以下の通り．
| 実際のカメラ高さ | 単純に平均 | max(分散) $-$ 分散 で重み付け平均 | 最頻値 |
| :----: | :----: | :----: | :----: |
|1.6500 m|2.1391 m|2.1355 m|1.9713 m|
<img src="https://user-images.githubusercontent.com/54442538/235125838-54af3780-3607-4f87-91ee-41376bf8682a.png" width=40%>

カメラ高さを除かなかった場合と変化は小さい．原因として考えられるのは以下の通り． 

- 異常な物体を十分に取り除けていない 
  <table>
  <tr><td><img src="https://user-images.githubusercontent.com/54442538/235142057-384e1a8e-c7a5-4ead-ae39-2c7e6511ab45.png"></td></tr>
  <tr><td><img src="https://user-images.githubusercontent.com/54442538/235142092-7cad2bb1-16ea-4fb3-9679-55379ba2f0a4.png"></td></tr>
  <tr><td><img src="https://user-images.githubusercontent.com/54442538/235142133-713c554b-4940-404f-9c51-a81effd08f80.png"></td></tr>
  <tr><td><img src="https://user-images.githubusercontent.com/54442538/235142242-3b0c7a50-1ed6-4359-862f-b9ad0df03c07.png"></td></tr>
  </table>
  - 現在の手法では，登場するインスタンスが少ないフレーム中の異常な物体を取り除くのは困難（RANSACを行った際にどれを信用すればよいかわからない）
  - また多くのインスタンスが異常である場合もそれらを取り除くのは困難
  - 単純な解決策としては，
    - 「RANSACに外れ値と見なされたvanishing pointを生成した回数」 $\div$ 「vanishing pointの計算に使われた回数」の閾値（＝異常な物体とみなす厳しさ）を低くすると，異常な物体は取り除きやすくなる（その分必要以上にスケール計算に有用な物体まで取り除いてしまう可能性あり）

    - 結局画像の **左右・下端に触れている物体は，異常な物体であることが大半なので，これらをまず取り除いて** から，cross-ratioとRANSACで異常な物体を特定するようにする
      - 実際は，画像の下端に触れている物が異常な物体であることが多いが，OneFormerで推定したSegmentation結果は，なぜか下端にまでセグメントが及ばないように推定しているものが多い

    - 特にゴミみたいな小さいセグメントが悪影響を及ぼしていることが多い（かも）．これらを取り除いてから，cross-ratioとRANSACによる異常物体検知を行う
      - 小さい物体＝遠くにある物体は特に深度推定精度も微妙でセグメント領域も正しいのか怪しいものが多い．

- インスタンス数が少ない＆それらのセグメントや深度推定結果が不正確
  - 単純にインスタンス数の少ないものを削除するだけでは良くない．以下の図は，1フレームあたりに登場するインスタンス数の下限を変え，この下限を上回ったインスタンスが登場したフレームだけでカメラ高さを計算した結果．
    <table align="center">
    <tr>
    <th>1個以上のインスタンス</th>
    <th>2個以上のインスタンス</th>
    <th>3個以上のインスタンス</th>
    </tr>
    <tr>
    <td><img src="https://user-images.githubusercontent.com/54442538/235140013-5df1c766-5b67-4a85-84b8-9fcf110f61f7.png"></td>
    <td><img src="https://user-images.githubusercontent.com/54442538/235140028-7c9a5cf6-6af0-45cc-87d6-32c53e6023ae.png"></td>
    <td><img src="https://user-images.githubusercontent.com/54442538/235140047-00744963-2d4f-49e4-b426-2f2c6add55a7.png"></td>
    </tr></table>


その他にも，カメラ高さが真値から外れてしまう問題の原因として，上記以外に考えられるのは以下の通り．

- 道路領域が正しく推定できていない
  - これの解決にはモデルを変えるくらいしか方法が無い．現在はCityscapesで学習済みのOneformerを使用している．インスタンスセグメンテーションはCOCOで学習したもののほうが精度が良かったのでこちらを用いてるが，semantic segmentationに関してはCityscapesで学習したモデルの方が精度良かった（電柱などを見逃さず推定）
  <table>
  <tr><td><img src="https://user-images.githubusercontent.com/54442538/235142629-e81e60e3-69e6-4047-9cc5-a523d39d63ce.png"></td></tr>
  <tr><td><img src="https://user-images.githubusercontent.com/54442538/235142726-1f540aa9-d3fc-4cfe-a8eb-3f25b8acc32f.png"></td></tr>
  </table>

- 深度推定結果が正しくない
  - 最近の論文で比較対象にされがちな [ManyDepth](https://github.com/nianticlabs/manydepth) (CVPR2021)で推定した深度マップを用いて同様にカメラ高さを計算したが，monodepth2の深度マップを使った場合とほぼ変化なし．これはカメラ高さが不正確になってしまう原因であったとしても解決が困難．

- 道路が平面である仮定が成り立たない時
  <table>
  <tr><td><img src="https://user-images.githubusercontent.com/54442538/235143256-5a426a8c-94c1-4639-b210-ba7cb707a8e0.png"></td></tr>
  <tr><td><img src="https://user-images.githubusercontent.com/54442538/235143552-9d6d0bec-1738-4c85-9ff1-c66409f4869c.png"></td></tr>
  </table>
  - 解決は難しいかも．一旦塩漬け．



## _KR-1d: 幾何制約式とカメラ高さの教師を両方組み込んだモデルを実装し，シンプルなmonodepth2・KR-1a,bの手法よりもあらゆる指標で精度が向上することを示す_

比較する評価指標は，
- absolute relative error
- squared relative error
- RMSE
- RMSE log
- $\delta < 1.25$
- $\delta < 1.25^2$
- $\delta < 1.25^3$
($\delta < threshold$ は， $max(gt/pred, pred/gt)$ が $threshold$ 未満となるピクセル数の割合)

※simpleなmonodepth2は，推定結果をmedian scalingしてから評価指標を計算．

### 経過
...
### 結果
...





-----------------------------------------------------------------------------------------
# 05-01 
-----------------------------------------------------------------------------------------

# 教師なしでの絶対深度推定

### 言葉の定義
- 「異常な物体」： 画像外に見切れた物体，遮蔽を含む物体，クラス分類・Segmentation領域を誤った物体
- 「幾何制約式」： 物体の高さ $H$, 物体の2D BBoxのピクセル高さ $h$, 物体の深度 $D$, y軸方向の焦点距離 $f_y$ の間に成立する以下の関係式
        $$H=\frac{D \cdot h}{f_y}$$

### 前回の総括
1. OneFormerのInstance Segmentation結果からcross-ratioとRANSACによる異常な物体を取り除いたデータセットを作成した

2. monodepth2に幾何的制約のみを損失に加えて学習するパイプラインを完成させた

3. 2 の学習パイプラインを使って学習を完了

4. 2 の学習パイプラインを使い，1 の異常な物体を除いたインスタンスのみに幾何的制約を加えるように学習
  - 学習が終了したので，テストデータで推論中
5. カメラ高さを計算するスクリプトを実装

6. 異常な物体を取り除いた場合と含めた場合で，別々にカメラ高さを計算し，差を比較．
  - あまり違いは出なかった．
  - <details><summary>詳細</summary>

    異常な物体を **除去せず** 計算したカメラ高さの結果は以下の通り．
    | 実際のカメラ高さ | 単純に平均 | max(分散) $-$ 分散 で重み付け平均 | 最頻値 |
    | :----: | :----: | :----: | :----: |
    |1.6500 m|2.1932 m|2.1895 m|1.9114 m|
    <img src="https://user-images.githubusercontent.com/54442538/233839651-2e83b3bc-ba06-471e-9cc7-67d943752694.png" width=40%>

    異常な物体を **除いて** 計算したカメラ高さの結果は以下の通り．
    | 実際のカメラ高さ | 単純に平均 | max(分散) $-$ 分散 で重み付け平均 | 最頻値 |
    | :----: | :----: | :----: | :----: |
    |1.6500 m|2.1391 m|2.1355 m|1.9713 m|
    <img src="https://user-images.githubusercontent.com/54442538/235125838-54af3780-3607-4f87-91ee-41376bf8682a.png" width=40%>

    差が出ていない原因として考えられるのは以下の通り．

    - 異常な物体を十分に取り除けていない 
      <table>
      <tr><td><img src="https://user-images.githubusercontent.com/54442538/235142057-384e1a8e-c7a5-4ead-ae39-2c7e6511ab45.png"></td></tr>
      <tr><td><img src="https://user-images.githubusercontent.com/54442538/235142092-7cad2bb1-16ea-4fb3-9679-55379ba2f0a4.png"></td></tr>
      <tr><td><img src="https://user-images.githubusercontent.com/54442538/235142133-713c554b-4940-404f-9c51-a81effd08f80.png"></td></tr>
      <tr><td><img src="https://user-images.githubusercontent.com/54442538/235142242-3b0c7a50-1ed6-4359-862f-b9ad0df03c07.png"></td></tr>
      </table>
      - 現在の手法では，登場するインスタンスが少ないフレーム中の異常な物体を取り除くのは困難（RANSACを行った際にどれを信用すればよいかわからない）
      - また多くのインスタンスが異常である場合もそれらを取り除くのは困難
      - 単純な解決策としては，
        - 「RANSACに外れ値と見なされたvanishing pointを生成した回数」 $\div$ 「vanishing pointの計算に使われた回数」の閾値（＝異常な物体とみなす厳しさ）を低くすると，異常な物体は取り除きやすくなる（その分必要以上にスケール計算に有用な物体まで取り除いてしまう可能性あり）

        - 結局画像の **左右・下端に触れている物体は，異常な物体であることが大半なので，これらをまず取り除いて** から，cross-ratioとRANSACで異常な物体を特定するようにする
          - 実際は，画像の下端に触れている物が異常な物体であることが多いが，OneFormerで推定したSegmentation結果は，なぜか下端にまでセグメントが及ばないように推定しているものが多い

        - 特にゴミみたいな小さいセグメントが悪影響を及ぼしていることが多い（かも）．これらを取り除いてから，cross-ratioとRANSACによる異常物体検知を行う
          - 小さい物体＝遠くにある物体は特に深度推定精度も微妙でセグメント領域も正しいのか怪しいものが多い．

    - インスタンス数が少ない＆それらのセグメントや深度推定結果が不正確（このようなインスタンスを取り除けていない）
      - 単純にインスタンス数の少ないものを削除するだけでは良くない．以下の図は，1フレームあたりに登場するインスタンス数の下限を変え，この下限を上回ったインスタンスが登場したフレームだけでカメラ高さを計算した結果．
        <table align="center">
        <tr>
        <th>1個以上のインスタンス</th>
        <th>2個以上のインスタンス</th>
        <th>3個以上のインスタンス</th>
        </tr>
        <tr>
        <td><img src="https://user-images.githubusercontent.com/54442538/235140013-5df1c766-5b67-4a85-84b8-9fcf110f61f7.png"></td>
        <td><img src="https://user-images.githubusercontent.com/54442538/235140028-7c9a5cf6-6af0-45cc-87d6-32c53e6023ae.png"></td>
        <td><img src="https://user-images.githubusercontent.com/54442538/235140047-00744963-2d4f-49e4-b426-2f2c6add55a7.png"></td>
        </tr></table>
    </details>

### 今週やることの概要
- [ ] 異常な物体を包含/除去して幾何制約式を損失として加え学習したモデルを評価
- [ ] 異常な物体の検知手法について微修正を施し，再度カメラ高さを計算→より改善が必要そうであれば案を出す
    - 「RANSACに外れ値と見なされたvanishing pointを生成した回数」 $\div$ 「vanishing pointの計算に使われた回数」の閾値（＝異常な物体とみなす厳しさ）を低くする
      - 必要以上にスケール計算に有用な物体まで取り除いてしまう可能性あり

    - 結局画像の **左右・下端に触れている物体は，異常な物体であることが大半なので，これらをまず取り除いて** から，cross-ratioとRANSACで異常な物体を特定する
      - 実際は，画像の下端に触れている物が異常な物体であることが多いが，OneFormerで推定したSegmentation結果は，なぜか下端にまでセグメントが及ばないように推定しているものが多い

    - 特にゴミみたいな小さいセグメントが悪影響を及ぼしていることが多い（かも）．これらを取り除いてから，cross-ratioとRANSACによる異常物体検知を行う
      - 小さい物体＝遠くにある物体は特に深度推定精度も微妙でセグメント領域も正しいのか怪しいものが多い．

# O-1: 異常な物体の検出手法を確立する

## _KR-1: 学習済みmonodepth2が推定したdepth mapからカメラ高さを計算した値が，より真値に近づくよう，十分に異常な物体を除去する_

最終的に考えているのは，毎エポックor数エポックごとにカメラ高さの計算を繰り返すこと．
その前段階として，一旦学習し終わったモデルでカメラ高さを計算した値が，ある程度真値に近くなっていてほしい．
全てのインスタンスを利用して計算したカメラ高さの値はおよそ2m(真値は1.65m)と，少し真値との乖離が大きい．この乖離を小さくするのを，異常な物体を取り除くことで実現したい．

単純にcross-ratioとRANSACを組み合わせた除去手法のみでは十分に異常な物体を取り除けておらず，それらを除去した場合としなかった場合で計算したカメラ高さの値に大きな差は生まれなかった．この差を生むように除去手法を改善する．

### 経過
- [ ] とりあえず単純な改善案を試して，その結果カメラ高さの計算結果に影響があったかを見てみる
    - 「RANSACに外れ値と見なされたvanishing pointを生成した回数」 $\div$ 「vanishing pointの計算に使われた回数」の閾値（＝異常な物体とみなす厳しさ）を低くする
      - 必要以上にスケール計算に有用な物体まで取り除いてしまう可能性あり

    - 結局画像の **左右・下端に触れている物体は，異常な物体であることが大半なので，これらをまず取り除いて** から，cross-ratioとRANSACで異常な物体を特定する
      - 実際は，画像の下端に触れている物が異常な物体であることが多いが，OneFormerで推定したSegmentation結果は，なぜか下端にまでセグメントが及ばないように推定しているものが多い

    - 特にゴミみたいな小さいセグメントが悪影響を及ぼしていることが多い（かも）．これらを取り除いてから，cross-ratioとRANSACによる異常物体検知を行う
      - 小さい物体＝遠くにある物体は特に深度推定精度も微妙でセグメント領域も正しいのか怪しいものが多い．
- [ ] 上記の結果を見て，改善が必要そうであれば案を出す．

### 結果
...


# O-2: 段階的に学習パイプラインを実装し学習する

## _KR-2a: 幾何制約式のみを損失に加えて訓練→実装・結果分析（異常な物体を除去せず学習）_
幾何制約式で用いる物体の深度として，物体領域の深度を**平均した深度**を使用．
学習パイプラインは既に完成．現在訓練中．

#### 学習設定
- 異常な物体は**取り除いていない**
- 「物体」としては **「車」と「人間」だけ** を扱うことにしている
    - バスやトラック，自転車などはKITTIの3D BBox labelのクラスとして定義されておらず，これらのクラスのheight priorの信頼度が少し低い．height priorの計算手順は以下の通り．
        - OneFormer(COCOで学習)が推定したセグメントとKITTIの2D BBox labelのIoUが閾値を超えた時，COCOのクラスとその物体の高さを紐付ける．これを繰り返すことでCOCOの各クラスに対して複数の物体高さ情報が紐づく．紐付いた高さから平均・分散を計算し，各COCOクラスのheight priorを算出していた．
- 幾何制約式（損失）に掛ける係数はとりあえず **1** とする．

### 経過
- [ ] 精度を分析する
  - [ ] GTとの絶対誤差を比較
    - [ ] 誤差が大きい/小さい領域を分析
  - [ ] シンプルなモデルの推定結果をGTのmedianでスケールした場合と，精度誤差を比較
  - [ ] 絶対誤差以外の指標について，シンプルなモデルと比較

### 結果
...

## _KR-2b: 幾何制約式のみを損失に加えて訓練し，実装・結果分析（異常な物体を除去して学習）_

#### 学習設定
- 異常な物体は**取り除く**
- 「物体」としては **「車」と「人間」だけ** を扱うことにしている
- 幾何制約式（損失）に掛ける係数はとりあえず **1** とする．

### 経過
- [ ] 精度を分析する
  - [ ] GTとの絶対誤差を比較
    - [ ] 誤差が大きい/小さい領域を分析
  - [ ] シンプルなモデルの推定結果をGTのmedianでスケールした場合と，精度誤差を比較
  - [ ] 絶対誤差以外の指標について，シンプルなモデルと比較
  - [ ] 異常な物体を除去せず学習(KR-1a)したときと比べて，精度が向上しているかを確認

### 結果
...



-----------------------------------------------------------------------------------------
# 05-08
-----------------------------------------------------------------------------------------
### 言葉の定義
- 「異常な物体」： 画像外に見切れた物体，遮蔽を含む物体，クラス分類・Segmentation領域を誤った物体
- 「幾何制約式」： 物体の高さ $H$, 物体の2D BBoxのピクセル高さ $h$, 物体の深度 $D$, y軸方向の焦点距離 $f_y$ の間に成立する以下の関係式
        $$H=\frac{D \cdot h}{f_y}$$

### 前回の総括
monodepth2に幾何的制約のみを損失に加えて学習するパイプラインを修正
    - 手違いにより学習率が非常に低い状態で学習していたたため，全く学習できてなかった．現在は再度学習中

### 今週やることの概要
- [ ] 異常な物体を包含/除去して幾何制約式を損失として加え学習したモデルを評価
- [ ] 異常な物体の検知手法について微修正を施し，再度カメラ高さを計算→より改善が必要そうであれば案を出す
    - RANSACで１度にサンプリングするvanishing pointの数を増やす（現在はデフォルト値の３個でやっていた）
      - あまり大きくしすぎると，ほぼ線形回帰になるので，outlierが非常に少なくなってくることに注意
    - RANSACで，現在注目しているフレームに時間的に近いフレームに現れる物体ほど大きな重みをおいて計算するようにする．
      - VPは２つのインスタンスから計算され，どちらかが遠いフレームだったり両方が遠いフレームだったりして，重みの付け方が難しい．
      - ２つのインスタンスが登場するフレームの現在のフレームからの距離を足し算 or 掛け算したものを重みとする？
    - 「RANSACに外れ値と見なされたvanishing pointを生成した回数」 $\div$ 「vanishing pointの計算に使われた回数」の閾値（＝異常な物体とみなす厳しさ）を低くする
      - 必要以上にスケール計算に有用な物体まで取り除いてしまう可能性あり

    - 結局画像の **左右・下端に触れている物体は，異常な物体であることが大半なので，これらをまず取り除いて** から，cross-ratioとRANSACで異常な物体を特定する
      - 実際は，画像の下端に触れている物が異常な物体であることが多いが，OneFormerで推定したSegmentation結果は，なぜか下端にまでセグメントが及ばないように推定しているものが多い

    - 特にゴミみたいな小さいセグメントが悪影響を及ぼしていることが多い（かも）．これらを取り除いてから，cross-ratioとRANSACによる異常物体検知を行う
      - 小さい物体＝遠くにある物体は特に深度推定精度も微妙でセグメント領域も正しいのか怪しいものが多い．

# O-1: 異常な物体の検出手法を確立する

## _KR-1: 学習済みmonodepth2が推定したdepth mapからカメラ高さを計算した値が，より真値に近づくよう，十分に異常な物体を除去する_

最終的に考えているのは，毎エポックor数エポックごとにカメラ高さの計算を繰り返すこと．
その前段階として，一旦学習し終わったモデルでカメラ高さを計算した値が，ある程度真値に近くなっていてほしい．
全てのインスタンスを利用して計算したカメラ高さの値はおよそ2m(真値は1.65m)と，少し真値との乖離が大きい．この乖離を小さくするのを，異常な物体を取り除くことで実現したい．

単純にcross-ratioとRANSACを組み合わせた除去手法のみでは十分に異常な物体を取り除けておらず，それらを除去した場合としなかった場合で計算したカメラ高さの値に大きな差は生まれなかった．この差を生むように除去手法を改善する．

### 経過
- [x] とりあえず単純な改善案を試して，その結果カメラ高さの計算結果に影響があったかを見てみる
    1. RANSACで１度にサンプリングするVanishing Point(VP)の数を増やす（現在はデフォルト値の３個でやっていた）
        - あまり大きくしすぎると，ほぼ線形回帰になるので，outlierが非常に少なくなってくることに注意

    2. RANSACで，現在注目しているフレームに時間的に近いフレームに現れる物体ほど，大きな重みをおいてスコアを計算するようにする．
        - VPは２つのインスタンスから計算され，どちらかが遠いフレームだったり両方が遠いフレームだったりして，重みの付け方が難しい．
        - ２つのインスタンスが登場するフレームの現在のフレームからの距離を足し算 or 掛け算したものを重みとする？

    3. 「RANSACに外れ値と見なされたvanishing pointを生成した回数」 $\div$ 「vanishing pointの計算に使われた回数」の閾値（＝異常な物体とみなす厳しさ）を低くする
        - 必要以上にスケール計算に有用な物体まで取り除いてしまう可能性あり

    4. 結局画像の **左右・下端に触れている物体は，異常な物体であることが大半なので，これらをまず取り除いて** から，cross-ratioとRANSACで異常な物体を特定する
        - 実際は，画像の下端に触れている物が異常な物体であることが多いが，OneFormerで推定したSegmentation結果は，なぜか下端にまでセグメントが及ばないように推定しているものが多い

    5. 特にゴミみたいな小さいセグメントが悪影響を及ぼしていることが多い（かも）．これらを取り除いてから，cross-ratioとRANSACによる異常物体検知を行う
        - 小さい物体＝遠くにある物体は特に深度推定精度も微妙でセグメント領域も正しいのか怪しいものが多い．

- [ ] 上記の結果を見て，改善が必要そうであれば案を出す．

### 結果
- i, ii, iii, iv, vの条件をいろいろ変えてみて，異常な物体を正しく取り除けているかを定性的に見てみたが，**大きな差は生まれなかった**．
    - iv 以外はどの場合も大きな差は生まれていない．ivはやはり取り除きすぎてしまう．
    - カメラ高さを出す場合には，小さい物体を取り除くのは良さそう（これから試す）
- 根本的に別の解決策が必要かもしれない．いろいろ検討してみる．


# O-2: 段階的に学習パイプラインを実装し学習する
## _KR-2a: 幾何制約式のみを損失に加えて訓練→実装・結果分析_
幾何制約式で用いる物体の深度として，物体領域の深度を**平均した深度**を使用．

#### 学習設定
- 異常な物体は**取り除く**
- 「物体」としては **「車」と「人間」だけ** を扱うことにしている
    - バスやトラック，自転車などはKITTIの3D BBox labelのクラスとして定義されておらず，これらのクラスのheight priorの信頼度が少し低い．height priorの計算手順は以下の通り．
        - OneFormer(COCOで学習)が推定したセグメントとKITTIの2D BBox labelのIoUが閾値を超えた時，COCOのクラスとその物体の高さを紐付ける．これを繰り返すことでCOCOの各クラスに対して複数の物体高さ情報が紐づく．紐付いた高さから平均・分散を計算し，各COCOクラスのheight priorを算出していた．
- 幾何制約式（損失）に掛ける係数はとりあえず 0.1, 0.5, 1.0, 2.0 とする．

### 経過
- [x] 精度を分析する
  - [x] GTとの絶対誤差を比較
    - [x] 誤差が大きい/小さい領域を分析
  - [x] シンプルなモデルの推定結果をGTのmedianでスケールした場合と，精度誤差を比較
  - [x] 絶対誤差以外の指標について，シンプルなモデルと比較

### 結果
| coefficient | abse $\downarrow$ | abs_rel $\downarrow$ | sq_rel $\downarrow$ | rmse $\downarrow$ | rmse_log $\downarrow$ | a1 $\uparrow$ | a2 $\uparrow$ | a3 $\uparrow$ |
| - | - | - | - | - | - | - | - | - |
| none w/o scaling | 15.574 | 0.967 | 15.078 | 19.161 | 3.450 | 0.000 | 0.000 | 0.000 |
| none w/  scaling | 2.154 | 0.109 | 0.872 | 4.680 | 0.186 | 0.889 | 0.963 | 0.982 |
| 0.1 | 11.676 | 0.728 | 10.269 | 15.065 | 1.238 | 0.003 | 0.007 | 0.013 |
| 0.5 | 11.667 | 0.732 | 11.038 | 15.292 | 1.209 | 0.004 | 0.009 | 0.018 |
| 0.5 with outliers | 11.723 | 0.741 | 11.853 | 15.544 | 1.200 | 0.005 | 0.012 | 0.024 |
| 1.0 | 11.667 | 0.732 | 11.038 | 15.292 | 1.209 | 0.004 | 0.009 | 0.018 |
| 2.0 | 16.161 | 1.058 | 21.826 | 20.581 | 3.864 | 0.009 | 0.018 | 0.028 |

<details><summary>指標の意味</summary>

- abse: absolute error
- abs_rel: absolute relative error
- sq_rel: squared relative error
- rmse: root mean squared error
- rmse_log: root mean squared error log
- a1:  $\max(\mathrm{gt}/\mathrm{pred}, \mathrm{pred}/\mathrm{gt})$ が $1.25$ 未満となるピクセル数の割合
- a2:  $\max(\mathrm{gt}/\mathrm{pred}, \mathrm{pred}/\mathrm{gt})$ が $1.25^2$ 未満となるピクセル数の割合
- a3:  $\max(\mathrm{gt}/\mathrm{pred}, \mathrm{pred}/\mathrm{gt})$ が $1.25^3$ 未満となるピクセル数の割合
</details>

さすがにスケールに関する損失をつけなかった場合(none w/o scaling)に比べて，スケールを理解しようとしている姿勢は見て取れるが，abseが11mとかなり誤差が大きい．また absolute relative error も 0.73 (=gtとpredの差がgtの0.7倍) と大きい．損失なしモデルをGTでmedian scalingした場合に比べてまだまだ負けている．
スケール損失に掛けた係数を0.5として異常な物体を**取り除かず**学習に使用した場合と比較したが，**差は小さかった**．

#### 学習過程の評価指標の推移
<img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/059b92e7-ba87-403c-ac70-8161ec9ef368" width=100%>

下がってほしい指標(de)は徐々に下がってきてくれている．ただ上がってほしい指標(da)は何故か下がり続ける...

#### 推論結果
左から順に，幾何制約式の損失にかけた係数は 0.1, 0.5, 1.0, 2.0, 0 としている．
![image](https://github.com/kyotovision/kinoshita_genki/assets/54442538/7ef1a096-9814-4f39-b4f0-aeb917a5daff)

係数を大きくすればするほどエッジがぼやける ＆ 謎の無限に飛ぶ領域が生まれる．
無限に飛ぶ領域の多くは車の窓の領域に現れている．窓の領域はそもそも推論結果が無限に飛びやすい（どのような深度を推定しても，隣接フレームにinverse warpingしてreconstruction lossを計算した結果が小さくならないから？）．なので物体領域の平均した深度を用いてスケールを学習させようとしても，この窓領域でスケールをうまいこと調節するだけで，物体全体が実スケールに学習できていない？その結果が無限に飛ぶ帯状の領域として現れてくるのでは？

<details><summary>後回しでやること</summary>


## _KR-2c: 幾何制約式とカメラ高さの教師を両方組み込んだモデルを実装し，シンプルなmonodepth2・KR-1a,bの手法よりもあらゆる指標で精度が向上することを示す_
比較する評価指標は，
- absolute relative error
- squared relative error
- RMSE
- RMSE log
- $\delta < 1.25$
- $\delta < 1.25^2$
- $\delta < 1.25^3$
($\delta < threshold$ は， $max(gt/pred, pred/gt)$ が $threshold$ 未満となるピクセル数の割合)

※simpleなmonodepth2は，推定結果をmedian scalingしてから評価指標を計算．

# O-3: 学習の高速化
## _KR-3: DDPにより複数GPUでの学習を可能にする_

元々のmonodepth2の実装がシングルGPUでの学習しか想定されておらず，学習速度が遅い．DDPを実装することで高速化する．
余裕があれば進めていく感じで．

</details>


-----------------------------------------------------------------------------------------
# 05-15
-----------------------------------------------------------------------------------------
### 前回の総括
- 幾何制約式による損失のみを設定して学習したモデルで推論・評価
    - 損失にかける係数を大きくするほどボヤケた深度マップを出力 ＆ 謎の無限の深度に飛ぶ帯状領域が発生
        - 車の窓で顕著．車全体でスケールを学べておらず，窓の領域でうまいことスケールを調整するよう学習しているだけ？
    - この損失を設定しなかった場合よりかはmedian scalingなしでも，GTのスケールに近い推定精度となったがまだまだ誤差は大きい
    - 訓練が進むに連れてa1, a2, a3が下降して行く謎
- 異常な物体を取り除く手法として，簡単な改善案を試したが目視で見た感じあまり差は出なかった
    - <details><summary>簡単な改善案</summary>

      1. RANSACで１度にサンプリングするVanishing Point(VP)の数を増やす（現在はデフォルト値の３個でやっていた）
          - あまり大きくしすぎると，ほぼ線形回帰になるので，outlierが非常に少なくなってくることに注意

      2. RANSACで，現在注目しているフレームに時間的に近いフレームに現れる物体ほど，大きな重みをおいてスコアを計算するようにする．
          - VPは２つのインスタンスから計算され，どちらかが遠いフレームだったり両方が遠いフレームだったりして，重みの付け方が難しい．
          - ２つのインスタンスが登場するフレームの現在のフレームからの距離を足し算 or 掛け算したものを重みとする？

      3. 「RANSACに外れ値と見なされたvanishing pointを生成した回数」 $\div$ 「vanishing pointの計算に使われた回数」の閾値（＝異常な物体とみなす厳しさ）を低くする
          - 必要以上にスケール計算に有用な物体まで取り除いてしまう可能性あり

      4. 結局画像の **左右・下端に触れている物体は，異常な物体であることが大半なので，これらをまず取り除いて** から，cross-ratioとRANSACで異常な物体を特定する
          - 実際は，画像の下端に触れている物が異常な物体であることが多いが，OneFormerで推定したSegmentation結果は，なぜか下端にまでセグメントが及ばないように推定しているものが多い

      5. 特にゴミみたいな小さいセグメントが悪影響を及ぼしていることが多い（かも）．これらを取り除いてから，cross-ratioとRANSACによる異常物体検知を行う
          - 小さい物体＝遠くにある物体は特に深度推定精度も微妙でセグメント領域も正しいのか怪しいものが多い．
      </details>

### 今週やることの概要
- カメラ高さの一貫性を利用したスケール教師を用いて学習するパイプラインを作っていく
  - 深度が無限に飛ぶ帯状領域も修正していく必要があるが，とりあえず研究の大枠の筋が問題ないか確認していきたいので，実装を進める
- 異常な物体の検知手法についての改善案を出す


# O-1: 異常な物体の検出手法を確立する

## _KR-1: 学習済みmonodepth2が推定したdepth mapからカメラ高さを計算した値が，より真値に近づくよう，十分に異常な物体を除去する_

最終的に考えているのは，毎エポックor数エポックごとにカメラ高さの計算を繰り返すこと．
その前段階として，一旦学習し終わったモデルでカメラ高さを計算した値が，ある程度真値に近くなっていてほしい．
全てのインスタンスを利用して計算したカメラ高さの値はおよそ2m(真値は1.65m)と，少し真値との乖離が大きい．この乖離を小さくするのを，異常な物体を取り除くことで実現したい．

単純にcross-ratioとRANSACを組み合わせた除去手法のみでは十分に異常な物体を取り除けておらず，それらを除去した場合としなかった場合で計算したカメラ高さの値に大きな差は生まれなかった．この差を生むように除去手法を改善する．もしくは根本的に異なる手法を提案する．

### 結果
考え中．．．

# O-2: 段階的に学習パイプラインを実装し学習する

## _KR-2a: カメラ高さの一貫性のみを損失に加えて訓練→結果分析_
<details><summary>手法</summary>

1. detachした深度マップから，物体領域のうちカメラ座標の原点に最も近い点の座標を取得．
2. これを物体深度の代表点とし，深度-高さの幾何制約式から最尤推定により各フレームのスケールを計算．
3. フレームごとにカメラ高さを計算．
4. 3.で計算したカメラ高さをdetachし，2.で計算したスケールによりスケーリング．全フレームに渡り平均を取ってカメラ高さの教師とする．
5. 3.で計算したカメラ高さの教師として，1エポック前に4.で計算したカメラ高さを利用
</details>

とりあえず研究の方向性が問題なさそうか（問題があっても解決できそうか）を見るためにKR-2bで発生している問題を後回しにして実装を進める．

#### 学習設定
- **異常な物体は取り除いていない**
- 1エポック目は全フレームにわたるカメラ高さを計算し，その平均値を求めるだけで，カメラ高さの損失は設けない

### 結果
coefficient(カメラ高さの損失に掛ける係数)が0のものはオリジナルのmonodepth2(w/ scalingはmedian scalingしてから評価した結果)．
オリジナルのmonodepth2は20エポック学習し，新たに損失を設定したモデルは最も精度の良かった2エポック学習させてモデルで評価（2エポック目以降lossが発散し学習失敗）


- テストデータでの評価結果
    | coefficient | abse $\downarrow$ | abs_rel $\downarrow$ | sq_rel $\downarrow$ | rmse $\downarrow$ | rmse_log $\downarrow$ | a1 $\uparrow$ | a2 $\uparrow$ | a3 $\uparrow$ |
    | - | - | - | - | - | - | - | - | - |
    | 0 w/o scaling | 15.574 | 0.967 | 15.078 | 19.161 | 3.450 | 0.000 | 0.000 | 0.000 |
    | 0 w/  scaling | 2.154 | 0.109 | 0.872 | 4.680 | 0.186 | 0.889 | 0.963 | 0.982 |
    | 0.1 w/o scaling | 8.836  |   0.666  |   7.497  |  12.461  |   0.690  |   0.224  |   0.424  |   0.624  |
    | 0.1 w/ scaling |   8.096  |   0.488  |   5.626  |  12.633  |   0.669  |   0.273  |   0.512  |   0.717  |
    | 0.5 w/o scaling |  15.987  |   0.991  |  15.888  |  19.695  |   4.912  |   0.000  |   0.000  |   0.000  |
    | 0.5 w/ scaling |   7.509  |   0.443  |   4.757  |  12.083  |   0.588  |   0.303  |   0.561  |   0.766  |
    | 1.0 w/o scaling |   8.056  |   0.589  |   6.137  |  11.801  |   0.614  |   0.254  |   0.480  |   0.701  |
    | 1.0 w/ scaling |   7.505  |   0.450  |   4.827  |  11.966  |   0.587  |   0.302  |   0.561  |   0.764  |
    | 2.0 w/o scaling |   8.042  |   0.592  |   6.048  |  11.765  |   0.609  |   0.251  |   0.477  |   0.699  |
    | 2.0 w/ scaling |   7.451  |   0.441  |   4.703  |  11.993  |   0.582  |   0.306  |   0.567  |   0.769  |

    - <details><summary>指標の意味</summary>

        - abse: absolute error

        - abs_rel: absolute relative error

        - sq_rel: squared relative error

        - rmse: root mean squared error

        - rmse_log: root mean squared error log

        - a1:  $\max(\mathrm{gt}/\mathrm{pred}, \mathrm{pred}/\mathrm{gt})$ が $1.25$ 未満となるピクセル数の割合

        - a2:  $\max(\mathrm{gt}/\mathrm{pred}, \mathrm{pred}/\mathrm{gt})$ が $1.25^2$ 未満となるピクセル数の割合

        - a3:  $\max(\mathrm{gt}/\mathrm{pred}, \mathrm{pred}/\mathrm{gt})$ が $1.25^3$ 未満となるピクセル数の割合
    </details>

    - **median scalingした結果と大きな差はない→スケールはおおよそ学べているが，細かい部分の学習が不十分**

    - 係数が0.5のときだけ評価がひどいという謎

- 深度マップ
    - オリジナル（エポック数20）
      ![download](https://github.com/kyotovision/kinoshita_genki/assets/54442538/0b0d34e4-803f-4a83-864b-c7165a5df161)

    - 係数 1.0（エポック数2）
      ![image](https://github.com/kyotovision/kinoshita_genki/assets/54442538/6894c2d1-9ab8-46a8-96e6-d282ac2cac44)

    そもそもエポック数が非常に少ないのでボヤケている（しかし学習が進んでもこんな感じのまま．．．）


- <details><summary>訓練時の様子</summary>
    <img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/b7953303-aab7-4c13-ac00-323393dd0298" width=60%>
    <img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/441d2aab-4607-4db8-98e0-ecf788cba0d9" width=90%>
    <img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/18b490f3-9adc-49cd-85eb-f28a0422e0a4" width=90%>
    <img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/c2336768-1540-4278-a83a-73a4b8bb11fe" width=90%>
  </details>


### 考察
- 学習が失敗した（カメラ高さが異常に大きくなる）理由の可能性
    - 単に実装のミス（挙動の不思議さ的にこれな気はする)
    - 損失に掛ける係数が大きすぎた
    - 損失関数が良くなかった（negative log likelihood）
- 修正する必要があるかも
    - **monodepth2では，softmax関数によりスケールを正規化された逆深度を出力し，逆深度の逆数（＝深度）の値域(0,1)が(min-depth, max-depth)になるようスケールしたものを最終的な深度マップとしている．min-depth, max-depthは予め定義しておく．スケールフリーな手法では評価時にmedian scalingされるので問題ないが，直接実スケールの深度マップを出力する場合は，問題に成りうる．**
        - max-depthよりも遠い地点が推定できなくなる


## _KR-2b: 幾何制約式のみを損失に加えて訓練したモデルの推定結果に現れる，無限に飛ぶ領域がなくなるように修正する_
<table>
<tbody>
  <tr>
    <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/74be2ae5-f4d5-4a58-8f26-d5906883aa86"></td>
    <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/795488e2-2600-4ffb-95cd-10e8a7719352"></td>
    <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/f3303f1b-a620-4d9c-a663-cd6f0ea2a442"></td>
  </tr>
</tbody>
</table>
幾何制約式のみを損失に加えて訓練したモデルでは，無限に深度が飛んでいく帯状領域が発生する．これを修正する案を出す．


#### 学習設定
- 異常な物体は**取り除く**
- 「物体」としては **「車」と「人間」だけ** を扱うことにしている
    - バスやトラック，自転車などはKITTIの3D BBox labelのクラスとして定義されておらず，これらのクラスのheight priorの信頼度が少し低い．height priorの計算手順は以下の通り．
        - OneFormer(COCOで学習)が推定したセグメントとKITTIの2D BBox labelのIoUが閾値を超えた時，COCOのクラスとその物体の高さを紐付ける．これを繰り返すことでCOCOの各クラスに対して複数の物体高さ情報が紐づく．紐付いた高さから平均・分散を計算し，各COCOクラスのheight priorを算出していた．

### 結果
経過なし

-----------------------------------------------------------------------------------------
# 05-22
-----------------------------------------------------------------------------------------
### 前回の総括
- カメラ高さの一貫性のみを損失に加えて訓練
    - <details><summary>手法</summary>

        1. detachした深度マップから，物体領域のうちカメラ座標の原点に最も近い点の座標を取得．
        2. これを物体深度の代表点とし，深度-高さの幾何制約式から最尤推定により各フレームのスケールを計算．
        3. フレームごとにカメラ高さを計算．
        4. 3.で計算したカメラ高さをdetachし，2.で計算したスケールによりスケーリング．全フレームに渡り平均を取ってカメラ高さの教師とする．
        5. 3.で計算したカメラ高さの教師として，1エポック前に4.で計算したカメラ高さを利用
      </details>
    - 2エポック目以降，カメラ高さが真の値からかけ離れた値を取るようになり学習が失敗
        - 1エポック目はフレーム全体の平均カメラ高さを計算するだけで，カメラ高さ損失は設けていない
    - 2エポックだけ学習したモデルで推論した結果と，それをmedian scalingした結果の精度差は大きくなかった
        - おおよそのスケールは学べているが，細かい部分が理解できていない
        - 2エポックしか学習できていないので細かい部分が理解できないのは当然．正常に学習が進むように修正していく必要あり．

### 今週やることの概要
- カメラ高さの一貫性のみを損失に加えて訓練した際，学習が失敗する原因を特定し，改善する
- 異常な物体の検知手法についての改善案を出す


# O-1: 異常な物体の検出手法を確立する

## _KR-1: 学習済みmonodepth2が推定したdepth mapからカメラ高さを計算した値が，より真値に近づくよう，十分に異常な物体を除去する_

最終的に考えているのは，毎エポックor数エポックごとにカメラ高さの計算を繰り返すこと．
その前段階として，一旦学習し終わったモデルでカメラ高さを計算した値が，ある程度真値に近くなっていてほしい．
全てのインスタンスを利用して計算したカメラ高さの値はおよそ2m(真値は1.65m)と，少し真値との乖離が大きい．この乖離を小さくするのを，異常な物体を取り除くことで実現したい．

単純にcross-ratioとRANSACを組み合わせた除去手法のみでは十分に異常な物体を取り除けておらず，それらを除去した場合としなかった場合で計算したカメラ高さの値に大きな差は生まれなかった．この差を生むように除去手法を改善する．もしくは根本的に異なる手法を提案する．


# O-2: 段階的に学習パイプラインを実装し学習する

## _KR-2a: カメラ高さの一貫性のみを損失に加えた際，学習が失敗する原因を特定し，改善する_
- <details><summary>手法</summary>

    1. detachした深度マップから，物体領域のうちカメラ座標の原点に最も近い点の座標を取得．
    2. これを物体深度の代表点とし，深度-高さの幾何制約式から最尤推定により各フレームのスケールを計算．
    3. フレームごとにカメラ高さを計算．
    4. 3.で計算したカメラ高さをdetachし，2.で計算したスケールによりスケーリング．全フレームに渡り平均を取ってカメラ高さの教師とする．
    5. 3.で計算したカメラ高さの教師として，1エポック前に4.で計算したカメラ高さを利用
  </details>
  
- 2エポック目学習後にカメラ高さが異常に大きな値を取るようになり，学習が失敗していた．
- 考えられる問題点は以下のいずれか
    - 単に実装のミス（挙動の不思議さ的にこれな気はする)
    - 損失に掛ける係数が大きすぎた
    - 損失関数が良くなかった（negative log likelihood）
- 修正する必要があるかもしれない点
    - **monodepth2では，softmax関数によりスケールを正規化された逆深度を出力し，逆深度の逆数（＝深度）の値域(0,1)が(min-depth, max-depth)になるようスケールしたものを最終的な深度マップとしている．min-depth, max-depthは予め定義しておく．スケールフリーな手法では評価時にmedian scalingされるので問題ないが，直接実スケールの深度マップを出力する場合は，問題に成りうる．**
        - max-depthよりも遠い地点が推定できなくなる

### 結果
学習コードに対する以下２点のバグを修正
- 深度マップと道路領域セグメントからカメラ高さを計算する際，`torch.pinverse()`を用いている．この擬似逆行列が正方行列でないと実装上微分可能にならない？ようで，途中でOut of memoryしていた．
    - `torch.pinverse(A)`という形ではなく，`torch.pinverse(A.T @ A)`という形で面倒を見てやることで正方行列にして解決（微分可能でなくなる真の原因はよくわかっていない）
- 異常なまでにカメラ高さが高くなってしまう
    - データセット自体に謎のフリップ処理が入っているものがあり，盛大に学習失敗していただけだった

バグを見つけるのに時間がかかってしまいほとんどこれだけで作業時間を費やしてしまった．．．現在学習中なので，ある程度学習が進み次第，随時評価値を掲載していく．



## _KR-2b: 幾何制約式のみを損失に加えて訓練したモデルの推定結果に現れる，無限に飛ぶ領域がなくなるように修正する_
<table>
<tbody>
  <tr>
    <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/74be2ae5-f4d5-4a58-8f26-d5906883aa86"></td>
    <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/795488e2-2600-4ffb-95cd-10e8a7719352"></td>
    <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/f3303f1b-a620-4d9c-a663-cd6f0ea2a442"></td>
  </tr>
</tbody>
</table>
幾何制約式のみを損失に加えて訓練したモデルでは，無限に深度が飛んでいく帯状領域が発生する．これを修正する案を出す．


#### 学習設定
- 異常な物体は**取り除く**
- 「物体」としては **「車」と「人間」だけ** を扱うことにしている
    - バスやトラック，自転車などはKITTIの3D BBox labelのクラスとして定義されておらず，これらのクラスのheight priorの信頼度が少し低い．height priorの計算手順は以下の通り．
        - OneFormer(COCOで学習)が推定したセグメントとKITTIの2D BBox labelのIoUが閾値を超えた時，COCOのクラスとその物体の高さを紐付ける．これを繰り返すことでCOCOの各クラスに対して複数の物体高さ情報が紐づく．紐付いた高さから平均・分散を計算し，各COCOクラスのheight priorを算出していた．

### 結果
経過なし

-----------------------------------------------------------------------------------------
# 05-26
-----------------------------------------------------------------------------------------
### 前回の総括
- カメラ高さの一貫性のみを損失に加えて訓練
    - <details><summary>手法</summary>

        1. detachした深度マップから，物体領域のうちカメラ座標の原点に最も近い点の座標を取得．
        2. これを物体深度の代表点とし，深度-高さの幾何制約式から最尤推定により各フレームのスケールを計算．
        3. フレームごとにカメラ高さを計算．
        4. 3.で計算したカメラ高さをdetachし，2.で計算したスケールによりスケーリング．全フレームに渡り平均を取ってカメラ高さの教師とする．
        5. 3.で計算したカメラ高さの教師として，1エポック前に4.で計算したカメラ高さを利用
      </details>
    - 学習に失敗していた原因（実装ミス）を特定し再学習させている．

### 今週やることの概要
- カメラ高さの一貫性のみを損失に加えて訓練・評価する
- 異常な物体の検知手法についての改善案を出す


# O-1: 異常な物体の検出手法を確立する

## _KR-1: 学習済みmonodepth2が推定したdepth mapからカメラ高さを計算した値が，より真値に近づくよう，十分に異常な物体を除去する_

最終的に考えているのは，毎エポックor数エポックごとにカメラ高さの計算を繰り返すこと．
その前段階として，一旦学習し終わったモデルでカメラ高さを計算した値が，ある程度真値に近くなっていてほしい．
全てのインスタンスを利用して計算したカメラ高さの値はおよそ2m(真値は1.65m)と，少し真値との乖離が大きい．この乖離を小さくするのを，異常な物体を取り除くことで実現したい．

単純にcross-ratioとRANSACを組み合わせた除去手法のみでは十分に異常な物体を取り除けておらず，それらを除去した場合としなかった場合で計算したカメラ高さの値に大きな差は生まれなかった．この差を生むように除去手法を改善する．もしくは根本的に異なる手法を提案する．


# O-2: 段階的に学習パイプラインを実装し学習する

## _KR-2a: カメラ高さの一貫性のみを損失に加えて訓練・評価_
- <details><summary>手法</summary>

    1. detachした深度マップから，物体領域のうちカメラ座標の原点に最も近い点の座標を取得．
    2. これを物体深度の代表点とし，深度-高さの幾何制約式から最尤推定により各フレームのスケールを計算．
    3. フレームごとにカメラ高さを計算．
    4. 3.で計算したカメラ高さをdetachし，2.で計算したスケールによりスケーリング．全フレームに渡り平均を取ってカメラ高さの教師とする．
    5. 3.で計算したカメラ高さの教師として，1エポック前に4.で計算したカメラ高さを利用
  </details>
- 修正する必要があるかもしれない点
    - **monodepth2では，softmax関数により値域が(0,1)に正規化された逆深度を出力し，逆深度の逆数（＝深度）の値域が(min-depth, max-depth)になるようスケールしたものを最終的な深度マップとしている．min-depth, max-depthは予め定義しておく．スケールフリーな手法では評価時にmedian scalingされるので問題ないが，直接実スケールの深度マップを出力する場合は，問題に成りうる．**
        - max-depthよりも遠い地点が推定できなくなる
        - [Insta-DM](https://sites.google.com/site/seokjucv/home/instadm)では出力層にsoftplus関数を用いて直接的に値を推定している

### 結果
- **問題点**
    - 1エポック目のカメラ高さの損失を設けず学習している時点でおおよそのgeometryは推定できるようになる．これをカメラ高さの損失を加えた2エポック目で，せっかく学習したgeometryをぶち壊し始める
        - 損失関数が変わったことにより最適解が変わるから
        - カメラ高さの一貫性に対する損失に掛ける係数を大きくしすぎる(1.0程度)と，GANのモード崩壊のように，画像平面で一様な深度マップを出力し始め，geometryを完全に無視するようになる．反対に小さくしすぎる(0.01程度)と，geometryはある程度担保されるが，スケールを理解することを放棄し始める．
            - 係数が大きすぎると，道路平面が画像平面と平行であるという無茶苦茶な推定をするようになる
            - 特に道路領域は多少現実的でない深度を推定したとしても，low textureなので reprojection loss は大きくならない

- **原因・やったこと・やろうとしていること**
    - 計算したカメラ高さの分散はかなり小さく，少し期待値から外れた値を予測するとカメラ高さの損失関数として採用している gaussian negative log likelihood が異常に大きな値をとる．
        - 分散がかなり小さくなるのは，全シーンの全フレームで平均を取っており，サンプル数が多いほど分散は小さくなるから．
        - [x] absolute errorで代用
            - こちらのほうが学習がかなり安定する傾向にある．ここからの実験はこちらを採用することとする．ただし依然としてlossがかなり大きな値を取ってしまい，学習に失敗する

    - reprojection loss（最重要のloss）とカメラ高さのloss（camera height loss）のオーダーが違いすぎる．
        - [ ] camera height lossに掛け合わせる係数として，`reprojection_loss.detach() / cam_height_loss.detach()` を採用する
            - cam_height_lossばかりに注目しないようにするという意味では良いが，cam_height_loss < reprojection_loss になった時に別の工夫が無いと，不必要にcam_height_lossに注目して学習し続けてしまう
            - 毎バッチごとに係数が変わるのでloss関数が変動し続けてしまうがそれは良い？
                - [Multi-Loss Weighting with Coefficient of Variations](https://openaccess.thecvf.com/content/WACV2021/papers/Groenendijk_Multi-Loss_Weighting_With_Coefficient_of_Variations_WACV_2021_paper.pdf)(WACV2021)のように動的に係数を決定する手法が提案されているので，そこまで問題ではなさそう
            - １バッチで計算した係数を，続く１０バッチでも同じ値を採用しつづける．などすると多少は安定するかもしれない．
        - **動的にlossに掛ける係数を調整する or そもそもそんなにlossのレンジが広くならないようにゆっくり丁寧に学習する or 非常に大きなカメラ高さになってしまった時，それを修正する方向に力が働くような仕組みを作る** あたりの手法を考えていきたい．

    - monodepth2では `auto_masking` という「カメラの静止時（画像全体）」「カメラと等速で動く領域」「low textureの領域」を自動でマスクする仕組みがある．
        - `auto_masking`  は 時刻 $t$ における入力画像 $I_t$ について，時間的に隣接した画像 $I_t'$ との photometric error(pe) が， $t' \rightarrow t$ へワーピングした画像 $I_{t' \rightarrow t}$ との pe よりも小さくなっているピクセルは，上記の３条件のいずれかに当てはまっている領域であると仮定し，reprojection loss の計算を行わないようにする仕組み．
        - この仕組み上，geometryがむちゃくちゃな推定をし始めると，ほとんどの領域がマスクされるようになり，深度推定の自己教師で最重要な reprojection loss が全く考慮されなくなってしまう．
        - [x] `auto_masking` を一旦取り除いてみる
            - ほぼ効果なし．むしろ少し精度が落ちた

    <!-- - 1エポックごとに，推定した深度マップと深度-高さの幾何制約式から計算・スケールさせたカメラ高さを固定するのではなく，1バッチごとにカメラ高さを更新していったほうが良い？→ 学習中 -->
    - 2エポック目の学習率が大きすぎて，せっかく学習した形状を破壊してしまう
        - [x] 2エポック目で学習率を一旦かなり小さくし，徐々に大きくしていく(fine-tuneするときのwarmupのイメージ)．
            - 2エポック目のみwarmupしたがほぼ意味なし．そもそもcam_height_lossが大きすぎるので，warmupした所でゆっくりcam_height_lossに過学習していくかどうかの違いしか生まれないから？


- **考察・今後の方針**
    - 2エポック目からカメラ高さの損失を追加した途端，2エポック目の学習が終わる時点で出力する深度マップが一様（無茶苦茶な推論）になっている．さすがにここまで崩壊するのは不自然なのでバグを含んでいる可能性あり．
        - 1エポック目の時点である程度形状が学習できている→たとえ2エポック目の開始時に損失関数の関数形状が変化したことで推論結果が崩壊したとしても，2エポック目が終わる頃にはそれが修復可能であるはず．
    - バグがないかを確認するために以下を実行
        1. **1エポック目からカメラ高さのGTを使って教師あり学習した場合に，深度マップが崩壊せずスケールを理解した学習が可能か**
        2. **2エポック目からカメラ高さのGTを使って教師あり学習した場合の挙動を，1. の場合と比較**

<!-- ### メモ
- カメラ高さが導入された2エポック目で基本的に学習は失敗する
    - そもそも勾配計算がうまくいってない？
    - もしくは途中でloss関数が変わるのが相当きつい？
        - 30000枚使って全くスケールがわかるようになってないのはおかしい気がする．
一旦はカメラ高さがずっと一定であるように学習させてそれでうまく行くかどうか確認（まともに学習できているか確認したい）
- 傾向として，1エポック目のカメラ高さの損失を設けず学習している時点でおおよそのgeometryは推定できるようになる．これをカメラ高さの損失を加えた2エポック目で，せっかく学習したgeometryをぶち壊し始める（損失関数が変わったことにより最適解が変わるため）．
    - geometryをぶち壊すことで，「物体領域の深度における深度-高さの幾何関係式が成立しなくなる→カメラ高さのスケールがぐちゃぐちゃになる→カメラ高さに対する損失（negative log likelihood）が異常に大きくなる」という負のループを辿り始める．
- **解決策の方針**
    - reprojection loss（最重要のloss）とカメラ高さのloss（camera height loss）のオーダーが違いすぎる．camera height lossに掛け合わせる係数として，`reprojection_loss.detach() / cam_height_loss.detach()` を採用する
        - adaptiveにlossの係数を決定する．問題として考えられるのは，毎バッチごとに係数が変わるのでloss関数が変動し続けてしまうこと
        - １バッチで計算した係数を，続く１０バッチでも同じ値を採用しつづける．などすると多少は安定するかもしれない．

    - monodepth2では `auto_masking` という「カメラの静止時（画像全体）」「カメラと等速で動く領域」「low textureの領域」を自動でマスクする仕組みがある．これを一旦取り除いてみる→学習中（おそらく本質的に問題なのはlossの大きさが違いすぎるところにあるので，そこまで影響はしなさそう）
        - `auto_masking`  は 時刻 $t$ における入力画像 $I_t$ について，時間的に隣接した画像 $I_t'$ との photometric error(pe) が， $t' \rightarrow t$ へワーピングした画像 $I_{t' \rightarrow t}$ との pe よりも小さくなっているピクセルは，上記の３条件のいずれかに当てはまっている領域であると仮定し，reprojection loss の計算を行わないようにする仕組み．
        - この仕組み上，geometryがむちゃくちゃな推定をし始めると，ほとんどの領域がマスクされるようになり，深度推定の自己教師で最重要な reprojection loss が全く考慮されなくなってしまう．

    - 1エポックごとに，推定した深度マップと深度-高さの幾何制約式から計算・スケールさせたカメラ高さを固定するのではなく，1バッチごとにカメラ高さを更新していったほうが良い？→ 学習中
    - もしくは，2エポック目で学習率を一旦かなり小さくし，徐々に大きくしていく(fine-tuningするときのwarmupのイメージ)．→2エポック目のみwarmupしたがほぼ意味なし．そもそもcam_height_lossが大きすぎるので，warmupした所でゆっくりcam_height_lossに過学習していくかどうかの違いしか生まれないから？
    - 計算したカメラ高さの分散はかなり小さく，少し期待値から外れた値を予測するとnegative log likelihoodは異常に大きな値になってしまう．一旦absolute errorで代用してみて，loss関数の選択の悪さがどれくらい悪影響を与えているのか見る．→学習中．こちらのほうが学習がかなり安定する傾向．
        - 分散がかなり小さくなるのは，全シーンの全フレームで平均を取っており，サンプル数が多いほど分散は小さくなるから．

- カメラ高さの一貫性に対する損失に掛ける係数を大きくしすぎる(1.0程度)と，GANのモード崩壊のように，画像平面で一様な深度マップを出力し始め，geometryを完全に無視するようになる．反対に小さくしすぎる(0.01程度)と，geometryはある程度担保されるが，スケールを理解することを放棄し始める．
    - 係数が大きすぎると，道路平面が画像平面と平行であるという無茶苦茶な推定をするようになる
    - 特に道路領域は多少現実的でない深度を推定したとしても，low textureなので，reprojection lossは大きくならない（と思われる）．
    - lossのレンジが非常に広いのが問題．**動的にlossに掛ける係数を調整する or そもそもそんなにlossのレンジが広くならないようにゆっくり丁寧に学習する or 非常に大きなカメラ高さになってしまった時，それを修正する方向に力が働くような仕組みを作る**

- 物体領域の平均深度-高さの粗い幾何的制約も損失関数として設定すると1エポック目の時点である程度スケールを考慮した推定結果が得られているので，これを導入すると問題は解決するかもしれない．
    - 損失関数の関数形状がそこまで変化しなくなる（ように思える） -->

## _KR-2b: 幾何制約式のみを損失に加えて訓練したモデルの推定結果に現れる，無限に飛ぶ領域がなくなるように修正する_
<table>
<tbody>
  <tr>
    <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/74be2ae5-f4d5-4a58-8f26-d5906883aa86"></td>
    <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/795488e2-2600-4ffb-95cd-10e8a7719352"></td>
    <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/f3303f1b-a620-4d9c-a663-cd6f0ea2a442"></td>
  </tr>
</tbody>
</table>
幾何制約式のみを損失に加えて訓練したモデルでは，無限に深度が飛んでいく帯状領域が発生する．これを修正する案を出す．

#### 学習設定
- 異常な物体は**取り除く**
- 「物体」としては **「車」と「人間」だけ** を扱うことにしている
    - バスやトラック，自転車などはKITTIの3D BBox labelのクラスとして定義されておらず，これらのクラスのheight priorの信頼度が少し低い．height priorの計算手順は以下の通り．
        - OneFormer(COCOで学習)が推定したセグメントとKITTIの2D BBox labelのIoUが閾値を超えた時，COCOのクラスとその物体の高さを紐付ける．これを繰り返すことでCOCOの各クラスに対して複数の物体高さ情報が紐づく．紐付いた高さから平均・分散を計算し，各COCOクラスのheight priorを算出していた．


-----------------------------------------------------------------------------------------
# 06-05
-----------------------------------------------------------------------------------------
### 前回の総括
2エポック目からカメラ高さの一貫性のみを損失に加えて訓練
- <details><summary>手法</summary>

    1. detachした深度マップから，物体領域のうちカメラ座標の原点に最も近い点の座標を取得．
    2. これを物体深度の代表点とし，深度-高さの幾何制約式から最尤推定により各フレームのスケールを計算．
    3. フレームごとにカメラ高さを計算．
    4. 3.で計算したカメラ高さをdetachし，2.で計算したスケールによりスケーリング．全フレームに渡り平均を取ってカメラ高さの教師とする．
    5. 3.で計算したカメラ高さの教師として，1エポック前に4.で計算したカメラ高さを利用
  </details>

- 1エポック目のカメラ高さの損失を設けず学習している時点でおおよそのgeometryは推定できるようになる．これをカメラ高さの損失を加えた2エポック目で，せっかく学習したgeometryをぶち壊し始める．

    - 損失関数に掛け合わせる係数を大きくすると，道路平面が画像平面と平行になるような無茶苦茶な深度を推定するようになる．小さくするとgeometryを担保した推定を行うが，スケールの学習を放棄する
    - 2エポック目からのwarmupなど色々工夫したが，あまり効果はなかった．

### 今週やることの概要
- 2エポック目からカメラ高さの損失を追加した途端，2エポック目の学習が終わる時点で出力する深度マップがgeometryを無視した無茶苦茶な推論をする．さすがにここまで崩壊するのは不自然なのでバグを含んでいる可能性あり．

    - 1エポック目の時点である程度形状が学習できている→たとえ2エポック目の開始時に損失関数の関数形状が変化したことで推論結果が崩壊したとしても，2エポック目が終わる頃にはそれが修復可能であるはず．

- バグがないかを確認するために以下を実行

    1. **1エポック目からカメラ高さのGTを使って教師あり学習した場合に，深度マップが崩壊せずスケールを理解した学習が可能か**

    2. **2エポック目からカメラ高さのGTを使って教師あり学習した場合の挙動を，1. の場合と比較**



# O-1: 異常な物体の検出手法を確立する

## _KR-1: 学習済みmonodepth2が推定したdepth mapからカメラ高さを計算した値が，より真値に近づくよう，十分に異常な物体を除去する_

最終的に考えているのは，毎エポックor数エポックごとにカメラ高さの計算を繰り返すこと．
その前段階として，一旦学習し終わったモデルでカメラ高さを計算した値が，ある程度真値に近くなっていてほしい．
全てのインスタンスを利用して計算したカメラ高さの値はおよそ2m(真値は1.65m)と，少し真値との乖離が大きい．この乖離を小さくするのを，異常な物体を取り除くことで実現したい．

単純にcross-ratioとRANSACを組み合わせた除去手法のみでは十分に異常な物体を取り除けておらず，それらを除去した場合としなかった場合で計算したカメラ高さの値に大きな差は生まれなかった．この差を生むように除去手法を改善する．もしくは根本的に異なる手法を提案する．


# O-2: 段階的に学習パイプラインを実装し学習する

## _KR-2a: カメラ高さの一貫性のみを損失に加えて訓練・評価_
- <details><summary>手法</summary>

    1. detachした深度マップから，物体領域のうちカメラ座標の原点に最も近い点の座標を取得．
    2. これを物体深度の代表点とし，深度-高さの幾何制約式から最尤推定により各フレームのスケールを計算．
    3. フレームごとにカメラ高さを計算．
    4. 3.で計算したカメラ高さをdetachし，2.で計算したスケールによりスケーリング．全フレームに渡り平均を取ってカメラ高さの教師とする．
    5. 3.で計算したカメラ高さの教師として，1エポック前に4.で計算したカメラ高さを利用
  </details>
- 修正する必要があるかもしれない点
    - **monodepth2では，softmax関数により値域が(0,1)に正規化された逆深度を出力し，逆深度の逆数（＝深度）の値域が(min-depth, max-depth)になるようスケールしたものを最終的な深度マップとしている．min-depth, max-depthは予め定義しておく．スケールフリーな手法では評価時にmedian scalingされるので問題ないが，直接実スケールの深度マップを出力する場合は，問題に成りうる．**
        - max-depthよりも遠い地点が推定できなくなる
        - [Insta-DM](https://sites.google.com/site/seokjucv/home/instadm)では出力層にsoftplus関数を用いて直接的に値を推定している

- **問題点**
    - 1エポック目のカメラ高さの損失を設けず学習している時点でおおよそのgeometryは推定できるようになる．これをカメラ高さの損失を加えた2エポック目で，せっかく学習したgeometryをぶち壊し始める
        - 損失関数が変わったことにより最適解が変わるから

        - カメラ高さの一貫性に対する損失に掛ける係数を大きくしすぎる(1.0程度)と，GANのモード崩壊のように，画像平面で一様な深度マップを出力し始め，geometryを完全に無視するようになる．反対に小さくしすぎる(0.01程度)と，geometryはある程度担保されるが，スケールを理解することを放棄し始める．
            - 係数が大きすぎると，道路平面が画像平面と平行であるという無茶苦茶な推定をするようになる

            - 特に道路領域は多少現実的でない深度を推定したとしても，low textureなので reprojection loss は大きくならない

- **まずやること**
    - 2エポック目からカメラ高さの損失を追加した途端，2エポック目の学習が終わる時点で出力する深度マップが一様（無茶苦茶な推論）になっている．さすがにここまで崩壊するのは不自然なのでバグを含んでいる可能性あり．

        - 1エポック目の時点である程度形状が学習できている→たとえ2エポック目の開始時に損失関数の関数形状が変化したことで推論結果が崩壊したとしても，2エポック目が終わる頃にはそれが修復可能であるはず．

    - バグがないかを確認するために以下を実行
        1. **1エポック目からカメラ高さのGTを使って教師あり学習した場合に，深度マップが崩壊せずスケールを理解した学習が可能か**
        
        2. **2エポック目からカメラ高さのGTを使って教師あり学習した場合の挙動を，1. の場合と比較**

- <details><summary> **原因・やったこと** </summary>

    - 計算したカメラ高さの分散はかなり小さく，少し期待値から外れた値を予測するとカメラ高さの損失関数として採用している gaussian negative log likelihood が異常に大きな値をとる．

        - 分散がかなり小さくなるのは，全シーンの全フレームで平均を取っており，サンプル数が多いほど分散は小さくなるから．

        - [x] absolute errorで代用
            - こちらのほうが学習がかなり安定する傾向にある．ここからの実験はこちらを採用することとする．ただし依然としてlossがかなり大きな値を取ってしまい，学習に失敗する

    - monodepth2では `auto_masking` という「カメラの静止時（画像全体）」「カメラと等速で動く領域」「low textureの領域」を自動でマスクする仕組みがある．

        - `auto_masking`  は 時刻 $t$ における入力画像 $I_t$ について，時間的に隣接した画像 $I_t'$ との photometric error(pe) が， $t' \rightarrow t$ へワーピングした画像 $I_{t' \rightarrow t}$ との pe よりも小さくなっているピクセルは，上記の３条件のいずれかに当てはまっている領域であると仮定し，reprojection loss の計算を行わないようにする仕組み．

        - この仕組み上，geometryがむちゃくちゃな推定をし始めると，ほとんどの領域がマスクされるようになり，深度推定の自己教師で最重要な reprojection loss が全く考慮されなくなってしまう．

        - [x] `auto_masking` を一旦取り除いてみる
            - ほぼ効果なし．むしろ少し精度が落ちた

    <!-- - 1エポックごとに，推定した深度マップと深度-高さの幾何制約式から計算・スケールさせたカメラ高さを固定するのではなく，1バッチごとにカメラ高さを更新していったほうが良い？→ 学習中 -->
    - 2エポック目の学習率が大きすぎて，せっかく学習した形状を破壊してしまう

        - [x] 2エポック目で学習率を一旦かなり小さくし，徐々に大きくしていく(fine-tuneするときのwarmupのイメージ)．

            - 2エポック目のみwarmupしたがほぼ意味なし．そもそもcam_height_lossが大きすぎるので，warmupした所でゆっくりcam_height_lossに過学習していくかどうかの違いしか生まれないから？
    </details>

- **試してみたいこと**
    - reprojection loss（最重要のloss）とカメラ高さのloss（camera height loss）のオーダーが違いすぎる．

    - [ ] camera height lossに掛け合わせる係数として，`reprojection_loss.detach() / cam_height_loss.detach()` を採用する

        - cam_height_lossばかりに注目しないようにするという意味では良いが，cam_height_loss < reprojection_loss になった時に別の工夫が無いと，不必要にcam_height_lossに注目して学習し続けてしまう

        - 毎バッチごとに係数が変わるのでloss関数が変動し続けてしまうがそれは良い？
            - [Multi-Loss Weighting with Coefficient of Variations](https://openaccess.thecvf.com/content/WACV2021/papers/Groenendijk_Multi-Loss_Weighting_With_Coefficient_of_Variations_WACV_2021_paper.pdf)(WACV2021)のように動的に係数を決定する手法が提案されているので，そこまで問題ではなさそう

        - １バッチで計算した係数を，続く１０バッチでも同じ値を採用しつづける．などすると多少は安定するかもしれない．

    - **動的にlossに掛ける係数を調整する or そもそもそんなにlossのレンジが広くならないようにゆっくり丁寧に学習する or 非常に大きなカメラ高さになってしまった時，それを修正する方向に力が働くような仕組みを作る** あたりの手法を考えていきたい．

### 結果 (デバッグ)
カメラ高さの一貫性に対する損失を2エポック目から追加した途端，不自然なほどに深度マップが崩壊するバグを修正
- 道路平面の方程式を計算する際に pseudo inverseを計算するが，その計算をdetachせずに計算していたのが良くなかった

- 数式上はdetachすべきではないはずだが，pseudo inverseの微分値が非常に複雑なせいで勾配爆発していたのが原因？
    - [VADepth](https://github.com/xjixzz/vadepth-net)(RA-L2022) ではdetachしていたのでそれを参考にした

### 結果（デバッグ後の学習）
上記のバグを修正後，以下の4パターンで学習を進めた
- ①: [VADepth](https://github.com/xjixzz/vadepth-net)(RA-L2022)と同様の係数を設定し，1エポック目からカメラ高さの真値を用いてmonodepth2を教師あり学習
    - [VADepth](https://github.com/xjixzz/vadepth-net)はカメラ高さのみの真値を用いて教師あり学習するモデル．道路領域は特定の矩形領域を決め打ち．
    - VADepthとは違い，今回の学習ではOneFormerのsemantic segmentationにより推定した道路領域を用いてそこからカメラ高さを計算
    
- ②: VADepthと同様の係数を設定し，学習と同時に幾何的に計算したカメラ高さを，2エポック目以降の教師としてmonodepth2を学習

- ③: ②と同じ問題設定で，前エポックで計算したカメラ高さがその前に計算したカメラ高さの2倍以上の値ならばカメラ高さを更新せずに学習

- ④: ③と同じ問題設定で，カメラ高さに関する係数を0.01から0.05に増やして学習

①は問題なく学習できた．オリジナルのmonodepth2では，1エポック目の学習終了時点でおおよそのgeometryを学習するが，①ではその学習に2エポック以上費やす傾向がある．

<table>
    <tr>
        <td>original(after 1 epoch)</td>
        <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/2339aeeb-d27b-4095-9781-0816732dbc27"></td>
    </tr>
    <tr>
        <td>①(after 1 epoch)</td>
        <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/c9bd19cd-bb71-43c5-b101-203d10892f38"></td>
    </tr>
    <tr>
        <td>①(after 2 epoch)</td>
        <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/2844110d-2b64-4b9a-a34a-9010e99c1f50"></td>
    </tr>
    <tr>
        <td>①(after 20 epoch)</td>
        <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/f782158c-f9d8-43b1-8064-2f2bfe8e9a84"></td>
    </tr>
</table>

②の学習では，カメラ高さ損失を導入して初期のエポックでgeometryを若干崩し始める（これは①と同じ傾向）．このgeometryを崩している期間で計算したカメラ高さの値がかなり大きくなり，学習が不安定になってしまう．そのため，前エポックで計算したカメラ高さがその前に計算したカメラ高さの2倍以上の値ならばカメラ高さを更新せずに学習してみた（③）．

③では，道路領域のみ実スケールに近付こうと学習するが，その影響が道路以外の領域まで波及しない．結果，①の1エポック後からgeometryのみが整った深度マップを出力するようになる．</br>
①と③の違いは，カメラ高さのlossが1エポック目から入ってるかどうか（カメラ高さの値は異なるが，そのスケールに合わせた深度が出力されるだけで深度マップのgeometryが変わるわけではない）．**③では1エポック目の学習時点で，スケールフリーな深度を出力するローカルオプティマムに入ってしまっており，ここから抜け出せてないため画像全体のスケールが実スケールに近づかないのでは？**
これは予想だが，ego-motion netは道路以外の領域からego-motionを推定しており，ego-motionのスケールは，スケールフリーのままになっている．そのため道路領域のreprojection errorは大きな値を取るはずだが，**道路領域はlow-textureなので，道路領域におけるreprojection errorの値はそこまで大きくならない**と思われる．

<table>
    <tr>
        <td>③(after 20 epoch)</td>
        <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/0e8e5f87-152f-4ba2-9ce5-28d299e22c10"></td>
    </tr>
</table>

③の結果から，カメラ高さ損失に掛け合わせる係数を大きくすれば，スケールに対する制約がきつくなり，その影響が全体にも及ぶかもしれないと思ったが，下記の図の通り，深度マップがぼやけるだけの結果に終わった．reprojection errorが軽視されてしまうのは良くないっぽい．

<table>
    <tr>
        <td>④(after 20 epoch)</td>
        <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/4ef9f279-fad0-4db3-87d7-df1f7bbf1ad6"></td>
    </tr>
</table>

以上の結果から，
1. カメラ高さの係数は0.01程度が良い
2. 1エポック目の学習でハマってしまったスケールフリーのlocal optimumから抜けだす必要がある

と言える．2.は，**1エポック目の学習終了後に重みとoptimizerを初期化すれば**解決しそう．もしかすると**物体領域深度の平均値を用いた粗いスケールに関するlossを導入することで，特に工夫なしでも解決できるかも**しれない．


-----------------------------------------------------------------------------------------
# 06-12
-----------------------------------------------------------------------------------------
### 前回の総括
2エポック目からカメラ高さの一貫性のみを損失に加えて訓練
- <details><summary>手法</summary>

    1. detachした深度マップから，物体領域のうちカメラ座標の原点に最も近い点の座標を取得．
    2. これを物体深度の代表点とし，深度-高さの幾何制約式から最尤推定により各フレームのスケールを計算．
    3. フレームごとにカメラ高さを計算．
    4. 3.で計算したカメラ高さをdetachし，2.で計算したスケールによりスケーリング．全フレームに渡り平均を取ってカメラ高さの教師とする．
    5. 3.で計算したカメラ高さの教師として，1エポック前に4.で計算したカメラ高さを利用
  </details>

- カメラ高さの損失を設定すると，出力するdepthマップが急に崩壊するバグを修正
    - 道路平面の方程式の計算時に `torch.pinverse` を使う箇所があるが，これをdetachすべきだった
    - 数式的にはdetachしない方が良いはずだが，おそらくpseudo inverseの微分が複雑で，勾配爆発してしまうっぽい．[VADepth](https://github.com/xjixzz/vadepth-net)(RA-L2022)でもdetachしていた．

- デバッグ後，以下の４パターンで学習
    - ①: [VADepth](https://github.com/xjixzz/vadepth-net)(RA-L2022)と同様の係数を設定し，1エポック目からカメラ高さの真値を用いてmonodepth2を教師あり学習
        - [VADepth](https://github.com/xjixzz/vadepth-net)はカメラ高さのみの真値を用いて教師あり学習するモデル．道路領域は特定の矩形領域を決め打ち．
        - VADepthとは違い，今回の学習ではOneFormerのsemantic segmentationにより推定した道路領域を用いてそこからカメラ高さを計算
        
    - ②: VADepthと同様の係数を設定し，学習と同時に幾何的に計算したカメラ高さを，2エポック目以降の教師としてmonodepth2を学習

    - ③: ②と同じ問題設定で，前エポックで計算したカメラ高さがその前に計算したカメラ高さの2倍以上の値ならばカメラ高さを更新せずに学習

    - ④: ③と同じ問題設定で，カメラ高さに関する係数を0.01から0.05に増やして学習

- <details><summary>結果</summary>

    ①は問題なく学習できた．オリジナルのmonodepth2では，1エポック目の学習終了時点でおおよそのgeometryを学習するが，①ではその学習に2エポック以上費やす傾向がある．

    <table>
        <tr>
            <td>original(after 1 epoch)</td>
            <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/2339aeeb-d27b-4095-9781-0816732dbc27"></td>
        </tr>
        <tr>
            <td>①(after 1 epoch)</td>
            <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/c9bd19cd-bb71-43c5-b101-203d10892f38"></td>
        </tr>
        <tr>
            <td>①(after 2 epoch)</td>
            <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/2844110d-2b64-4b9a-a34a-9010e99c1f50"></td>
        </tr>
        <tr>
            <td>①(after 20 epoch)</td>
            <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/f782158c-f9d8-43b1-8064-2f2bfe8e9a84"></td>
        </tr>
    </table>

    ②の学習では，カメラ高さ損失を導入して初期のエポックでgeometryを若干崩し始める（これは①と同じ傾向）．このgeometryを崩している期間で計算したカメラ高さの値がかなり大きくなり，学習が不安定になってしまう．そのため，前エポックで計算したカメラ高さがその前に計算したカメラ高さの2倍以上の値ならばカメラ高さを更新せずに学習してみた（③）．

    ③では，道路領域のみ実スケールに近付こうと学習するが，その影響が道路以外の領域まで波及しない．結果，①の1エポック後からgeometryのみが整った深度マップを出力するようになる．</br>
    ①と③の違いは，カメラ高さのlossが1エポック目から入ってるかどうか（カメラ高さの値は異なるが，そのスケールに合わせた深度が出力されるだけで深度マップのgeometryが変わるわけではない）．**③では1エポック目の学習時点で，スケールフリーな深度を出力するローカルオプティマムに入ってしまっており，ここから抜け出せてないため画像全体のスケールが実スケールに近づかないのでは？**
    これは予想だが，ego-motion netは道路以外の領域からego-motionを推定しており，ego-motionのスケールは，スケールフリーのままになっている．そのため道路領域のreprojection errorは大きな値を取るはずだが，**道路領域はlow-textureなので，道路領域におけるreprojection errorの値はそこまで大きくならない**と思われる．

    <table>
        <tr>
            <td>③(after 20 epoch)</td>
            <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/0e8e5f87-152f-4ba2-9ce5-28d299e22c10"></td>
        </tr>
    </table>

    ③の結果から，カメラ高さ損失に掛け合わせる係数を大きくすれば，スケールに対する制約がきつくなり，その影響が全体にも及ぶかもしれないと思ったが，下記の図の通り，深度マップがぼやけるだけの結果に終わった．reprojection errorが軽視されてしまうのは良くないっぽい．

    <table>
        <tr>
            <td>④(after 20 epoch)</td>
            <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/4ef9f279-fad0-4db3-87d7-df1f7bbf1ad6"></td>
        </tr>
    </table>
    </details>

- わかったこと
    1. カメラ高さの係数は0.01程度が良い
    2. 1エポック目の学習でハマってしまったスケールフリーのlocal optimumから抜けだす必要がある

    2.は，**1エポック目の学習終了後にネットワークの重みとoptimizerを初期化**すれば解決しそう．もしかすると**物体領域深度の平均値を用いた粗いスケールに関するlossを導入することで，特に工夫なしでも解決できるかも**しれない．


### 今週やることの概要
- [x] 1エポック目学習後にネットワーク・adamを再度初期化し，2エポック目以降は，前エポックで計算したカメラ高さを教師にして学習を進める
- [x] カメラ高さの一貫性に加え，物体領域深度を平均値で代表して計算した深度-高さの幾何的制約による損失を設けて学習させる（1エポック目学習後にネットワーク・adamは初期化しない）


# O-1: 段階的に学習パイプラインを実装し学習する

## _KR-1a: カメラ高さの一貫性のみを損失に加えた訓練が，1エポック目後にAdamと重みを初期化することで，geometryを維持しつつ深度マップ全体がスケール的に一貫性を持った深度マップを出力できることを確認する_
- <details><summary>手法</summary>

    1. detachした深度マップから，物体領域のうちカメラ座標の原点に最も近い点の座標を取得．
    2. これを物体深度の代表点とし，深度-高さの幾何制約式から最尤推定により各フレームのスケールを計算．
    3. フレームごとにカメラ高さを計算．
    4. 3.で計算したカメラ高さをdetachし，2.で計算したスケールによりスケーリング．全フレームに渡り平均を取ってカメラ高さの教師とする．
    5. 3.で計算したカメラ高さの教師として，1エポック前に4.で計算したカメラ高さを利用
  </details>
- <details><summary>修正する必要があるかもしれない点</summary>

    - **monodepth2では，softmax関数により値域が(0,1)に正規化された逆深度を出力し，逆深度の逆数（＝深度）の値域が(min-depth, max-depth)になるようスケールしたものを最終的な深度マップとしている．min-depth, max-depthは予め定義しておく．スケールフリーな手法では評価時にmedian scalingされるので問題ないが，直接実スケールの深度マップを出力する場合は，問題に成りうる．**
        - max-depthよりも遠い地点が推定できなくなる
        - [Insta-DM](https://sites.google.com/site/seokjucv/home/instadm)では出力層にsoftplus関数を用いて直接的に値を推定している
  </details>

### 経過
- [x] 1エポック目後にadamと重み初期化→2エポック目以降は前エポックで計算したカメラ高さから，カメラ高さの一貫性の損失を設定
- [x] 1エポック目後にadamと重みを初期化→2,3エポック目はカメラ高さを更新せず1エポック目で計算したカメラ高さを利用して，カメラ高さの一貫性の損失を設定（4エポック目以降はカメラ高さを更新し続ける）

### 結果
1. 「1エポック目後にadamと重み初期化→2エポック目以降は前エポックで計算したカメラ高さから，カメラ高さの一貫性の損失を設定」を行うことにより，1エポック目の学習でハマってしまったスケールフリーのlocal optimumから抜けだすことはある程度できた（＝道路領域のみスケールを合わせようとするのを多少避けれた）
    - <img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/057b081d-678a-4cef-b404-af2af1546477" width=80%>

        - <span style="color:blue">青線</span>が1エポック目後の初期化**なし**．<span style="color:cyan">水色</span>の線が初期化**あり**

    - <img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/a986eee8-0fd5-4a14-a0af-2dff02b83903" width=20%>

        - 初期化なしver.では，カメラ高さがエポックごとにかなりブレてしまって学習が困難だったので「カメラ高さが前エポックの計算結果よりも2倍以上の値になった場合更新しない」という制約を加えている
        - GTのカメラ高さは 1.65 m なのでもっと小さくなってほしい．
    
    - カメラ高さの図の<span style="color:cyan">初期化ありver.</span>を見ると，最初にカメラ高さを更新した際に，大きな値に更新してしまっている．[VADepth](https://github.com/xjixzz/vadepth-net)(RA-L2022)の学習では，大まかなgeometryの学習に2エポック以上費やしていたことからも，このgeometryを学習する期間では擬似教師として用いているカメラ高さは安定させておくべき？

2. 1.の結果から，カメラ高さの一貫性の損失を加え始めてから2エポック程度はカメラ高さを更新すべきではないのではと仮定できる．まだ学習途中だが結果は以下の通り．
    - <img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/2f62e30a-a4e0-4743-b719-ce42e0b71196" width=80%>

        - <span style="color:red">赤線</span>は1エポック目後重み/adamを初期化し，その後の2エポックはカメラ高さを更新しなかった時

    - <img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/e5855970-67f3-4299-bed9-e7e9bbc17460" width=20%>

        - 精度向上しカメラ高さもより安定している（＝真値に近づいていってる）
        - しかし，カメラ高さの値自体が真値と2倍近く離れているため，a1・a2値がまだまだ低い．カメラ高さがより真値に近づくような工夫が必要．

### 思ったこと
- 現在はカメラ高さの初期値として1エポック目の学習で計算したカメラ高さの値を計算しているが，この値が真値からかなり乖離している．1エポック目後に初期化するのなら，**学習済みmonodepth2で計算したより真値に近い値を初期値**として採用しても良い気がしてきた．
    - もちろん学習の手間がないという意味では前者の方が良い．
    - 初期値がボトルネックになっているなら変更するくらいの心持ちでいとく．

- 学習済みのオリジナルのmonodepth2が出力する深度マップから道路領域の法線ベクトルを計算し，そこから計算したhorizonとGTのhorizonを見比べると，特にGTの傾きが大きい時に差が大きくなる．つまりmonodepth2は**道路領域の法線ベクトルはほぼy軸方向であると推定しがち**．
    <img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/cb2c1749-a09b-4251-b6d8-b88f14bc285e" width=40%>
    - これにより計算するカメラ高さの値が不正確になっている？
    - **道路平面の傾きを理解した推定が可能になるように，損失を加える or Data Augmentation手法を考える**

## _KR-1b: カメラ高さの一貫性と，物体領域深度を平均値で代表して計算した深度-高さの粗い幾何的制約を損失として加えて訓練した場合の出力結果を確認する_
- <details><summary>手法（カメラ高さの一貫性）</summary>

    1. detachした深度マップから，物体領域のうちカメラ座標の原点に最も近い点の座標を取得．
    2. これを物体深度の代表点とし，深度-高さの幾何制約式から最尤推定により各フレームのスケールを計算．
    3. フレームごとにカメラ高さを計算．
    4. 3.で計算したカメラ高さをdetachし，2.で計算したスケールによりスケーリング．全フレームに渡り平均を取ってカメラ高さの教師とする．
    5. 3.で計算したカメラ高さの教師として，1エポック前に4.で計算したカメラ高さを利用
  </details>

### 結果
深度-高さの粗い幾何制約を加えたことで，KR-1cの図のように，やはり無限に飛ぶ領域が現れ，結果として精度が悪くなる．
<img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/e6a0c7d3-9d5e-4bed-a9da-d46dc2f0c306" width=100%>

深度マップはひどいが，精度はそこまで悪くないという謎．
<img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/17700402-1e3d-4030-96b3-02607ce22cbc" width=80%>


### 今後やること
- 一通り見た感じは問題なさそうだったが，学習パイプラインにバグがないかを確認する．
- バグが無いことを確認したら，このように無限に飛ぶ領域が発生する原因を探る



-----------------------------------------------------------------------------------------
# 06-19
-----------------------------------------------------------------------------------------
### 前回の総括
- 「1エポック目学習後にadam/重みを初期化→2エポック目以降は前エポックで計算したカメラ高さから，カメラ高さの一貫性の損失を設定し学習」することで，学習が安定し，精度も向上．

- さらに，[VADepth](https://github.com/xjixzz/vadepth-net)(RA-L2022)では，大まかなgeometryの学習に2エポック以上費やしていたことから類推し，1エポック目学習後の2エポックはカメラ高さを更新しないことにより，更に学習が安定した．

- やはり計算したカメラ高さが真値から倍ほど大きいため，どうしても精度が頭打ちになる．
    - カメラ高さの計算をする際に，物体の深度の代表値として Max ではなく[DESC: Domain Adaptation for Depth Estimation via Semantic Consistency, BMVC2020(oral)](https://github.com/kyotovision/kinoshita_genki/issues/13) に習って Median を取るとカメラ高さがより真値に近くなり，精度が向上した．
    - 「真値に近くなった」というよりも，単に「物体の代表深度が大きくなる→スケール係数が小さくなりカメラ高さが低くなった結果，真値に近くなった」だけなので，本質的ではない．現状ではカメラ高さが真値よりも大きく計算されがちなので，プラスに働いているだけ．

- syntheticなデータセットを使ってカメラ高さ推定器を学習させ，単眼深度推定のpseudo-labelに利用する論文を読んだ([DESC: Domain Adaptation for Depth Estimation via Semantic Consistency, BMVC2020(oral)](https://github.com/kyotovision/kinoshita_genki/issues/13))

### 今週やることの概要

- [x] 幾何制約式のみを損失に加えて訓練したモデルの推定結果に現れる，無限に飛ぶ領域がなくなるように修正する．
    - 本当に，物体深度を平均で代表した幾何制約式による損失が学習に有用なのか，立ち返って考える必要もある．当初想定していたよりも，カメラ高さのブレが学習の難化につながることがわかったが，その観点から見てもこの手法は有用なのか？

- [ ] カメラ高さの一貫性の損失のみを設定した学習の後半で，道路領域が平面的でない推定を行い始める原因を特定する


- [ ] 道路平面の傾きを理解した推定が可能になるように，損失を加える or Data Augmentation手法を考える
    - 学習済みのオリジナルのmonodepth2が出力する深度マップから道路領域の法線ベクトルを計算し，そこから計算したhorizonとGTのhorizonを見比べると，特にGTの傾きが大きい時に差が大きくなる．つまりmonodepth2は**道路領域の法線ベクトルはほぼy軸方向であると推定しがち**．
        <img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/cb2c1749-a09b-4251-b6d8-b88f14bc285e" width=40%>
        - これが原因となり，計算するカメラ高さの値が不正確になっているのかも．


# O-1: 段階的に学習パイプラインを実装し学習する

## _KR-1a: 幾何制約式のみを損失に加えて訓練したモデルの推定結果に現れる，無限に飛ぶ領域がなくなるように修正する_
<table>
<tbody>
  <tr>
    <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/74be2ae5-f4d5-4a58-8f26-d5906883aa86"></td>
    <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/795488e2-2600-4ffb-95cd-10e8a7719352"></td>
    <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/f3303f1b-a620-4d9c-a663-cd6f0ea2a442"></td>
  </tr>
</tbody>
</table>
幾何制約式のみを損失に加えて訓練したモデルでは，無限に深度が飛んでいく帯状領域が発生する．これを修正する案を出す．

### 結果
**修正前：**  物体領域の深度を平均し，それを元に幾何制約式から物体の高さを決定した後，その値と事前知識の物体高さについてのgaussian negative log likelihoodを損失とする

**修正後：**  物体領域の深度の各点について，幾何制約式から物体の高さを決定する．修正前では１物体につき唯一の高さを決定したが，修正後では１物体の各ピクセルごとに１つずつ高さを計算する．各ピクセルの深度を元に計算した高さと事前知識の物体高さのAbsolute Errorを損失とする．

この修正により，各ピクセルについて均等に教師信号が行き渡るようになり，無限に深度が飛ぶ（＝物体領域の一部だけでスケール合わせをするような挙動）ことを回避できた．
精度も非常に良くカメラ高さのGTラベルを用いた教師あり学習モデルとcompetitive（下のグラフ）．

<span style="color:red">赤色</span>： Monodepth2 supervised with GT camera height</br>
<span style="color:cyan">水色</span>： Monodepth2に幾何制約式による損失のみを加えて学習
<table>
    <tr>
        <td>higher is better</td>
        <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/f548cb58-de29-461f-ae2d-8f18cfe9bc7e" width=80%></td>
    </tr>
    <tr>
        <td>lower is better</td>
        <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/d543396b-8cb9-43d4-8745-c0cc05eb3d03" width=80%></td>
    </tr>
</table>

問題として残っているのは，
1. **物体領域が平面的になる**
    - 教師信号の与え方的にこれが最適解になってしまう
      ![image](https://github.com/kyotovision/kinoshita_genki/assets/54442538/8b52ba12-3808-40f0-af35-753710f7dc20)

2. **各物体で独立したスケールに関する教師を与えているため，右下の車のような見切れ物体では，現実よりも遠くに物体があるような不正確な教師信号を伝えてしまう**
    - これは異常な物体を取り除くことにより回避したい
      ![image](https://github.com/kyotovision/kinoshita_genki/assets/54442538/498543af-d31a-4f80-9737-23869cf18a43)
    - 車の領域はガラス部が多く，LiDARの取得点がスパース．そのためこれを改善しても定量的に性能は大して変化しなそうだが，定性評価としては重要．

上記の問題の解決策を探るため，幾何制約式による雑な損失に掛ける係数を学習が進むに連れて徐々に小さくし，カメラ高さの一貫性に対する損失に掛ける係数を大きくして学習をしてみた．メリットと課題は以下の通り．

<span style="font-size:20px">:+1:</span>
- 学習初期では物体領域が平面になるが，徐々に物体形状を取り戻していく

- 学習初期では見切れ物体を現実よりも遠くにあるように推定してしまうが，適切な３次元位置に修正される

<table>
    <tr>
        <th>epoch 10</th>
        <th>→</th>
        <th>epoch 20</th>
    </tr>
    <tr>
        <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/bb81e71c-e5b0-4a30-9295-22c1465d43c1"></td>
        <td></td>
        <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/e6d2cf59-cbd0-4044-b020-4ddc21dc72f0"></td>
    </tr>
    <tr>
        <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/9c0c02be-92c5-4456-8ecc-3081b53e1166"></td>
        <td> </td>
        <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/fb822bcb-01f5-4b10-9469-31665b0849b5"></td>
    </tr>
</table>


<span style="font-size:20px">:-1:</span>
- カメラ高さが徐々に高くなるように計算されてしまう

    - 見切れ物体が徐々に近くになるように学習が進み，その結果スケール係数が大きくなるように計算されてしまう．おそらくこれが原因．
    - 見切れ物体などの「異常な物体」をきちんと取り除ければ解決する問題

- 道路平面が不安定になり，遠くにある道路が逆に近くなるような推定がなされる場合がある

    - 部分的にはカメラ高さが不安定なことが原因（カメラ高さを一定に固定した場合，マシになる）
        - ただこれが唯一の理由ではないので，原因を特定する必要あり

### やるべきこと
- 異常な物体を取り除く手法を考案する．


## _KR-1b: カメラ高さの一貫性の損失のみを設定した学習の後半で，道路領域が平面的でない推定を行い始める原因を特定する_
- <details><summary>手法</summary>

    1. detachした深度マップから，物体領域のうちカメラ座標の原点に最も近い点の座標を取得．
    2. これを物体深度の代表点とし，深度-高さの幾何制約式から最尤推定により各フレームのスケールを計算．
    3. フレームごとにカメラ高さを計算．
    4. 3.で計算したカメラ高さをdetachし，2.で計算したスケールによりスケーリング．全フレームに渡り平均を取ってカメラ高さの教師とする．
    5. 3.で計算したカメラ高さの教師として，1エポック前に4.で計算したカメラ高さを利用
  </details>
- <details><summary>修正する必要があるかもしれない点</summary>

    - **monodepth2では，softmax関数により値域が(0,1)に正規化された逆深度を出力し，逆深度の逆数（＝深度）の値域が(min-depth, max-depth)になるようスケールしたものを最終的な深度マップとしている．min-depth, max-depthは予め定義しておく．スケールフリーな手法では評価時にmedian scalingされるので問題ないが，直接実スケールの深度マップを出力する場合は，問題に成りうる．**
        - max-depthよりも遠い地点が推定できなくなる
        - [Insta-DM](https://sites.google.com/site/seokjucv/home/instadm)では出力層にsoftplus関数を用いて直接的に値を推定している

    - 現在はカメラ高さの初期値として1エポック目の学習で計算したカメラ高さの値を計算しているが，この値が真値からかなり乖離している．1エポック目後に初期化するのなら，**学習済みmonodepth2で計算したより真値に近い値を初期値**として採用しても良い気がしてきた．
        - もちろん学習の手間がないという意味では前者の方が良い．
        - 初期値がボトルネックになっているなら変更するくらいの心持ちでいとく．
  </details>

学習が進むに連れて，道路領域が過学習(?)し始める．全てのフレームで起こっている現象ではないので，①どのようなフレームでこれが生じているのか，②なぜそれが起こるのかを特定する．
この現象は [VADepth](https://github.com/xjixzz/vadepth-net)(RA-L2022) でも起こっていた．

<table>
    <tr>
        <td>10 epoch</td>
        <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/fe31894d-56c5-426e-bf8b-2c56404fc64f"></td>
    </tr>
    <tr>
        <td>15 epoch</td>
        <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/2c06800f-2536-4f7c-b2b3-3822d141ebda"></td>
    </tr>
    <tr>
        <td>20 epoch</td>
        <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/40aeb8c1-cd4d-4bb4-976b-10906d578d0e"></td>
    </tr>
    <tr>
        <td>20 epoch(VADepth)</td>
        <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/bd810e78-3e14-43a9-a1ca-c5b518135d39"></td>
    </tr>
</table>


## _KR-1c: 道路平面の傾きを理解した推定が可能になるように，損失を加える or Data Augmentation手法を考える_

学習済みのオリジナルのmonodepth2が出力する深度マップから道路領域の法線ベクトルを計算し，そこから計算したhorizonとGTのhorizonを見比べると，特にGTの傾きが大きい時に差が大きくなる．つまりmonodepth2は**道路領域の法線ベクトルはほぼy軸方向であると推定しがち**．
    <img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/cb2c1749-a09b-4251-b6d8-b88f14bc285e" width=40%>
    これが原因となり，計算するカメラ高さの値が不正確になっている？


-----------------------------------------------------------------------------------------
# 06-26
-----------------------------------------------------------------------------------------
### 前回の総括

- 物体領域の深度を平均してから深度-高さ幾何制約式による損失を計算するのではなく，物体領域の各ピクセルの深度を用いて幾何制約式を計算し，物体領域深度が無限に飛ぶ問題を解決．
    - 定量評価ではカメラ高さのGTラベルを使って学習したMonodepth2と同程度 or 少し上回る

- 上記解決策の定性的な問題点は，
    1. **物体領域が平面的になる**
    2. **各物体で独立したスケールに関する教師を与えているため，見切れ物体などでは現実よりも遠くに物体があるような非常に不正確な教師信号を与えてしまう**

- 幾何制約式による雑な損失に掛ける係数を学習が進むに連れて徐々に小さくし，カメラ高さの一貫性に対する損失に掛ける係数を大きくして学習することで，次第に上記の問題は解決していく．

- 残る問題は，
    1. **カメラ高さが徐々に高くなっていく**
        - 学習が進み見切れ物体が徐々にカメラ側に近づく．その結果スケール係数が大きくなるように計算されてしまうことが大きな原因と考えられる．
    2. **道路平面が不安定になり，遠くにあるはずの道路が逆に近くなるような推定がなされる場合がある**
        - カメラ高さが不安定なことが原因の一つ（カメラ高さを一定にした場合，少しこの傾向がマシ）

- まずやるべきこととしては，
    1. 見切れ物体等の「異常な物体」を取り除く手法を確立し，定量・定性評価で既存研究とcompetitiveな結果を出せるようにする
    2. ↑これを解決してもなお道路平面が不安定になる場合，原因の特定と解決策を考案．

### 今週やることの概要

- [x] 異常な物体を取り除く手法を確立
- [x] カメラ高さの一貫性の損失のみを設定した学習の後半で，道路領域が平面的でない推定を行い始める原因を特定する
- [ ] 道路平面の傾きを理解した推定が可能になるように，損失を加える or Data Augmentation手法を考える

    - 学習済みのオリジナルのmonodepth2が出力する深度マップから道路領域の法線ベクトルを計算し，そこから計算したhorizonとGTのhorizonを見比べると，特にGTの傾きが大きい時に差が大きくなる．つまりmonodepth2は**道路領域の法線ベクトルはほぼy軸方向であると推定しがち**．
        <img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/cb2c1749-a09b-4251-b6d8-b88f14bc285e" width=40%>
        - これが原因となり，計算するカメラ高さの値が不正確になっているのかも．


# O-1: 幾何制約式による損失 + カメラ高さの一貫性による損失 + 異常物体の排除 で既存の教師あり手法とcompetitiveな結果を出す

## _KR-1a: 異常物体の排除手法を考案する_

<details><summary>以前提案した，cross-ratio + RANSACによる異常物体特定手法</summary>

- ここで「**異常物体**」とは，「画像外に見切れた物体，遮蔽を含む物体，クラス分類・Segmentation領域を誤った物体」を指す．
- 学習前に1度だけ行う，$D_1, D_2$ で高さ-深度の幾何的制約を計算する際に使用すべきでない物体の特定法．

- **手続き**
    1. １組の物体ペアについて，cross ratioの式に基づき消失点を計算
        - 両物体の高さは，各所属カテゴリーの平均高さを使用
        - 物体の高さを定義する部分として，2D BBoxの中間を使用（＝道路平面に対するrollを無視）
            <img src="https://user-images.githubusercontent.com/54442538/232188731-a17dc92c-525f-4e8d-9cb4-64ab40ac287b.png" width=400px>

    2. 複数ペアから計算した消失点を通る直線をRANSACにより求める
        - 物体ペアは複数フレームに渡って作成し，異なるフレーム内に登場する同一物体は別の物体とみなす
            <img src="https://user-images.githubusercontent.com/54442538/232190129-1f2379b4-0457-477d-a568-98079df925d0.png" width=600px>

    3. RANSACにより外れ値とみなされた消失点を生成した物体ペアのうちのいずれか，または両方は異常物体であると考えられる．あるフレームのある舞台について，消失点の計算に使用された回数のうち，そのペアがRANSACにより外れ値と判断された割合が閾値を超えた場合，それを異常な物体として決定する．
</details>

~~以前提案した異常物体特定手法では，定性的に見た感じ十分に異常な物体を取り除けていないようだった．
まずはこれがどれだけ学習の安定性につながるかを見てみる．~~

また学習中には，pseudo camera height と 道路セグメントが得られている．これらを利用すれば「**地面に接地しているある物体が，この位置でこのピクセル高さであれば，実際の高さは〇〇m**」というのを計算できる．これにより，動的に異常物体を特定し損失計算・カメラ高さ計算に使用しないよう調整していくことができる．
こちらの手法についてもちゃんと定式化した後，試してみる．並行してより良い方法がないかを考える．

### 結果
一旦，[Single View Scene Scale Estimation using Scale Field](https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_Single_View_Scene_Scale_Estimation_Using_Scale_Field_CVPR_2023_paper.pdf)(CVPR2023) に記載されている数式（Eq(2)）を参考に，異常物体を取り除くためのスクリプトを作成した．

Eq(2)では「**画像平面上での物体下端からhorizonまでの距離と物体のピクセル高さの比は，カメラ高さと物体の実高さの比と近似できる**」ことを主張している．

**手続き：**  カメラ高さを固定した時，上の近似から計算される物体の実高さの推定値と，その物体カテゴリーの高さ（事前知識）の差を計算．差が閾値よりも大きくなれば異常な物体とみなして取り除く．

カメラ高さを1.8m（真値は1.65mだが，学習中のモデルを使うとだいたいカメラ高さは1.8m程度であると計算される）に固定し，オリジナルの学習済みMonodepth2で推論したDepth mapからhorizonを計算し，異常値を取り除いてみると以下のような結果になった．

四角で囲ったセグメントは異常とみなされてもの．赤線はMonodepth2の出力する深度マップから計算したhorizon．白線はGTのhorizon
必要最低限は取り除けていそう．

https://github.com/kyotovision/kinoshita_genki/assets/54442538/75e6477f-b6c5-4304-9508-56110b8b1ba6

https://github.com/kyotovision/kinoshita_genki/assets/54442538/117d19b7-45ca-4db0-9f09-f043658d4283

推定するhorizonが間違えている場合，必要以上に物体を取り除こうとしてしまう．

https://github.com/kyotovision/kinoshita_genki/assets/54442538/ede041eb-2335-4b22-a664-482c7b53d4c0


現在は，学習時に計算したカメラ高さとhorizonを用いて，学習と並行して異常な物体を取り除く（＝幾何制約式による損失を設定しない＆カメラ高さのスケール計算時に用いない）ようにしたモデルを学習中．


## _KR-1b: カメラ高さの一貫性の損失を設定した学習の後半で，道路領域が平面的でない推定を行い始める原因を特定する_
- <details><summary>手法</summary>

    1. detachした深度マップから，物体領域のうちカメラ座標の原点に最も近い点の座標を取得．
    2. これを物体深度の代表点とし，深度-高さの幾何制約式から最尤推定により各フレームのスケールを計算．
    3. フレームごとにカメラ高さを計算．
    4. 3.で計算したカメラ高さをdetachし，2.で計算したスケールによりスケーリング．全フレームに渡り平均を取ってカメラ高さの教師とする．
    5. 3.で計算したカメラ高さの教師として，1エポック前に4.で計算したカメラ高さを利用
  </details>
- <details><summary>修正する必要があるかもしれない点</summary>

    - **monodepth2では，softmax関数により値域が(0,1)に正規化された逆深度を出力し，逆深度の逆数（＝深度）の値域が(min-depth, max-depth)になるようスケールしたものを最終的な深度マップとしている．min-depth, max-depthは予め定義しておく．スケールフリーな手法では評価時にmedian scalingされるので問題ないが，直接実スケールの深度マップを出力する場合は，問題に成りうる．**
        - max-depthよりも遠い地点が推定できなくなる
        - [Insta-DM](https://sites.google.com/site/seokjucv/home/instadm)では出力層にsoftplus関数を用いて直接的に値を推定している

    - 現在はカメラ高さの初期値として1エポック目の学習で計算したカメラ高さの値を計算しているが，この値が真値からかなり乖離している．1エポック目後に初期化するのなら，**学習済みmonodepth2で計算したより真値に近い値を初期値**として採用しても良い気がしてきた．
        - もちろん学習の手間がないという意味では前者の方が良い．
        - 初期値がボトルネックになっているなら変更するくらいの心持ちでいとく．
  </details>

学習が進むに連れて，道路領域が過学習(?)し始める．全てのフレームで起こっている現象ではないので，①どのようなフレームでこれが生じているのか，②なぜそれが起こるのかを特定する．
この現象は [VADepth](https://github.com/xjixzz/vadepth-net)(RA-L2022) でも起こっていた．

<table>
    <tr>
        <td>10 epoch</td>
        <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/fe31894d-56c5-426e-bf8b-2c56404fc64f"></td>
    </tr>
    <tr>
        <td>15 epoch</td>
        <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/2c06800f-2536-4f7c-b2b3-3822d141ebda"></td>
    </tr>
    <tr>
        <td>20 epoch</td>
        <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/40aeb8c1-cd4d-4bb4-976b-10906d578d0e"></td>
    </tr>
    <tr>
        <td>20 epoch(VADepth)</td>
        <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/bd810e78-3e14-43a9-a1ca-c5b518135d39"></td>
    </tr>
</table>

### 結果

おそらく，「道路はlow textureであることから，道路深度に対する最適解は複数存在→現実的ではないが局所解の不自然な道路深度を推定してしまう可能性→その際に推定される法線も不自然→法線の計算はdetachされており，この不自然な法線を基準にカメラ高さの制約が入る（ノルムが１の法線 $\cdot$ 道路領域の３次元点 ＝ カメラ高さ）→ゆえに道路領域の３次元点（＝深度）の最適解はより不自然なものになる」という負のループに入ってる?

解決策についてはこれから考えていく．

## _KR-1c: 道路平面の傾きを理解した推定が可能になるように，損失を加える or Data Augmentation手法を考える_

学習済みのオリジナルのmonodepth2が出力する深度マップから道路領域の法線ベクトルを計算し，そこから計算したhorizonとGTのhorizonを見比べると，特にGTの傾きが大きい時に差が大きくなる．つまりmonodepth2は**道路領域の法線ベクトルはほぼy軸方向であると推定しがち**．
    <img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/cb2c1749-a09b-4251-b6d8-b88f14bc285e" width=40%>
    これが原因となり，計算するカメラ高さの値が不正確になっている？


-----------------------------------------------------------------------------------------
# 07-03
-----------------------------------------------------------------------------------------
### 前回の総括

1. **カメラ高さが徐々に高くなっていく**
    - 学習が進み見切れ物体が徐々にカメラ側に近づく．その結果スケール係数が大きくなるように計算されてしまうことが大きな原因と考えられる．
2. **道路平面が不安定になり，遠くにあるはずの道路が逆に近くなるような推定がなされる場合がある**
    - カメラ高さが不安定なことが原因の一つ（カメラ高さを一定にした場合，少しこの傾向がマシ）

という問題を解決するために，[Single View Scene Scale Estimation using Scale Field](https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_Single_View_Scene_Scale_Estimation_Using_Scale_Field_CVPR_2023_paper.pdf)(CVPR2023) のEq(2)を参考に，異常物体を取り除くためのスクリプトを作成．

Eq(2)では「**画像平面上での物体下端からhorizonまでの距離と物体のピクセル高さの比は，カメラ高さと物体の実高さの比に等しいと近似できる**」ことを主張している．

**手続き：**  カメラ高さを固定した時，上の近似から計算される物体の実高さの推定値と，その物体カテゴリーの高さ（事前知識）の差を計算．差が閾値よりも大きくなれば異常な物体とみなして取り除く．

カメラ高さを1.8m（真値は1.65mだが，学習中のモデルを使うとだいたいカメラ高さは1.8m程度であると計算される）に固定し，オリジナルの学習済みMonodepth2で推論したDepth mapからhorizonを計算し，異常値を取り除いてみると以下のような結果になった．

<details><summary>結果</summary>

四角で囲ったセグメントは異常とみなされてもの．赤線はMonodepth2の出力する深度マップから計算したhorizon．白線はGTのhorizon
必要最低限は取り除けていそう．

https://github.com/kyotovision/kinoshita_genki/assets/54442538/75e6477f-b6c5-4304-9508-56110b8b1ba6

https://github.com/kyotovision/kinoshita_genki/assets/54442538/117d19b7-45ca-4db0-9f09-f043658d4283

推定するhorizonが間違えている場合，必要以上に物体を取り除こうとしてしまう．

https://github.com/kyotovision/kinoshita_genki/assets/54442538/ede041eb-2335-4b22-a664-482c7b53d4c0
</details>

また，カメラ高さの一貫性の損失を導入した際，学習が進むに連れて，道路領域が不連続な深度マップを出力し始める問題については，**「道路はlow textureであることから，道路深度に対する最適解は複数存在→現実的ではないが局所解の不自然な道路深度を推定してしまう可能性→その際に推定される法線も不自然→法線の計算はdetachされており，この不自然な法線を基準にカメラ高さの制約が入る（ノルムが１の法線 $\cdot$ 道路領域の３次元点 ＝ カメラ高さ）→ゆえに道路領域の３次元点（＝深度）の最適解はより不自然なものになる」という負のループに入ってる**という理由が考えられることを導いた．

<details><summary>道路領域深度が不連続的になる例</summary>

<table>
    <tr>
        <td>10 epoch</td>
        <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/fe31894d-56c5-426e-bf8b-2c56404fc64f"></td>
    </tr>
    <tr>
        <td>15 epoch</td>
        <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/2c06800f-2536-4f7c-b2b3-3822d141ebda"></td>
    </tr>
    <tr>
        <td>20 epoch</td>
        <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/40aeb8c1-cd4d-4bb4-976b-10906d578d0e"></td>
    </tr>
    <tr>
        <td>20 epoch(VADepth)</td>
        <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/bd810e78-3e14-43a9-a1ca-c5b518135d39"></td>
    </tr>
</table>
</details>


### 今週やることの概要

- [x] 異常な物体を取り除く手法を取り入れて学習した結果の考察
- [x] カメラ高さの一貫性の損失のみを設定した学習の後半で，道路領域が平面的でない推定を行い始める問題の解決法を考える
- [ ] 道路平面の傾きを理解した推定が可能になるように，損失を加える or Data Augmentation手法を考える

    - 学習済みのオリジナルのmonodepth2が出力する深度マップから道路領域の法線ベクトルを計算し，そこから計算したhorizonとGTのhorizonを見比べると，特にGTの傾きが大きい時に差が大きくなる．つまりmonodepth2は**道路領域の法線ベクトルはほぼy軸方向であると推定しがち**．
        <img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/cb2c1749-a09b-4251-b6d8-b88f14bc285e" width=40%>
        - これが原因となり，計算するカメラ高さの値が不正確になっているのかも．


# O-1: 幾何制約式による損失 + カメラ高さの一貫性による損失 + 異常物体の排除 で既存の教師あり手法とcompetitiveな結果を出す

## _KR-1a: 異常物体の排除手法を取り込んだ学習結果の考察_

[Single View Scene Scale Estimation using Scale Field](https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_Single_View_Scene_Scale_Estimation_Using_Scale_Field_CVPR_2023_paper.pdf)(CVPR2023) のEq.2 (「**画像平面上での物体下端からhorizonまでの距離と物体のピクセル高さの比は，カメラ高さと物体の実高さの比と近似できる**」を主張）を用いて，以下の手続きにより学習と並行して異常物体を取り除く（＝幾何制約式による損失を設定しない＆カメラ高さのスケール計算時に用いない）ようにしたモデルの学習結果について考察する．

**手続き：**  カメラ高さを固定した時，上の近似から計算される物体の実高さの推定値と，その物体カテゴリーの高さ（事前知識）の差を計算．差が閾値よりも大きくなれば異常な物体とみなして取り除く．


### メモ
- 下図の右下の車のように，車の最深部については「高さ」をうまく定義できているため現在の手法で取り除かれることはない．しかし，第一四分位数で代表した物体深度と物体の高さの事前知識の間で成り立つ幾何制約式からスケール係数を決定する際に，学習が進み形状が正確になるに連れて第一四分位数の物体領域深度は小さく（＝近く）なっていくため，計算されるスケール係数がどんどん大きくなり，その結果スケールしたカメラ高さも大きくなっていくという問題がある．

    <img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/9cc05462-b301-4c82-b8e5-0290a1b17cb8" width=50%>

### 結果
紫：異常な物体を含む，橙：異常な物体を除去

- カメラ高さ
    <img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/939eeceb-8242-485d-aa74-80a6b1d8e000" width=50%>

- 大きいほうが良い手法
    <img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/31752753-d0c1-45f5-bdf2-5b582c62c93e" width=80%>

- 小さいほうが良い手法
    <img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/f204f74a-c465-4d57-a630-c3ce034360f5" width=80%>

カメラ高さも安定し，精度も向上した．ただし道路平面が平面的でない推定を行い始める問題は解決していない．
一旦異常物体を取り除く手法は放置して，道路平面が正しく推定するよう対処することに注力する．それが解決し次第，より良い除去手法がないか模索する．

## _KR-1b: カメラ高さの一貫性の損失を設定した学習の後半で，道路領域が平面的でない推定を行い始める問題の解決策を考える_
- <details><summary>手法</summary>

    1. detachした深度マップから，物体領域のうちカメラ座標の原点に最も近い点の座標を取得．
    2. これを物体深度の代表点とし，深度-高さの幾何制約式から最尤推定により各フレームのスケールを計算．
    3. フレームごとにカメラ高さを計算．
    4. 3.で計算したカメラ高さをdetachし，2.で計算したスケールによりスケーリング．全フレームに渡り平均を取ってカメラ高さの教師とする．
    5. 3.で計算したカメラ高さの教師として，1エポック前に4.で計算したカメラ高さを利用
  </details>
- <details><summary>修正する必要があるかもしれない点</summary>

    - **monodepth2では，softmax関数により値域が(0,1)に正規化された逆深度を出力し，逆深度の逆数（＝深度）の値域が(min-depth, max-depth)になるようスケールしたものを最終的な深度マップとしている．min-depth, max-depthは予め定義しておく．スケールフリーな手法では評価時にmedian scalingされるので問題ないが，直接実スケールの深度マップを出力する場合は，問題に成りうる．**
        - max-depthよりも遠い地点が推定できなくなる
        - [Insta-DM](https://sites.google.com/site/seokjucv/home/instadm)では出力層にsoftplus関数を用いて直接的に値を推定している

    - 現在はカメラ高さの初期値として1エポック目の学習で計算したカメラ高さの値を計算しているが，この値が真値からかなり乖離している．1エポック目後に初期化するのなら，**学習済みmonodepth2で計算したより真値に近い値を初期値**として採用しても良い気がしてきた．
        - もちろん学習の手間がないという意味では前者の方が良い．
        - 初期値がボトルネックになっているなら変更するくらいの心持ちでいとく．
  </details>

おそらく，「道路はlow textureであることから，道路深度に対する最適解は複数存在→現実的ではないが局所解の不自然な道路深度を推定してしまう可能性→その際に推定される法線も不自然→法線の計算はdetachされており，この不自然な法線を基準にカメラ高さの制約が入る（ノルムが１の法線 $\cdot$ 道路領域の３次元点 ＝ カメラ高さ）→ゆえに道路領域の３次元点（＝深度）の最適解はより不自然なものになる」という負のループに入ってることにより，このような問題が起こると考えられる．

<table>
    <tr>
        <td>10 epoch</td>
        <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/fe31894d-56c5-426e-bf8b-2c56404fc64f"></td>
    </tr>
    <tr>
        <td>15 epoch</td>
        <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/2c06800f-2536-4f7c-b2b3-3822d141ebda"></td>
    </tr>
    <tr>
        <td>20 epoch</td>
        <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/40aeb8c1-cd4d-4bb4-976b-10906d578d0e"></td>
    </tr>
    <tr>
        <td>20 epoch(VADepth)</td>
        <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/bd810e78-3e14-43a9-a1ca-c5b518135d39"></td>
    </tr>
</table>

### メモ
- [DepthP+P: Metric Accurate Monocular Depth Estimation using Planar and Parallax](https://arxiv.org/pdf/2301.02092.pdf) とか [Deep Planar Parallax for Monocular Depth Estimation](https://arxiv.org/pdf/2301.03178.pdf) あたりの考え方が使える？

- ~「各ピクセルを３次元に飛ばした点の高さが，前エポックで計算したカメラ高さに等しくなるように損失を加える」→「道路領域の深度からdetachして計算したunscaledな道路平面の方程式を，前エポックで計算したカメラ高さでスケールし，その平面の方程式を道路領域深度の教師として与える」に変換してみる．~
    - 「法線(≒平面の方程式)を正しく推定できていれば正しい教師が与えられる」という仮定に変わりはなく，法線を正しく推定できている前提で話がすすむのが良くない．

### 結果

Monodepth2のautomaskにより，不自然な道路領域を推定した場合にreprojection errorによる修正が効かなくなってしまっているのが問題ではないかと考え，まずはautomaskを可視化してみた．結果は以下の通り，道路領域に黒くマスクがかかり，reprojection errorの逆伝播が伝わらないようになっていた．

<img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/f1a7aa8f-f4a5-480f-b7c6-7b033f22b764" width=30%>
<img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/fe1b4835-1305-4130-9355-824626777a42" width=30%>
<img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/6746c20b-8e45-4451-a0d1-b7262f62da8e" width=30%>

解決策として，道路領域にはmaskを掛けないようにした．ただし **画像領域の75%以上の領域にmaskがかかっている場合は，カメラが静止していると見なし道路領域にもmaskを掛けるようにする**．

現在学習中．ただ定性的に見てそれほどうまく行っているようには見えないので，そもそも悪い方向に学習が進まないように損失を加える等の工夫をした方が良い気がする．


## _KR-1c: 道路平面の傾きを理解した推定が可能になるように，損失を加える or Data Augmentation手法を考える_

学習済みのオリジナルのmonodepth2が出力する深度マップから道路領域の法線ベクトルを計算し，そこから計算したhorizonとGTのhorizonを見比べると，特にGTの傾きが大きい時に差が大きくなる．つまりmonodepth2は**道路領域の法線ベクトルはほぼy軸方向であると推定しがち**．
    <img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/cb2c1749-a09b-4251-b6d8-b88f14bc285e" width=40%>
    これが原因となり，計算するカメラ高さの値が不正確になっている？


-----------------------------------------------------------------------------------------
# 07-10
-----------------------------------------------------------------------------------------
### 前回の総括

1. **カメラ高さが徐々に高くなっていく**
    - 学習が進み見切れ物体が徐々にカメラ側に近づく．その結果スケール係数が大きくなるように計算されてしまうことが大きな原因と考えられる．
2. **道路平面が不安定になり，遠くにあるはずの道路が逆に近くなるような推定がなされる場合がある**
    - カメラ高さが不安定なことが原因の一つ（カメラ高さを一定にした場合，少しこの傾向がマシ）

という問題を解決するために，[Single View Scene Scale Estimation using Scale Field](https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_Single_View_Scene_Scale_Estimation_Using_Scale_Field_CVPR_2023_paper.pdf)(CVPR2023) のEq(2)を参考に，異常物体を取り除き，カメラ高さがより安定し精度向上に貢献した．

Eq(2)では「**画像平面上での物体下端からhorizonまでの距離と物体のピクセル高さの比は，カメラ高さと物体の実高さの比に等しいと近似できる**」ことを主張している．

**手続き：**  カメラ高さを固定した時，上の近似から計算される物体の実高さの推定値と，その物体カテゴリーの高さ（事前知識）の差を計算．差が閾値よりも大きくなれば異常な物体とみなして取り除く．

また道路領域の深度が不連続的になる問題の原因の１つは，Monodepth2のautomaskがかかることにより，不自然な道路領域深度を推定した際にreprojection errorによる修正がかからないことであると仮定し，カメラが静止していないとき道路領域のmaskを無効化するように修正した．現在学習中．

<details><summary>道路領域深度が不連続的になる例</summary>

<table>
    <tr>
        <td>10 epoch</td>
        <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/fe31894d-56c5-426e-bf8b-2c56404fc64f"></td>
    </tr>
    <tr>
        <td>15 epoch</td>
        <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/2c06800f-2536-4f7c-b2b3-3822d141ebda"></td>
    </tr>
    <tr>
        <td>20 epoch</td>
        <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/40aeb8c1-cd4d-4bb4-976b-10906d578d0e"></td>
    </tr>
    <tr>
        <td>20 epoch(VADepth)</td>
        <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/bd810e78-3e14-43a9-a1ca-c5b518135d39"></td>
    </tr>
</table>
</details>

### 残っているタスク
- [ ] カメラ高さの一貫性による制約を加えると道路平面が不連続になるシーンが発生する問題を解決
- [ ] 異常物体の除去手法を確立 
    - 現状とりあえず異常と判断した物体は閾値で切り捨ててるが，異常度合いを考慮して損失に組み込む/カメラ高さ計算時のスケール計算に利用することをまずはやってみたい．
- [ ] GTのカメラ高さ以外を用いた教師あり学習手法との比較


### 今週やることの概要

- カメラ高さの一貫性の損失のみを設定した学習の後半で，道路領域が平面的でない推定を行い始める問題に対して，
    - [x] 道路領域にautomaskがかからないようにしたときの結果を見る
    - [x] 関連論文を読みその他の解決法を考える

# O-1: 幾何制約式による損失 + カメラ高さの一貫性による損失 + 異常物体の排除 で既存の教師あり手法とcompetitiveな結果を出す

## _KR-1a: 異常物体の排除手法を取り込んだ学習結果の考察_

<details><summary>一旦塩漬け</summary>

[Single View Scene Scale Estimation using Scale Field](https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_Single_View_Scene_Scale_Estimation_Using_Scale_Field_CVPR_2023_paper.pdf)(CVPR2023) のEq.2 (「**画像平面上での物体下端からhorizonまでの距離と物体のピクセル高さの比は，カメラ高さと物体の実高さの比と近似できる**」を主張）を用いて，以下の手続きにより学習と並行して異常物体を取り除く（＝幾何制約式による損失を設定しない＆カメラ高さのスケール計算時に用いない）ようにしたモデルの学習結果について考察する．

**手続き：**</br>
1. 推定した深度マップの道路領域からhorizonを計算

2. データセット全体を使って計算したカメラ高さを用いて上の近似から計算される物体の実高さの推定値と，その物体のカテゴリーの高さ（事前知識）の差が，ある閾値よりも大きくなれば異常な物体とみなす

3. 異常とみなした物体については，カメラ高さの計算時・粗い幾何制約式の損失計算時に利用しない

### メモ
幾何制約式を設定すべきでないものとカメラ高さを計算する際に取り除くべきものが完全に一致しておらず，前者のみが取り除かれている(異常物体の取り除き方の問題)

- 例えば下図の左下の車は，ピクセル高さは車両後方部で定義される実高さに対応しており，異常な物体とみなされない．また粗い幾何制約式を設けても問題ない．しかしカメラ高さの計算時には，物体領域の代表深度値として物体深度の第一四分位数を用いている．この第一四分位数の深度値をとる手前側の領域では幾何制約式は成立しない．

    <img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/04dc4946-7bbf-445c-a6f8-5ec18f7497b6" width=60%>

- おそらくこれが原因で，学習が進むに連れてカメラ高さが大きくなる（=GTから外れていく)
    - 上記の物体では，粗い幾何制約式により物体深度が物体後方付近の深度となるように学習する．粗い幾何制約式の損失係数が小さくなるに連れて，物体形状はカメラ座標原点側に突出して始める．結果スケールファクタが大きくなるように計算されてしまう．
</details>

## _KR-1b: 道路領域にautomaskを掛けないことで，カメラ高さの一貫性の損失を設定した学習で道路領域が不連続な推定を行い始める問題が解決できているか確認_
- <details><summary>手法</summary>

    1. detachした深度マップから，物体領域のうちカメラ座標の原点に最も近い点の座標を取得．
    2. これを物体深度の代表点とし，深度-高さの幾何制約式から最尤推定により各フレームのスケールを計算．
    3. フレームごとにカメラ高さを計算．
    4. 3.で計算したカメラ高さをdetachし，2.で計算したスケールによりスケーリング．全フレームに渡り平均を取ってカメラ高さの教師とする．
    5. 3.で計算したカメラ高さの教師として，1エポック前に4.で計算したカメラ高さを利用
  </details>

- <details><summary>問題の原因</summary>

    おそらく，「道路はlow textureであることから，道路深度に対する最適解は複数存在→現実的ではないが局所解の不自然な道路深度を推定してしまう可能性→その際に推定される法線も不自然→法線の計算はdetachされており，この不自然な法線を基準にカメラ高さの制約が入る（ノルムが１の法線 $\cdot$ 道路領域の３次元点 ＝ カメラ高さ）→ゆえに道路領域の３次元点（＝深度）の最適解はより不自然なものになる」という負のループに入ってることにより，このような問題が起こると考えられる．

    - もう少しざっくり言うと，「法線の計算をdetachしてから行っているが，一度法線が不自然な値を計算してしまうと良くない方向に学習が進み，それを修正してくれるはずのreprojection errorも機能しない．また法線の計算をdetachせずに行うと途中のpinverseの計算で誤差逆伝播がうまく行かない」というのが現状の問題．
  </details>

Monodepth2のautomaskにより，不自然な道路領域を推定した場合にreprojection errorによる修正が効かなくなってしまっているのが問題ではないかと考え，まずはautomaskを可視化してみた．結果は以下の通り，道路領域に黒くマスクがかかり，reprojection errorの逆伝播が伝わらないようになっていた．

<img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/f1a7aa8f-f4a5-480f-b7c6-7b033f22b764" width=30%>
<img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/fe1b4835-1305-4130-9355-824626777a42" width=30%>
<img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/6746c20b-8e45-4451-a0d1-b7262f62da8e" width=30%>

解決策として，道路領域にはmaskを掛けないようにした．ただし **画像領域の75%以上の領域にmaskがかかっている場合は，カメラが静止していると見なし道路領域にもmaskを掛けるようにする**．

### メモ
- [DepthP+P: Metric Accurate Monocular Depth Estimation using Planar and Parallax](https://arxiv.org/pdf/2301.02092.pdf) とか [Deep Planar Parallax for Monocular Depth Estimation](https://arxiv.org/pdf/2301.03178.pdf) あたりの考え方が使える？

- ~~「各ピクセルを３次元に飛ばした点の高さが，前エポックで計算したカメラ高さに等しくなるように損失を加える」→「道路領域の深度からdetachして計算したunscaledな道路平面の方程式を，前エポックで計算したカメラ高さでスケールし，その平面の方程式を道路領域深度の教師として与える」に変換してみる．~~
    - 「法線(≒平面の方程式)を正しく推定できていれば正しい教師が与えられる」という仮定に変わりはなく，法線を正しく推定できている前提で話がすすむのが良くない．

### 結果
多少精度は良くなったが，定性的な部分はまだまだ．

- カメラ高さの更新頻度を3エポックに1回にしたときのvalidationの結果
    - 紫： 道路領域のautomaskを無効化，緑： 今までどおり画像全体でautomaskを有効化
    - da＝大きいほどよい指標，de=小さいほどよい指標
    - 以下のグラフは25エポックまで学習を回している．オリジナルのmonodepth2では20エポックまで学習しており，横軸＝100Kの時がそれに対応している．
        - 学習が進むに連れてカメラ高さがどんどん真値から離れていく→精度が低くなっていくという問題がある．雑な解決法として「〇〇エポック目まででカメラ高さの更新をストップする」が挙げられる．もっと根本的な解決策を探りたい．

    <img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/5b352612-508a-4d4c-a8ce-732329e619ab" width=80%>

- カメラ高さの更新頻度を5エポックに1回にしたとき
    - 橙： 道路領域のautomaskを無効化，藍： 今までどおり画像全体でautomaskを有効化

    <img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/7d6105c6-9758-4994-b986-d314dba614cc" width=80%>

- 定性評価
    - 多くのシーンで，不自然な深度の道路領域がほんの少し小さくなっている傾向にはあるが，不十分．

    <img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/792bd6c2-43d1-4cbe-b8f4-2fc744b45720" width=50%>


上記の結果から，道路領域のautomaskを無効化するのは，道路領域の深度をなめらかにするためのコアにはならないことがわかった．問題解決の糸口を見つけるために，3D reconstruction や 教師あり/なしの深度推定の先行研究で，法線に対するLossを与えて精度を向上させる工夫を行っている手法を勉強した．とりあえず以下を読了．

- [Unsupervised learning of geometry from videos with edge-aware depth-normal consistency](https://arxiv.org/abs/1711.03665)(AAAI2018)
- [LEGO: Learning edge with geometry all at once by watching videos](https://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_LEGO_Learning_Edge_CVPR_2018_paper.pdf)(CVPR2018)
- [P3Depth: Monocular depth estimation with a piecewise planarity prior](https://www.trace.ethz.ch/publications/2022/p3depth/index.html)(CVPR2022)
- [Enforcing geometric constraints of virtual normal for depth prediction](https://openaccess.thecvf.com/content_ICCV_2019/papers/Yin_Enforcing_Geometric_Constraints_of_Virtual_Normal_for_Depth_Prediction_ICCV_2019_paper.pdf)(ICCV2019)
- [Adaptive surface normal constraint for depth estimation](https://openaccess.thecvf.com/content/ICCV2021/papers/Long_Adaptive_Surface_Normal_Constraint_for_Depth_Estimation_ICCV_2021_paper.pdf)(ICCV2021)



## _KR-1c: 道路平面の傾きを理解した推定が可能になるように，損失を加える or Data Augmentation手法を考える_

優先度：低

学習済みのオリジナルのmonodepth2が出力する深度マップから道路領域の法線ベクトルを計算し，そこから計算したhorizonとGTのhorizonを見比べると，特にGTの傾きが大きい時に差が大きくなる．つまりmonodepth2は**道路領域の法線ベクトルはほぼy軸方向であると推定しがち**．
    <img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/cb2c1749-a09b-4251-b6d8-b88f14bc285e" width=40%>
    これが原因となり，計算するカメラ高さの値が不正確になっている？


-----------------------------------------------------------------------------------------
# 07-17
-----------------------------------------------------------------------------------------
### 前回の総括

1. 道路領域のautomaskを無効化すると多少精度は向上したが，カメラ高さのlossを設定した時道路領域深度が不連続になることがある問題の解決にはつながらなかった
2. 上記問題の解決策の糸口を探るために，法線に制約を加える工夫を施した3D reconstruction, depth estimation系の先行研究をいくつか読んだ
3. [現状と課題](https://github.com/kyotovision/kinoshita_genki/issues/17)をまとめた

### 残っているタスク
- [x] カメラ高さの一貫性による制約を加えると道路平面が不連続になるシーンが発生する問題を解決
- [ ] 異常物体の除去手法を確立 
    - 現状とりあえず異常と判断した物体は閾値で切り捨ててるが，異常度合いを考慮して損失に組み込む/カメラ高さ計算時のスケール計算に利用することをまずはやってみたい．
- [ ] GTのカメラ高さ以外を用いた教師あり学習手法との比較


### 今週やることの概要

- [x] カメラ高さの一貫性の損失のみを設定した学習の後半で，道路領域が平面的でない推定を行い始める問題に対して，解決法を考える

# O-1: 幾何制約式による損失 + カメラ高さの一貫性による損失 + 異常物体の排除 で既存の教師あり手法とcompetitiveな結果を出す

## _KR-1a: 異常物体の排除手法を確立する_

<details><summary>一旦塩漬け</summary>

[Single View Scene Scale Estimation using Scale Field](https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_Single_View_Scene_Scale_Estimation_Using_Scale_Field_CVPR_2023_paper.pdf)(CVPR2023) のEq.2 (「**画像平面上での物体下端からhorizonまでの距離と物体のピクセル高さの比は，カメラ高さと物体の実高さの比と近似できる**」を主張）を用いて，以下の手続きにより学習と並行して異常物体を取り除く（＝幾何制約式による損失を設定しない＆カメラ高さのスケール計算時に用いない）ようにしたモデルの学習結果について考察する．

**手続き：**</br>
1. 推定した深度マップの道路領域からhorizonを計算

2. データセット全体を使って計算したカメラ高さを用いて上の近似から計算される物体の実高さの推定値と，その物体のカテゴリーの高さ（事前知識）の差が，ある閾値よりも大きくなれば異常な物体とみなす

4. 異常とみなした物体については，カメラ高さの計算時・粗い幾何制約式の損失計算時に利用しない

### メモ
幾何制約式を設定すべきでないものとカメラ高さを計算する際に取り除くべきものが完全に一致しておらず，前者のみが取り除かれている(異常物体の取り除き方の問題)

- 例えば下図の左下の車は，ピクセル高さは車両後方部で定義される実高さに対応しており，異常な物体とみなされない．また粗い幾何制約式を設けても問題ない．しかしカメラ高さの計算時には，物体領域の代表深度値として物体深度の第一四分位数を用いている．この第一四分位数の深度値をとる手前側の領域では幾何制約式は成立しない．

    <img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/04dc4946-7bbf-445c-a6f8-5ec18f7497b6" width=60%>

- おそらくこれが原因で，学習が進むに連れてカメラ高さが大きくなる（=GTから外れていく)
    - 上記の物体では，粗い幾何制約式により物体深度が物体後方付近の深度となるように学習する．粗い幾何制約式の損失係数が小さくなるに連れて，物体形状はカメラ座標原点側に突出して始める．結果スケールファクタが大きくなるように計算されてしまう．
</details>

## _KR-1b: カメラ高さの一貫性の損失を設定した学習で道路領域が不連続な推定を行い始める問題の解決策を考案_
- <details><summary>手法</summary>

    1. detachした深度マップから，物体領域のうちカメラ座標の原点に最も近い点の座標を取得．
    2. これを物体深度の代表点とし，深度-高さの幾何制約式から最尤推定により各フレームのスケールを計算．
    3. フレームごとにカメラ高さを計算．
    5. 3.で計算したカメラ高さをdetachし，2.で計算したスケールによりスケーリング．全フレームに渡り平均を取ってカメラ高さの教師とする．
    6. 3.で計算したカメラ高さの教師として，1エポック前に4.で計算したカメラ高さを利用
  </details>

- <details><summary>問題の原因</summary>

    おそらく，「道路はlow textureであることから，道路深度に対する最適解は複数存在→現実的ではないが局所解の不自然な道路深度を推定してしまう可能性→その際に推定される法線も不自然→法線の計算はdetachされており，この不自然な法線を基準にカメラ高さの制約が入る（ノルムが１の法線 $\cdot$ 道路領域の３次元点 ＝ カメラ高さ）→ゆえに道路領域の３次元点（＝深度）の最適解はより不自然なものになる」という負のループに入ってることにより，このような問題が起こると考えられる．

    - もう少しざっくり言うと，「法線の計算をdetachしてから行っているが，一度法線が不自然な値を計算してしまうと良くない方向に学習が進み，それを修正してくれるはずのreprojection errorも機能しない．また法線の計算をdetachせずに行うと途中のpinverseの計算で誤差逆伝播がうまく行かない」というのが現状の問題．
  </details>

- <details><summary>道路領域が不連続になる例</summary>

    <table>
        <tr>
            <td>10 epoch</td>
            <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/fe31894d-56c5-426e-bf8b-2c56404fc64f"></td>
        </tr>
        <tr>
            <td>15 epoch</td>
            <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/2c06800f-2536-4f7c-b2b3-3822d141ebda"></td>
        </tr>
        <tr>
            <td>20 epoch</td>
            <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/40aeb8c1-cd4d-4bb4-976b-10906d578d0e"></td>
        </tr>
        <tr>
            <td>20 epoch(VADepth)</td>
            <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/bd810e78-3e14-43a9-a1ca-c5b518135d39"></td>
        </tr>
    </table>
  </details>

- <details><summary>今までにやったこと</summary>

    - automaskの無効化→多少精度は向上したが，根本からの解決にはならなかった
  </details>
  
**道路領域深度からの法線計算をdetachしてからpinverseで計算しているのが問題→各ピクセルの近傍8点を用いた外積計算で法線を計算**

### 結果

**定量評価**
validationでの精度評価．紫がpinverse・緑が8近傍点の外積から法線を計算したとき．エポックが増えても精度が落ちることはなくなった．

<img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/0d608681-5cfc-4d88-a4c5-8d698ced29dc" width=80%>


**定性評価**
道路領域が自然な深度になるように改善されている．
<table>
    <tr>
        <td>入力</td>
        <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/27781359-af5f-4a51-bc07-56136cca469b"></td>
        <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/50ab53ad-04c6-48f0-a6ec-641cbc13d2f5"></td>
    </tr>
    <tr>
        <td>pinverse</td>
        <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/3f4c12b6-c676-45cc-96c0-be554bfde0f2"></td>
        <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/96711b4c-8a2d-47a3-a946-3866ca7d317b"></td>
    </tr>
    <tr>
        <td>8 neighbors</td>
        <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/d4e16a2a-5670-4b7a-9775-e73f62252d61"></td>
        <td><img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/48ad256e-6890-44b1-9af4-5655375e0b13"></td>
    </tr>
</table>


## _KR-1c: 道路平面の傾きを理解した推定が可能になるように，損失を加える or Data Augmentation手法を考える_

<details><summary>優先度が低いので後回し</summary>

学習済みのオリジナルのmonodepth2が出力する深度マップから道路領域の法線ベクトルを計算し，そこから計算したhorizonとGTのhorizonを見比べると，特にGTの傾きが大きい時に差が大きくなる．つまりmonodepth2は**道路領域の法線ベクトルはほぼy軸方向であると推定しがち**．
    <img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/cb2c1749-a09b-4251-b6d8-b88f14bc285e" width=40%>
    これが原因となり，計算するカメラ高さの値が不正確になっている？
</details>

# O-2: 大量のデータセットで提案手法によりpre-train(fully self-supervised)→KITTIでfine-tune

objective1が終わってから取り組む

<details><summary>詳細</summary>

## 主張
1. ラベルなしのデータセットだけでも，スケールをある程度理解した学習が可能
2. そのようなデータセットを大量に使用し事前学習することが，スケール未知のまま事前学習した場合に比べて精度が上回る
3. (できれば) KITTIを用いたfine-tuning後，事前学習に使用したデータセットでも精度良くメトリックな深度を推定できる
    - スケール未知のまま事前学習した手法では，スケールを1から学習する必要があり，その際に事前学習に使用したデータセットの情報をけっこう忘れる？反対に，事前学習したときのことをある程度覚えているせいで，事前学習データのドメインではテキトーなスケールで推論する？
    - [PackNet](https://openaccess.thecvf.com/content_CVPR_2020/html/Guizilini_3D_Packing_for_Self-Supervised_Monocular_Depth_Estimation_CVPR_2020_paper.html)(CVPR2020 oral. 速さのGTによりメトリックな深度を学習) では Cityscapes で事前学習した後にKITTIでfine-tuneしている．このとき全層freezeされず(?)，学習率も小さくするわけではない．これにより破滅的忘却が起こりやすくなっているのでは？最初からある程度のスケールを理解しているのであれば，破滅的忘却を防ぐ手段が増える．


## 求められる結果
大きい方が優れているメトリックを考えた際，
1. ```(カメラ高さやDepth mapのGTを用いた教師あり学習) (>)≒ (提案手法を用いた教師なし学習)```

2. ```(スケール未知のまま事前学習＋カメラ高さやDepth mapのGTを用いた教師あり学習) < (提案手法を用いたスケールを理解した事前学習＋カメラ高さやDepth mapのGTを用いた教師あり学習)```

3. ```(「スケール未知のまま事前学習＋カメラ高さやDepth MapのGTのを用いた教師あり学習」した後，事前学習データセットでの推論) < (「提案手法を用いてスケールを理解した事前学習＋カメラ高さやDepth MapのGTのを用いた教師あり学習」した後，事前学習データセットでの推論)```

## とりあえずやるべきこと
- 提案手法により，KITTIで教師なし学習した場合の精度が，VADepthで学習した場合と比べてcompetitiveになる
    - そもそもこれの精度が悪ければ主張②も導けないので，最優先で取り組む．

- 深度推定タスクにおいて事前学習時にスケールを理解していることが，fine-tune後の推定精度にどれだけ効いてくるかを確認
    - 例えば，[PackNet](https://openaccess.thecvf.com/content_CVPR_2020/html/Guizilini_3D_Packing_for_Self-Supervised_Monocular_Depth_Estimation_CVPR_2020_paper.html)で「CSで速さのGTを用いて事前学習→KITTIで速さGTを用いてfine-tune」 vs 「CSでスケール未知のまま事前学習→KITTIで速さGTを用いてfine-tune」した時，前者の方が精度が良くなることを確認する
    - CS = CityScapesデータセット

## 事前学習周りで参考になりそうな論文
- [Geometric pretraining for monocular depth estimation](https://ieeexplore-ieee-org.kyoto-u.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=9196847)(ICRA2020)
- [Online depth learning against forgetting in monocular videos](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Online_Depth_Learning_Against_Forgetting_in_Monocular_Videos_CVPR_2020_paper.pdf)(CVPR2020) とかのonline refinement系の論文

## 問題になりそうな点
- 解像度の違い
    - おそらくfine-tuningするデータセットの解像度に各データセットの解像度を合わせておく必要がある．これは単純にクロップ＋resizeで良い？

- カメラ高さをどのように計算するのか
    - 現在はKITTIの各フレームで計算したカメラ高さを全フレームで平均するようにしているが，複数のデータセットで事前学習する場合どのようにカメラ高さを計算すれば良いのか
    - データセットごとに別々に計算するなら技術的に解決可能．
</details>

<details><summary>後回しでやること</summary>

# O-3: 学習の高速化

## _KR-3: DDPにより複数GPUでの学習を可能にする_

元々のmonodepth2の実装がシングルGPUでの学習しか想定されておらず，学習速度が遅い．DDPを実装することで高速化する．
余裕があれば進めていく感じで．

# 一応書き残しておきたいメモ
- 修正する必要があるかもしれない点

    - **monodepth2では，softmax関数により値域が(0,1)に正規化された逆深度を出力し，逆深度の逆数（＝深度）の値域が(min-depth, max-depth)になるようスケールしたものを最終的な深度マップとしている．min-depth, max-depthは予め定義しておく．スケールフリーな手法では評価時にmedian scalingされるので問題ないが，直接実スケールの深度マップを出力する場合は，問題に成りうる．**
        - max-depthよりも遠い地点が推定できなくなる
        - [Insta-DM](https://sites.google.com/site/seokjucv/home/instadm)では出力層にsoftplus関数を用いて直接的に値を推定している
</details>


-----------------------------------------------------------------------------------------
# 07-24
-----------------------------------------------------------------------------------------
### 前回の総括
- カメラ高さの一貫性の損失を設定すると，道路領域が平面的でない推定を行い始める問題に対して，法線の計算方法を微分可能な形になるよう変更することで解決した


### 残っているタスク
- [ ] 異常物体の除去手法を確立 
    - 現状とりあえず異常と判断した物体は閾値で切り捨ててるが，異常度合いを考慮して損失に組み込む/カメラ高さ計算時のスケール計算に利用することをまずはやってみたい．
- [ ] GTのカメラ高さ以外を用いた教師あり学習手法との比較


### 今週やることの概要
- [ ] 異常物体の除去手法を確立する
- [ ] 研究のゴールを明確にする

# O-1: 幾何制約式による損失 + カメラ高さの一貫性による損失 + 異常物体の排除 で既存の教師あり手法とcompetitiveな結果を出す

## _KR-1a: 異常物体の排除手法を確立する_

- 現状の手法
    - [Single View Scene Scale Estimation using Scale Field](https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_Single_View_Scene_Scale_Estimation_Using_Scale_Field_CVPR_2023_paper.pdf)(CVPR2023) のEq.2 (「**画像平面上での物体下端からhorizonまでの距離と物体のピクセル高さの比は，カメラ高さと物体の実高さの比と近似できる**」を主張）を用いて，学習と並行して異常物体を取り除く（＝幾何制約式による損失を設定しない＆カメラ高さのスケール計算時に用いない）．

    - イメージとして，道路平面深度がわかっていれば，「画像上のある位置・ある大きさで写っている物体は現実的にはこのくらいのサイズ」というのが計算できるのでそれと事前知識の乖離を見て異常度を決定

    - <details><summary>具体的な手続き</summary>

        1. 推定した深度マップの道路領域からhorizonを計算

        2. データセット全体を使って計算したカメラ高さを用いて上の近似から計算される物体の実高さの推定値と，その物体のカテゴリーの高さ（事前知識）の差が，ある閾値よりも大きくなれば異常な物体とみなす

        3. 異常とみなした物体については，カメラ高さの計算時・粗い幾何制約式の損失計算時に利用しない
      </details>

- 問題点
    - 幾何制約式を設定すべきでないものとカメラ高さを計算する際に取り除くべきものが完全に一致しておらず，前者のみが取り除かれている(異常物体の取り除き方の問題)
    - 例えば下図の左下の車は，ピクセル高さは車両後方部で定義される実高さに対応しており，異常な物体とみなされない．また粗い幾何制約式を設けても問題ない．しかしカメラ高さの計算時には，物体領域の代表深度値として物体深度の第一四分位数を用いている．この第一四分位数の深度値をとる手前側の領域では幾何制約式は成立しない．

        <img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/04dc4946-7bbf-445c-a6f8-5ec18f7497b6" width=60%>

    - おそらくこれが原因で，学習が進むに連れてカメラ高さが大きくなる（=GTから外れていく)
        - 上記の物体では，粗い幾何制約式により物体深度が物体後方付近の深度となるように学習する．粗い幾何制約式の損失係数が小さくなるに連れて，物体形状はカメラ座標原点側に突出して始める．結果スケールファクタが大きくなるように計算されてしまう．

- その他の案（[https://github.com/kyotovision/kinoshita_genki/issues/17#issuecomment-1637333629](https://github.com/kyotovision/kinoshita_genki/issues/17#issuecomment-1637333629)で書いたもの）

    1. **物体の全貌が見えていた場合のDepth推定モデルをCarlaで学習**

        - 遮蔽されている物体なら「仮に物体の全貌が見えていた場合このようなDepth Mapになるだろう」，画像外に見切れた物体なら「視野角がもっと広かったらこのような深度マップが推定できただろう」というのを推定するモデル．
            - 見切れた物体のDepthはoutpaintingするイメージ
        - 物体の全貌が見えていたら，カメラ高さの計算時などに深度の代表値を取り出すべき点が決定できる（カメラ座標の原点の最近傍点）
            - 見切れた物体がz<0の領域まで入ってしまうとかなりめんどくさそう
        - Domain Gapを生みたくないのでセグメントマスクのみを入力とする．出力は物体全貌のセグメントマスク（確率マップ）と物体領域の深度マップ
            - 学習済みMonodepth2が出力する深度マップでは，物体形状があまり正確ではないのでここからGTを作って学習するのは微妙？
            - マスクのみから深度を生成するできるのか...？ マスク形状から姿勢方向はわかりそうだが，appearanceがないと見切れ物体がどのくらい見切れているかを判断するのが難しい．
        - マスクをどんどん大きくしていくことを考えるとdiffusionも使えそう

    2. **セグメントマスクが異常物体かどうか・どこのピクセルの深度を代表値として用いるべきかを特定するモデルをCarlaで学習**

        - I/O： セグメントマスク/[異常な物体・カメラ座標原点への最近傍点・後方点]のクラス（後ろ２つはどこの深度を代表値として用いるべきかのクラス）
            - <img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/2dd4f931-edf0-4732-aa88-e18de4ffb24f" width=80%>
            - もしくは[異常 or 正常]のクラス分類と深度を代表すべき点の確率マップを出力する回帰問題に落とし込む
        - シンプルで学習もうまく行きやすそう．今考えているメインの手法に取り込むのが簡単．面白さと新規性は謎．

### 結果
一旦Carlaで学習データを作っていろいろ試せるようにしている．土日の間でデータセットを生成できるようにしておく．


-----------------------------------------------------------------------------------------
# 07-31
-----------------------------------------------------------------------------------------
### 前回の総括
- カメラ高さの一貫性の損失を設定すると，道路領域が平面的でない推定を行い始める問題に対して，法線の計算方法を微分可能な形になるよう変更することで解決した


### 残っているタスク
- [ ] 異常物体の除去手法を確立 
- [ ] GTのカメラ高さ以外を用いた教師あり学習手法との比較
    - [DynaDepth](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136980140.pdf)(ECCV2022)
    - [Do what you can, with what you have](https://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Swami_Do_What_You_Can_With_What_You_Have_Scale-Aware_and_CVPRW_2022_paper.pdf)(CVPR2022Workshop)
    - [ZoeDepth](https://github.com/isl-org/ZoeDepth)(arxiv preprint)
    - [ZeroDepth](https://arxiv.org/pdf/2306.17253.pdf)(arxiv preprint)


### 今週やることの概要
- [ ] 異常物体の除去手法を確立する
- [ ] 研究のゴールを明確にする

# O-1: 幾何制約式による損失 + カメラ高さの一貫性による損失 + 異常物体の排除 で既存の教師あり手法とcompetitiveな結果を出す

## _KR-1a: 異常物体の排除手法を確立する_

- <details><summary>現状の手法</summary>

    - [Single View Scene Scale Estimation using Scale Field](https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_Single_View_Scene_Scale_Estimation_Using_Scale_Field_CVPR_2023_paper.pdf)(CVPR2023) のEq.2 (「**画像平面上での物体下端からhorizonまでの距離と物体のピクセル高さの比は，カメラ高さと物体の実高さの比と近似できる**」を主張）を用いて，学習と並行して異常物体を取り除く（＝幾何制約式による損失を設定しない＆カメラ高さのスケール計算時に用いない）．

    - イメージとして，道路平面深度がわかっていれば，「画像上のある位置・ある大きさで写っている物体は現実的にはこのくらいのサイズ」というのが計算できるのでそれと事前知識の乖離を見て異常度を決定

    - <details><summary>具体的な手続き</summary>

        1. 推定した深度マップの道路領域からhorizonを計算

        2. データセット全体を使って計算したカメラ高さを用いて上の近似から計算される物体の実高さの推定値と，その物体のカテゴリーの高さ（事前知識）の差が，ある閾値よりも大きくなれば異常な物体とみなす

        3. 異常とみなした物体については，カメラ高さの計算時・粗い幾何制約式の損失計算時に利用しない
      </details>
  </details>

- <details><summary>問題点</summary>

    - 幾何制約式を設定すべきでないものとカメラ高さを計算する際に取り除くべきものが完全に一致しておらず，前者のみが取り除かれている(異常物体の取り除き方の問題)
    - 例えば下図の左下の車は，ピクセル高さは車両後方部で定義される実高さに対応しており，異常な物体とみなされない．また粗い幾何制約式を設けても問題ない．しかしカメラ高さの計算時には，物体領域の代表深度値として物体深度の第一四分位数を用いている．この第一四分位数の深度値をとる手前側の領域では幾何制約式は成立しない．

        <img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/04dc4946-7bbf-445c-a6f8-5ec18f7497b6" width=60%>

    - おそらくこれが原因で，学習が進むに連れてカメラ高さが大きくなる（=GTから外れていく)
        - 上記の物体では，粗い幾何制約式により物体深度が物体後方付近の深度となるように学習する．粗い幾何制約式の損失係数が小さくなるに連れて，物体形状はカメラ座標原点側に突出して始める．結果スケールファクタが大きくなるように計算されてしまう．
  </details>

- その他の案（[https://github.com/kyotovision/kinoshita_genki/issues/17#issuecomment-1637333629](https://github.com/kyotovision/kinoshita_genki/issues/17#issuecomment-1637333629)で書いたもの）

    1. **物体の全貌が見えていた場合のDepth推定モデルをCarlaで学習**

        - 遮蔽されている物体なら「仮に物体の全貌が見えていた場合このようなDepth Mapになるだろう」，画像外に見切れた物体なら「視野角がもっと広かったらこのような深度マップが推定できただろう」というのを推定するモデル．
            - 見切れた物体のDepthはoutpaintingするイメージ
        - 物体の全貌が見えていたら，カメラ高さの計算時などに深度の代表値を取り出すべき点が決定できる（カメラ座標の原点の最近傍点）
            - 見切れた物体がz<0の領域まで入ってしまうとかなりめんどくさそう
        - Domain Gapを生みたくないのでセグメントマスクのみを入力とする．出力は物体全貌のセグメントマスク（確率マップ）と物体領域の深度マップ
            - 学習済みMonodepth2が出力する深度マップでは，物体形状があまり正確ではないのでここからGTを作って学習するのは微妙？
            - マスクのみから深度を生成するできるのか...？ マスク形状から姿勢方向はわかりそうだが，appearanceがないと見切れ物体がどのくらい見切れているかを判断するのが難しい．
        - マスクをどんどん大きくしていくことを考えるとdiffusionも使えそう

    2. **セグメントマスクが異常物体かどうか・どこのピクセルの深度を代表値として用いるべきかを特定するモデルをCarlaで学習**

        - I/O： セグメントマスク/[異常な物体・カメラ座標原点への最近傍点・後方点]のクラス（後ろ２つはどこの深度を代表値として用いるべきかのクラス）
            - <img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/2dd4f931-edf0-4732-aa88-e18de4ffb24f" width=80%>
            - もしくは[異常 or 正常]のクラス分類と深度を代表すべき点の確率マップを出力する回帰問題に落とし込む
        - シンプルで学習もうまく行きやすそう．今考えているメインの手法に取り込むのが簡単．面白さと新規性は謎．

### 結果
- 一旦②の手法を試してみたくてCarlaでデータセットを作った．

- ②の手法では「2D BBoxのピクセル高さに対応する深度を持った点」を特定することが考えていたが，下図のインスタンスのように，そのような点を定義するのが困難な場合がままある．
    - <img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/b016b55b-4b9b-4627-9fd9-ec34eb50b5d9" width=50%>

- ちょっと寄り道して，以下の別の手法を考えた．

#### 案１：道路平面に置いた垂直な壁に，物体を正射影した得られるシルエットの高さでスケールファクタを計算

<img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/35d8fbe6-1845-4007-abbf-dc54a72747d7" width=70%>

- メリット:+1:
    - 「2D BBoxのピクセル高さに対応する深度を持つ点はどこか」というめんどくさい問題を考えなくて良くなる
    - 例えば車の上部だけが見えている場合，その物体のシルエットの最上部から道路平面までの高さをその物体の高さと定義することでスケールファクタの計算に使える．つまり今まで排除していた物体を有効活用できる

- デメリット:-1:
    - 依然として頭がちょっとだけ見えている車とかだとスケールファクタの計算に使うのは難しいのでこういった物体は取り除く必要がある．
    - セグメンテーションのクラス分類誤りにも気づけない
    - セグメント領域のエラーに対するロバスト性が2D BBoxの高さを使ってた時にくらべて劣る

#### 案２：車の3D BBoxの側面の壁は道路平面に垂直であると仮定をした上で，なんとか物体を3D BBoxで囲む
上記の条件を置くことで，3D BBoxを作成するのに未知の値は「どの方向に１つの壁を設置するか（＝車体方向）」という１つのスカラー値を求めれば良いだけになる．

- メリット:+1:
    - 案１とほぼ同じ．
    - 今まで物体のおおよその深度を掴むために用いていた粗い幾何制約による損失を，もっと正確かつ丁寧に設定できるようになる

- デメリット:-1:
    - 案１の上２つのデメリット同じ

「どの方向に１つの壁を設置するか（＝車体方向）」のスカラー値の求め方として以下の候補が考えられる．

1. 幾何的な制約などから求める
2. NNモジュールを作る
    1. 事前に学習したモデルを用いる
       1. 教師あり
       2. 教師なし
    2. monodepth2の学習と同時並行で学習させる
        1. lossを新たに付け加える
        2. lossは特に付け加えなくても微分可能な形にできればその他のlossがうまいこと良い解に導いてくれる

一番単純な「1. 幾何的な制約などから求める」ができそうかを簡単に調べてみた．そこまで深くはまだ考えられてない．

**物体領域の法線マップを道路平面上に射影しPCA→第１，２主成分に対応するベクトルがそれぞれ3D BBoxの壁に対応する(?)**
- 第３主成分に対応するベクトルを道路法線になるよう強制している
- 結果はイマイチだった．
- 自分のためのメモ用に結果だけ貼り付けておく．
    - <img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/f70c06a0-5f20-4ab7-9627-1a7f12eaf561" width=70%>
    - <img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/b0208763-1d6f-492c-a0f8-1b919d59672e" width=70%>
- 青の点群で表した，道路平面上に射影した法線マップの集合のうち**最も密度が大きい部分は車の側面or前面に対応しているように見える**．この最密部へのベクトルに垂直なベクトルが3D BBoxの壁の１つとして定義できるかも．
    - <img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/253a4334-215c-4a7a-9f7c-709fe7094ca2" width=70%>


## _KR-1b: 最近出たZero-shot転移でスケールを保持するモデルの論文を読む_
最近この手の論文が増えているっぽい．
[ZoeDepth](https://github.com/isl-org/ZoeDepth)(arxiv preprint), [ZeroDepth](https://arxiv.org/pdf/2306.17253.pdf)(arxiv preprint)あたりを読む．特にZeroDepthの方は大量の教師ありデータセットを使っている分めちゃくちゃ精度が良い．

## _KR-1b: 道路平面の傾きを理解した推定が可能になるように，損失を加える or Data Augmentation手法を考える_

<details><summary>優先度が低いので後回し</summary>

学習済みのオリジナルのmonodepth2が出力する深度マップから道路領域の法線ベクトルを計算し，そこから計算したhorizonとGTのhorizonを見比べると，特にGTの傾きが大きい時に差が大きくなる．つまりmonodepth2は**道路領域の法線ベクトルはほぼy軸方向であると推定しがち**．
    <img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/cb2c1749-a09b-4251-b6d8-b88f14bc285e" width=40%>
    これが原因となり，計算するカメラ高さの値が不正確になっている？
</details>



-----------------------------------------------------------------------------------------
# 08-07
-----------------------------------------------------------------------------------------
### 前回の総括
- Carlaでデータセットを作った（使うか微妙）
- 今まで異常として排除してきた物体をできるだけ有効活用してスケールファクタを計算する方法の案を出した

### 残っているタスク
- [ ] 異常物体の除去手法を確立 
- [ ] GTのカメラ高さ以外を用いた教師あり学習手法との比較
    - [DynaDepth](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136980140.pdf)(ECCV2022)
    - [Do what you can, with what you have](https://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Swami_Do_What_You_Can_With_What_You_Have_Scale-Aware_and_CVPRW_2022_paper.pdf)(CVPR2022Workshop)
    - [ZoeDepth](https://github.com/isl-org/ZoeDepth)(arxiv preprint)
    - [ZeroDepth](https://arxiv.org/pdf/2306.17253.pdf)(arxiv preprint)


### 今週やることの概要
- [x] 最新のzero-shot転移可能なmetric depthを出力するモデルの論文を読む
- [ ] 異常物体の除去手法を確立する

# O-1: 研究のゴールを考える

## _KR-1a: 最新のzero-shot転移可能なmetric depthを出力するモデルの論文を読む_

### [Metric3D](https://github.com/YvanYin/Metric3D) (ICCV2023)
<img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/db3c0f70-bcd9-4954-90f7-21496bba7161" width=90%>

#### 概要
- 800万枚という桁違いのデータセットで深度を教師あり学習．Indoor, outdoor両方のドメインでzero-shot転移可能．精度も非常に良い．
    - A100を48枚学習時に使用
- 内部パラメータによる深度の曖昧性を回避するため，任意の入力を共通の内部パラメータになるように調整し，その空間で深度を推定．その後内部パラメータが元に戻るよう再調整したものを出力とする．
    - 推論時にも内部パラメータは既知と仮定

#### 思ったこと
- 精度が良すぎるので自己教師のみでこれに勝つのは不可能（KITTIのLiDARで教師あり学習したモデルとcompetitiveな結果）．自己教師は「LiDAR不要であらゆる環境を網羅的に学習して汎化させれる」というメリットがあるが，悪天候などの特殊な環境でない限り十分汎化しているので，そのメリットを打ち出すことは難しい．自己教師の残された道は以下のように教師あり学習モデルを応用するしか無いのでは？

    - test-time refinement
        - 正しいスケールを保ちつつrefineしてる研究はなさそうだった．
        - 悪天候時にはセンサーデータが正常に取得できず，そのような環境のRealなデータセットの作成は不可→まだ悪天候には対応できていない？ただsyntheticなデータセットに悪天候環境を含めたら解決するだけかも．

    - knowledge distillation
        - 198Mパラメータ（LeResの2倍強）なのでちょっと重い

- 恐らくモデルは**物体のおおよその大きさを理解して**推定している．なのでモデルの中間表現を使えば3D Object Detectionなどの別タスクに応用できそう
    - CLIPと合わせればSegment Anythingのように学習時に存在しないクラスも3D Object Detectionできそう

### [ZeroDepth](https://arxiv.org/abs/2306.17253) (ICCV2023)
<img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/8f07e768-cda7-4299-8650-1a1ca3d18c74" width=90%>

#### 概要
- 300万枚のデータセットでDepth推定器を教師あり学習．Indoor, outdoor両方のドメインでzero-shot転移可能．学習枚数の違いのせいか，Metric3Dよりかは精度が低い
- 内部パラメータを元にembeddingを獲得し，内部パラメータによる深度の曖昧性を回避．

#### 思ったこと
- アーキテクチャはけっこうこだわって作ってるが，ViT + UNetのMetric3Dに負けているのが残念．結局学習枚数とパラメータ数が大事．

### [ZoeDepth](https://github.com/isl-org/ZoeDepth) (arxiv preprint)
Midasを教師データ使って実スケールにする論文．まだ読めてない．

## _KR-1b: Depth推定周りの最新の論文を読む_

### [SfM-TTR](https://openaccess.thecvf.com/content/CVPR2023/papers/Izquierdo_SfM-TTR_Using_Structure_From_Motion_for_Test-Time_Refinement_of_Single-View_CVPR_2023_paper.pdf) (CVPR2023)
- self-supervisedに学習したMonodepthを，推論時にCOLMAP or visual SLAMを使ってtest-time refineする手法．
- 本当にCOLMAPを組み合わせただけなのでCVPR2023に通っているのに驚き．

### [Robust Depth](https://kieran514.github.io/Robust-Depth-Project/) (ICCV2023)
- 単一のモデルで様々なドメインギャップに対応するよう自己教師のみで学習する方法を提案
- 学習時は，オリジナル画像とそれにFog等の加工を加えた画像を用意．加工画像を入力して推定したDepthからreconstruction lossを計算する際，加工画像ではなくオリジナル画像を使用．

### [SlowTV](https://github.com/jspenmar/slowtv_monodepth) (ICCV2023)
- 200万枚近くのデータセットで**自己教師あり学習**し，zero-shot転移可能にしたモデル．ただDPTやMiDASにしっかり負けている．
- 自己教師あり学習するためのデータセットとしてSlowTVというyoutubeチャンネルからダウンロードした動画を提案．森林や水中といった自然環境などを様々なジャンルの動画が含まれている．

# O-2: 幾何制約式による損失 + カメラ高さの一貫性による損失 + 異常物体の排除 で既存の教師あり手法とcompetitiveな結果を出す

## _KR-2a: 異常物体の排除手法を確立する_

- <details><summary>現状の手法</summary>

    - [Single View Scene Scale Estimation using Scale Field](https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_Single_View_Scene_Scale_Estimation_Using_Scale_Field_CVPR_2023_paper.pdf)(CVPR2023) のEq.2 (「**画像平面上での物体下端からhorizonまでの距離と物体のピクセル高さの比は，カメラ高さと物体の実高さの比と近似できる**」を主張）を用いて，学習と並行して異常物体を取り除く（＝幾何制約式による損失を設定しない＆カメラ高さのスケール計算時に用いない）．

    - イメージとして，道路平面深度がわかっていれば，「画像上のある位置・ある大きさで写っている物体は現実的にはこのくらいのサイズ」というのが計算できるのでそれと事前知識の乖離を見て異常度を決定

    - <details><summary>具体的な手続き</summary>

        1. 推定した深度マップの道路領域からhorizonを計算

        2. データセット全体を使って計算したカメラ高さを用いて上の近似から計算される物体の実高さの推定値と，その物体のカテゴリーの高さ（事前知識）の差が，ある閾値よりも大きくなれば異常な物体とみなす

        3. 異常とみなした物体については，カメラ高さの計算時・粗い幾何制約式の損失計算時に利用しない
      </details>
  </details>

- <details><summary>問題点</summary>

    - 幾何制約式を設定すべきでないものとカメラ高さを計算する際に取り除くべきものが完全に一致しておらず，前者のみが取り除かれている(異常物体の取り除き方の問題)
    - 例えば下図の左下の車は，ピクセル高さは車両後方部で定義される実高さに対応しており，異常な物体とみなされない．また粗い幾何制約式を設けても問題ない．しかしカメラ高さの計算時には，物体領域の代表深度値として物体深度の第一四分位数を用いている．この第一四分位数の深度値をとる手前側の領域では幾何制約式は成立しない．

        <img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/04dc4946-7bbf-445c-a6f8-5ec18f7497b6" width=60%>

    - おそらくこれが原因で，学習が進むに連れてカメラ高さが大きくなる（=GTから外れていく)
        - 上記の物体では，粗い幾何制約式により物体深度が物体後方付近の深度となるように学習する．粗い幾何制約式の損失係数が小さくなるに連れて，物体形状はカメラ座標原点側に突出して始める．結果スケールファクタが大きくなるように計算されてしまう．
  </details>

- Carlaを使う案（[https://github.com/kyotovision/kinoshita_genki/issues/17#issuecomment-1637333629](https://github.com/kyotovision/kinoshita_genki/issues/17#issuecomment-1637333629)で書いたもの）

    1. **物体の全貌が見えていた場合のDepth推定モデルをCarlaで学習**
        <details><summary>詳細</summary>

        - 遮蔽されている物体なら「仮に物体の全貌が見えていた場合このようなDepth Mapになるだろう」，画像外に見切れた物体なら「視野角がもっと広かったらこのような深度マップが推定できただろう」というのを推定するモデル．
            - 見切れた物体のDepthはoutpaintingするイメージ
        - 物体の全貌が見えていたら，カメラ高さの計算時などに深度の代表値を取り出すべき点が決定できる（カメラ座標の原点の最近傍点）
            - 見切れた物体がz<0の領域まで入ってしまうとかなりめんどくさそう
        - Domain Gapを生みたくないのでセグメントマスクのみを入力とする．出力は物体全貌のセグメントマスク（確率マップ）と物体領域の深度マップ
            - 学習済みMonodepth2が出力する深度マップでは，物体形状があまり正確ではないのでここからGTを作って学習するのは微妙？
            - マスクのみから深度を生成するできるのか...？ マスク形状から姿勢方向はわかりそうだが，appearanceがないと見切れ物体がどのくらい見切れているかを判断するのが難しい．
        - マスクをどんどん大きくしていくことを考えるとdiffusionも使えそう
        </details>

    2. **セグメントマスクが異常物体かどうか・どこのピクセルの深度を代表値として用いるべきかを特定するモデルをCarlaで学習**

        - I/O： セグメントマスク/[異常な物体・カメラ座標原点への最近傍点・後方点]のクラス（後ろ２つはどこの深度を代表値として用いるべきかのクラス）
            - <img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/2dd4f931-edf0-4732-aa88-e18de4ffb24f" width=80%>
            - もしくは[異常 or 正常]のクラス分類と深度を代表すべき点の確率マップを出力する回帰問題に落とし込む
        - シンプルで学習もうまく行きやすそう．今考えているメインの手法に取り込むのが簡単．面白さと新規性は謎．

        - 「2D BBoxのピクセル高さに対応する深度を持った点」を特定することが考えていたが，下図のように，そのような点を定義するのが困難な場合がままある．
            - <img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/b016b55b-4b9b-4627-9fd9-ec34eb50b5d9" width=50%>

- Carlaを使わずシンプルにやる案（異常物体の除去法ではなくスケールファクタの計算時に異常物体をできるだけ有効活用する方針)
    1. **道路平面に置いた垂直な壁に，物体を正射影して得られるシルエットの高さでスケールファクタを計算**

       <img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/35d8fbe6-1845-4007-abbf-dc54a72747d7" width=70%>

        - <details><summary>詳細</summary>

            - メリット:+1:
                - 「2D BBoxのピクセル高さに対応する深度を持つ点はどこか」というめんどくさい問題を考えなくて良くなる
                - 例えば車の上部だけが見えている場合，その物体のシルエットの最上部から道路平面までの高さをその物体の高さと定義することでスケールファクタの計算に使える．つまり今まで排除していた物体を有効活用できる

            - デメリット:-1:
                - 依然として頭がちょっとだけ見えている車とかだとスケールファクタの計算に使うのは難しいのでこういった物体は取り除く必要がある．
                - セグメンテーションのクラス分類誤りにも気づけない
                - セグメント領域のエラーに対するロバスト性が2D BBoxの高さを使ってた時にくらべて劣る
          </details>

    2. **車の3D BBoxの側面の壁は道路平面に垂直であると仮定をした上で，なんとか物体を3D BBoxで囲む**
        - 上記の条件を置くことで，3D BBoxを作成するのに未知の値は「どの方向に１つの壁を設置するか（＝車体方向）」という１つのスカラー値を求めれば良いだけになる．

        - <details><summary>詳細</summary>

            - メリット:+1:
                - 案１とほぼ同じ．
                - 今まで物体のおおよその深度を掴むために用いていた粗い幾何制約による損失を，もっと正確かつ丁寧に設定できるようになる

            - デメリット:-1:
                - 案１の上２つのデメリット同じ


            「どの方向に１つの壁を設置するか（＝車体方向）」のスカラー値の求め方として以下の候補が考えられる．

            1. 幾何的な制約などから求める
            2. NNモジュールを作る
                1. 事前に学習したモデルを用いる
                   1. 教師あり
                   2. 教師なし
                2. monodepth2の学習と同時並行で学習させる
                    1. lossを新たに付け加える
                    2. lossは特に付け加えなくても微分可能な形にできればその他のlossがうまいこと良い解に導いてくれる

            一番単純な「1. 幾何的な制約などから求める」ができそうかを簡単に調べてみた．そこまで深くはまだ考えられてない．

            **物体領域の法線マップを道路平面上に射影しPCA→第１，２主成分に対応するベクトルがそれぞれ3D BBoxの壁に対応する(?)**
            - 第３主成分に対応するベクトルを道路法線になるよう強制している
            - 結果はイマイチだった．
            - 自分のためのメモ用に結果だけ貼り付けておく．
                - <img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/f70c06a0-5f20-4ab7-9627-1a7f12eaf561" width=70%>
                - <img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/b0208763-1d6f-492c-a0f8-1b919d59672e" width=70%>
            - 青の点群で表した，道路平面上に射影した法線マップの集合のうち**最も密度が大きい部分は車の側面or前面に対応しているように見える**．この最密部へのベクトルに垂直なベクトルが3D BBoxの壁の１つとして定義できるかも．
                - <img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/253a4334-215c-4a7a-9f7c-709fe7094ca2" width=70%>
          </details>


### 経過
</br>

### 結果
</br>


-----------------------------------------------------------------------------------------
# 08-28
-----------------------------------------------------------------------------------------
### 前回の総括
- 最新のdepth推定周りの論文を読んだ
- 最新のzero-shot転移可能なmetric depthを推定するモデルの論文を読んだ

### 今週やることの概要
- 今まで2D BBoxの高さと事前知識の高さの比から決定していたスケールファクタを，シルエットの高さと事前知識の高さの比で決定するようにコードを修正
- 手法の問題点の解決策の考案・実装
    - 物体高さの事前知識としてKITTIの3D BBoxの高さの平均値を使用している（KITTIは訓練・テスト時にも使用しているが，本来これはわからないはずなので良くない）部分を修正

    - 推定した道路平面上に存在しない物体の除去法

    - その他問題になり得る点・改善できそうな点を探る

# O-1: 研究のゴールを考える

## KR-1a: 最新のzero-shot転移可能なmetric depthを出力するモデルの論文を読む

### [Metric3D](https://github.com/YvanYin/Metric3D) (ICCV2023)
<img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/db3c0f70-bcd9-4954-90f7-21496bba7161" width=90%>

#### 概要
- 800万枚という桁違いのデータセットで深度を教師あり学習．Indoor, outdoor両方のドメインでzero-shot転移可能．精度も非常に良い．

- 内部パラメータによる深度の曖昧性を回避するため，任意の入力を共通の内部パラメータになるように調整し，その空間で深度を推定．その後内部パラメータが元に戻るよう再調整したものを出力とする．

#### 思ったこと
- 恐らくモデルは**物体のおおよその大きさを理解して**推定している．なのでモデルの中間表現を使えば3D Object Detectionなどの別タスクに応用できそう
    - **CLIPと合わせれば学習時に存在しないクラスも3D Object Detectionできそう**

<details><summary>その他</summary>

### [ZeroDepth](https://arxiv.org/abs/2306.17253) (ICCV2023)
<img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/8f07e768-cda7-4299-8650-1a1ca3d18c74" width=90%>

#### 概要
- 300万枚のデータセットでDepth推定器を教師あり学習．Indoor, outdoor両方のドメインでzero-shot転移可能．学習枚数の違いのせいか，Metric3Dよりかは精度が低い
- 内部パラメータを元にembeddingを獲得し，内部パラメータによる深度の曖昧性を回避．

#### 思ったこと
- アーキテクチャはけっこうこだわって作ってるが，ViT + UNetのMetric3Dに負けているのが残念．結局学習枚数とパラメータ数が大事．

### [ZoeDepth](https://github.com/isl-org/ZoeDepth) (arxiv preprint)
Midasを教師データ使って実スケールにする論文．まだ読めてない．

## _KR-1b: Depth推定周りの最新の論文を読む_

### [SfM-TTR](https://openaccess.thecvf.com/content/CVPR2023/papers/Izquierdo_SfM-TTR_Using_Structure_From_Motion_for_Test-Time_Refinement_of_Single-View_CVPR_2023_paper.pdf) (CVPR2023)
- self-supervisedに学習したMonodepthを，推論時にCOLMAP or visual SLAMを使ってtest-time refineする手法．
- 本当にCOLMAPを組み合わせただけなのでCVPR2023に通っているのに驚き．

### [Robust Depth](https://kieran514.github.io/Robust-Depth-Project/) (ICCV2023)
- 単一のモデルで様々なドメインギャップに対応するよう自己教師のみで学習する方法を提案
- 学習時は，オリジナル画像とそれにFog等の加工を加えた画像を用意．加工画像を入力して推定したDepthからreconstruction lossを計算する際，加工画像ではなくオリジナル画像を使用．

### [SlowTV](https://github.com/jspenmar/slowtv_monodepth) (ICCV2023)
- 200万枚近くのデータセットで**自己教師あり学習**し，zero-shot転移可能にしたモデル．ただDPTやMiDASにしっかり負けている．
- 自己教師あり学習するためのデータセットとしてSlowTVというyoutubeチャンネルからダウンロードした動画を提案．森林や水中といった自然環境などを様々なジャンルの動画が含まれている．
</details>


# O-2: スケールファクタを物体シルエットの高さを用いて決定することで，これまで考えていた手法よりも精度が上回ることを確認する

## _KR-2a: スケールファクタの決定に物体シルエットの高さを用いるようコードを修正_

### 結果
2D BBoxの大きさと幾何制約式からスケールを決定していた時に比べて，カメラ高さがより真値に近づいた．まだ学習中だが，精度はほんの少し改善してそう．

## _KR-2b: 異常物体の排除手法を確立する_

異常物体の定義： 「道路平面に接地してない・クラス分類誤り・セグメント領域誤り」

### 経過

現状の手法は以下の通り．イメージ的には **「道路平面に存在していると仮定した物体が，画像上のこの位置にこの大きさで存在しているのは自然かどうか」**

1. 推定した深度マップの道路領域からhorizonを計算

2. データセット全体を使って計算したカメラ高さを用いて以下の近似から計算される物体の実高さの推定値と，その物体のカテゴリーの高さ（事前知識）の差が，ある閾値よりも大きくなれば異常な物体とみなす
   - [Single View Scene Scale Estimation using Scale Field](https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_Single_View_Scene_Scale_Estimation_Using_Scale_Field_CVPR_2023_paper.pdf)(CVPR2023) のEq.2 (「**画像平面上での物体下端からhorizonまでの距離と物体のピクセル高さの比は，カメラ高さと物体の実高さの比と近似できる**」を主張）

3. 異常とみなした物体については，カメラ高さの計算時・粗い幾何制約式の損失計算時に利用しない

上記の方法だと，物体上部だけ見えているが道路平面に接地している物体を取り除いてしまうというデメリットがある．これらの物体は本来スケールの決定に用いることができるので取り除くべきではない．

とりあえずは以下の方法を試してみる．同時にもっと良い方法がないかを考え続ける．

1. 推定したDepth mapを前エポックに計算したカメラ高さでスケール
2. Depth mapから構成したpoint cloudの物体上部に対応する点から道路平面へ鉛直方向に伸ばした直線の高さが，事前情報よりもかなり大き/小さかったら，その物体は接地していないとみなす

デメリットとしては，前エポックに計算したカメラ高さに依存しており，この値に最適化されてしまう可能性が高い．

### 結果
まだ取り組めてない．現状の手法でも割と良い感じに取り除けてはいるので優先度としては低め．


## _KR-2c: 物体高さの事前知識としてKITTIの3D BBoxの高さの平均値を使用しないような手法に修正_

高さの分布はデータセット間において差分はそれほどないが「この平均値が少し変わるだけで推定結果も全体的にズレる」というのはかなり欠点．またKITTIとCityscapesは他のデータセットに比べて車の高さ分布の平均値が小さい（[MonoCInIS](https://openaccess.thecvf.com/content/ICCV2021W/3DODI/papers/Heylen_MonoCInIS_Camera_Independent_Monocular_3D_Object_Detection_Using_Instance_Segmentation_ICCVW_2021_paper.pdf)(ICCV2021W)によるとKITTIとCityscapesは3D BBoxがtightでWaymoとかは緩い）という特徴があるので，他のデータセットと合わせて平均値を計算しようとすると確実に精度が変わってしまう．

### 経過
一番シンプルにやろうとすると，画像&車種ペアのデータセットに「高さ」の情報を追加し，appearanceから車の高さを推定するモデルを構築する．とかになる．

[X-Distill](https://www.bmvc2021-virtualconference.com/conference/papers/paper_0510.html)(BMVC2021)のように，高さ推定器の推定結果をpseudo labelとして，Depth推定器から生やした高さを推定するブランチを教師あり学習すると，物体の高さを理解した上で深度の推定を行うことが可能になるかも．

もうちょっと良いやり方があるかもしれないので，継続的に考え続ける．

### 結果
一旦[DVM-CAR](https://deepvisualmarketing.github.io/) datasetを使い，マスクした車画像から車の高さ推定器を学習させた．DVM-CARを使った理由は車のマスク画像と高さの情報の両方が既にアノテートされている唯一のデータセットだから．このデータセットには以下の問題があるため，将来的には「使用する車種データセットを変更し高さ情報をアノテート or 画像データをクロールして車種データセットを作る」までやりたい．

 - 899種類とそんなに多くない
 - マスク精度が結構悪い
 - 車内画像が含まれてたりする
- アノテートされた高さが明らかに間違っているものがある

</br>

アーキテクチャは，ImageNet22KでpretrainしたConvNeXt V2(base) + 2層のMLPを採用．
validationの精度としては高さが0.03mの誤差のオーダーまで精度が良くなっている（現在も学習中）．現状ではColorJitterでのみAugmentationしているが，車載動画の視点で得られる車のマスク画像にドメインを近づけるために，

- ズームアウト
- 画像の一部を隠してオクルージョンを増やす

といったAugmentationも加えていく．

-----------------------------------------------------------------------------------------
# 09-04
-----------------------------------------------------------------------------------------
### 前回の総括
- スケールファクタをシルエットの高さを用いて決定するようにコードを修正
- I/O: 車のマスク画像/高さ のモデルを[DVM-CAR](https://deepvisualmarketing.github.io/)を使って学習させた

### 今週やることの概要
- 高さ推定器の学習に，ズームアウト・オクルージョン領域の増加 によるAugmentationを加えて再学習
- メインモデルに対して，より改善できそうな点を探る
- 推定した道路平面上に存在しない物体の除去法の考案

# O-1: 研究のゴールを考える

## KR-1a: 最新のzero-shot転移可能なmetric depthを出力するモデルの論文を読む

### [Metric3D](https://github.com/YvanYin/Metric3D) (ICCV2023)
<img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/db3c0f70-bcd9-4954-90f7-21496bba7161" width=90%>

#### 概要
- 800万枚という桁違いのデータセットで深度を教師あり学習．Indoor, outdoor両方のドメインでzero-shot転移可能．精度も非常に良い．

- 内部パラメータによる深度の曖昧性を回避するため，任意の入力を共通の内部パラメータになるように調整し，その空間で深度を推定．その後内部パラメータが元に戻るよう再調整したものを出力とする．

#### 思ったこと
- 恐らくモデルは**物体のおおよその大きさを理解して**推定している．なのでモデルの中間表現を使えば3D Object Detectionなどの別タスクに応用できそう
    - **CLIPと合わせれば学習時に存在しないクラスも3D Object Detectionできそう**

<details><summary>その他</summary>

### [ZeroDepth](https://arxiv.org/abs/2306.17253) (ICCV2023)
<img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/8f07e768-cda7-4299-8650-1a1ca3d18c74" width=90%>

#### 概要
- 300万枚のデータセットでDepth推定器を教師あり学習．Indoor, outdoor両方のドメインでzero-shot転移可能．学習枚数の違いのせいか，Metric3Dよりかは精度が低い
- 内部パラメータを元にembeddingを獲得し，内部パラメータによる深度の曖昧性を回避．

#### 思ったこと
- アーキテクチャはけっこうこだわって作ってるが，ViT + UNetのMetric3Dに負けているのが残念．結局学習枚数とパラメータ数が大事．

### [ZoeDepth](https://github.com/isl-org/ZoeDepth) (arxiv preprint)
Midasを教師データ使って実スケールにする論文．まだ読めてない．

## _KR-1b: Depth推定周りの最新の論文を読む_

### [SfM-TTR](https://openaccess.thecvf.com/content/CVPR2023/papers/Izquierdo_SfM-TTR_Using_Structure_From_Motion_for_Test-Time_Refinement_of_Single-View_CVPR_2023_paper.pdf) (CVPR2023)
- self-supervisedに学習したMonodepthを，推論時にCOLMAP or visual SLAMを使ってtest-time refineする手法．
- 本当にCOLMAPを組み合わせただけなのでCVPR2023に通っているのに驚き．

### [Robust Depth](https://kieran514.github.io/Robust-Depth-Project/) (ICCV2023)
- 単一のモデルで様々なドメインギャップに対応するよう自己教師のみで学習する方法を提案
- 学習時は，オリジナル画像とそれにFog等の加工を加えた画像を用意．加工画像を入力して推定したDepthからreconstruction lossを計算する際，加工画像ではなくオリジナル画像を使用．

### [SlowTV](https://github.com/jspenmar/slowtv_monodepth) (ICCV2023)
- 200万枚近くのデータセットで**自己教師あり学習**し，zero-shot転移可能にしたモデル．ただDPTやMiDASにしっかり負けている．
- 自己教師あり学習するためのデータセットとしてSlowTVというyoutubeチャンネルからダウンロードした動画を提案．森林や水中といった自然環境などを様々なジャンルの動画が含まれている．
</details>


## _KR-2b: 異常物体の排除手法を確立する_

異常物体の定義： 「道路平面に接地してない・クラス分類誤り・セグメント領域誤り」

### 経過

現状の手法は以下の通り．イメージ的には **「道路平面に存在していると仮定した物体が，画像上のこの位置にこの大きさで存在しているのは自然かどうか」**

1. 推定した深度マップの道路領域からhorizonを計算

2. データセット全体を使って計算したカメラ高さを用いて以下の近似から計算される物体の実高さの推定値と，その物体のカテゴリーの高さ（事前知識）の差が，ある閾値よりも大きくなれば異常な物体とみなす
   - [Single View Scene Scale Estimation using Scale Field](https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_Single_View_Scene_Scale_Estimation_Using_Scale_Field_CVPR_2023_paper.pdf)(CVPR2023) のEq.2 (「**画像平面上での物体下端からhorizonまでの距離と物体のピクセル高さの比は，カメラ高さと物体の実高さの比と近似できる**」を主張）

3. 異常とみなした物体については，カメラ高さの計算時・粗い幾何制約式の損失計算時に利用しない

上記の方法だと，物体上部だけ見えているが道路平面に接地している物体を取り除いてしまうというデメリットがある．これらの物体は本来スケールの決定に用いることができるので取り除くべきではない．

とりあえずは以下の方法を試してみる．同時にもっと良い方法がないかを考え続ける．

1. 推定したDepth mapを前エポックに計算したカメラ高さでスケール
2. Depth mapから構成したpoint cloudの物体上部に対応する点から道路平面へ鉛直方向に伸ばした直線の高さが，事前情報よりもかなり大き/小さかったら，その物体は接地していないとみなす

デメリットとしては，前エポックに計算したカメラ高さに依存しており，この値に最適化されてしまう可能性が高い．



## _KR-2c: 物体高さの事前知識としてKITTIの3D BBoxの高さの平均値を使用しないような手法に修正_

高さの分布はデータセット間において差分はそれほどないが「この平均値が少し変わるだけで推定結果も全体的にズレる」というのはかなり欠点．またKITTIとCityscapesは他のデータセットに比べて車の高さ分布の平均値が小さい（[MonoCInIS](https://openaccess.thecvf.com/content/ICCV2021W/3DODI/papers/Heylen_MonoCInIS_Camera_Independent_Monocular_3D_Object_Detection_Using_Instance_Segmentation_ICCVW_2021_paper.pdf)(ICCV2021W)によるとKITTIとCityscapesは3D BBoxがtightでWaymoとかは緩い）という特徴があるので，他のデータセットと合わせて平均値を計算しようとすると確実に精度が変わってしまう．

### 経過
現状ではColorJitterでのみAugmentationしているが，車載動画の視点で得られる車のマスク画像にドメインを近づけるために，

- ズームアウト
- 画像の一部を隠してオクルージョンを増やす

といったAugmentationも加えていく．

### メモ
[X-Distill](https://www.bmvc2021-virtualconference.com/conference/papers/paper_0510.html)(BMVC2021)のように，高さ推定器の推定結果をpseudo labelとして，Depth推定器から生やした高さを推定するブランチを教師あり学習すると，物体の高さを理解した上で深度の推定を行うことが可能になるかも．


# メモ
- 以前考えてた異常物体除去手法のための案．何かに使えるかもしれないので一応おいとく

    - **物体の全貌が見えていた場合のDepth推定モデルをCarlaで学習**

        - 遮蔽されている物体なら「仮に物体の全貌が見えていた場合このようなDepth Mapになるだろう」，画像外に見切れた物体なら「視野角がもっと広かったらこのような深度マップが推定できただろう」というのを推定するモデル．
            - 見切れた物体のDepthはoutpaintingするイメージ
        - 物体の全貌が見えていたら，カメラ高さの計算時などに深度の代表値を取り出すべき点が決定できる（カメラ座標の原点の最近傍点）
            - 見切れた物体がz<0の領域まで入ってしまうとかなりめんどくさそう
        - Domain Gapを生みたくないのでセグメントマスクのみを入力とする．出力は物体全貌のセグメントマスク（確率マップ）と物体領域の深度マップ
            - 学習済みMonodepth2が出力する深度マップでは，物体形状があまり正確ではないのでここからGTを作って学習するのは微妙？
            - マスクのみから深度を生成するできるのか...？ マスク形状から姿勢方向はわかりそうだが，appearanceがないと見切れ物体がどのくらい見切れているかを判断するのが難しい．
        - マスクをどんどん大きくしていくことを考えるとdiffusionも使えそう

## _KR-1b: 道路平面の傾きを理解した推定が可能になるように，損失を加える or Data Augmentation手法を考える_

<details><summary>後回し</summary>

学習済みのオリジナルのmonodepth2が出力する深度マップから道路領域の法線ベクトルを計算し，そこから計算したhorizonとGTのhorizonを見比べると，特にGTの傾きが大きい時に差が大きくなる．つまりmonodepth2は**道路領域の法線ベクトルはほぼy軸方向であると推定しがち**．
    <img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/cb2c1749-a09b-4251-b6d8-b88f14bc285e" width=40%>
    これが原因となり，計算するカメラ高さの値が不正確になっている？
</details>

# O-2: 大量のデータセットで提案手法によりpre-train(fully self-supervised)→KITTIでfine-tune

objective1,2が終わってから取り組む

<details><summary>詳細</summary>

## 主張
1. ラベルなしのデータセットだけでも，スケールをある程度理解した学習が可能
2. そのようなデータセットを大量に使用し事前学習することが，スケール未知のまま事前学習した場合に比べて精度が上回る
3. (できれば) KITTIを用いたfine-tuning後，事前学習に使用したデータセットでも精度良くメトリックな深度を推定できる
    - スケール未知のまま事前学習した手法では，スケールを1から学習する必要があり，その際に事前学習に使用したデータセットの情報をけっこう忘れる？反対に，事前学習したときのことをある程度覚えているせいで，事前学習データのドメインではテキトーなスケールで推論する？
    - [PackNet](https://openaccess.thecvf.com/content_CVPR_2020/html/Guizilini_3D_Packing_for_Self-Supervised_Monocular_Depth_Estimation_CVPR_2020_paper.html)(CVPR2020 oral. 速さのGTによりメトリックな深度を学習) では Cityscapes で事前学習した後にKITTIでfine-tuneしている．このとき全層freezeされず(?)，学習率も小さくするわけではない．これにより破滅的忘却が起こりやすくなっているのでは？最初からある程度のスケールを理解しているのであれば，破滅的忘却を防ぐ手段が増える．


## 求められる結果
大きい方が優れているメトリックを考えた際，
1. ```(カメラ高さやDepth mapのGTを用いた教師あり学習) (>)≒ (提案手法を用いた教師なし学習)```

2. ```(スケール未知のまま事前学習＋カメラ高さやDepth mapのGTを用いた教師あり学習) < (提案手法を用いたスケールを理解した事前学習＋カメラ高さやDepth mapのGTを用いた教師あり学習)```

3. ```(「スケール未知のまま事前学習＋カメラ高さやDepth MapのGTのを用いた教師あり学習」した後，事前学習データセットでの推論) < (「提案手法を用いてスケールを理解した事前学習＋カメラ高さやDepth MapのGTのを用いた教師あり学習」した後，事前学習データセットでの推論)```

## とりあえずやるべきこと
- 提案手法により，KITTIで教師なし学習した場合の精度が，VADepthで学習した場合と比べてcompetitiveになる
    - そもそもこれの精度が悪ければ主張②も導けないので，最優先で取り組む．

- 深度推定タスクにおいて事前学習時にスケールを理解していることが，fine-tune後の推定精度にどれだけ効いてくるかを確認
    - 例えば，[PackNet](https://openaccess.thecvf.com/content_CVPR_2020/html/Guizilini_3D_Packing_for_Self-Supervised_Monocular_Depth_Estimation_CVPR_2020_paper.html)で「CSで速さのGTを用いて事前学習→KITTIで速さGTを用いてfine-tune」 vs 「CSでスケール未知のまま事前学習→KITTIで速さGTを用いてfine-tune」した時，前者の方が精度が良くなることを確認する
    - CS = CityScapesデータセット

## 事前学習周りで参考になりそうな論文
- [Geometric pretraining for monocular depth estimation](https://ieeexplore-ieee-org.kyoto-u.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=9196847)(ICRA2020)
- [Online depth learning against forgetting in monocular videos](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Online_Depth_Learning_Against_Forgetting_in_Monocular_Videos_CVPR_2020_paper.pdf)(CVPR2020) とかのonline refinement系の論文

## 問題になりそうな点
- 解像度の違い
    - おそらくfine-tuningするデータセットの解像度に各データセットの解像度を合わせておく必要がある．これは単純にクロップ＋resizeで良い？

- カメラ高さをどのように計算するのか
    - 現在はKITTIの各フレームで計算したカメラ高さを全フレームで平均するようにしているが，複数のデータセットで事前学習する場合どのようにカメラ高さを計算すれば良いのか
    - データセットごとに別々に計算するなら技術的に解決可能．
</details>

<details><summary>後回しでやること</summary>

# O-3: 学習の高速化

## _KR-3: DDPにより複数GPUでの学習を可能にする_

元々のmonodepth2の実装がシングルGPUでの学習しか想定されておらず，学習速度が遅い．DDPを実装することで高速化する．
余裕があれば進めていく感じで．

# 一応書き残しておきたいメモ
- 修正する必要があるかもしれない点

    - **monodepth2では，softmax関数により値域が(0,1)に正規化された逆深度を出力し，逆深度の逆数（＝深度）の値域が(min-depth, max-depth)になるようスケールしたものを最終的な深度マップとしている．min-depth, max-depthは予め定義しておく．スケールフリーな手法では評価時にmedian scalingされるので問題ないが，直接実スケールの深度マップを出力する場合は，問題に成りうる．**
        - max-depthよりも遠い地点が推定できなくなる
        - [Insta-DM](https://sites.google.com/site/seokjucv/home/instadm)では出力層にsoftplus関数を用いて直接的に値を推定している
</details>




-----------------------------------------------------------------------------------------
# 09-19
-----------------------------------------------------------------------------------------
### 前回の総括
- スケルトンを書いた

### 今週やることの概要
- [ ] 高さ推定器の学習に，ズームアウト・オクルージョン領域の増加 のAugmentationを加えて再学習
- [ ] 高さ推定器の精度をKITTI 3Dで評価
- 学習の安定化
    - [ ] カメラ高さをエポックごとに更新するように変更（今までは２エポックおきに更新）＆前エポックで計算したカメラ高さとの移動平均を取る
    - [ ] シルエットを使ってスケールファクタを計算すると，カメラ高さが大きくなりすぎてしまう原因を探る
    - [ ] camera height lossとrough geometric lossのバランスする係数を変えてみる

# O-1: 高さ推定器が高さの事前情報の固定値よりもKITTI 3D BBoxの高さ推定精度を上回ることを実証

## _KR-1: 高さ推定器の訓練にAugmentationを追加して訓練・KITTIで評価_
ズームアウト・オクルージョン領域増加のAugmentationを加えて再学習


# O-2: Training frameworkの学習安定化のための改善実験・考察

## _KR-2a: カメラ高さの更新を移動平均（モーメンタム）に変える_

## _KR-2b: 結局カメラ高さは何エポックおきに更新するのが良いのか実験_
現在は安定化のために2エポックおきに更新している（update->stay->stay->update)．更新時にモーメンタムを計算するなら

## _KR-2c: 異常物体の排除手法を確立する_

異常物体の定義： 「道路平面に接地してない・クラス分類誤り・セグメント領域誤り」

現状の手法は以下の通り．イメージ的には **「道路平面に存在していると仮定した物体が，画像上のこの位置にこの大きさで存在しているのは自然かどうか」**

<details><summary>手続き</summary>

1. 推定した深度マップの道路領域からhorizonを計算

2. データセット全体を使って計算したカメラ高さを用いて以下の近似から計算される物体の実高さの推定値と，その物体のカテゴリーの高さ（事前知識）の差が，ある閾値よりも大きくなれば異常な物体とみなす
   - [Single View Scene Scale Estimation using Scale Field](https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_Single_View_Scene_Scale_Estimation_Using_Scale_Field_CVPR_2023_paper.pdf)(CVPR2023) のEq.2 (「**画像平面上での物体下端からhorizonまでの距離と物体のピクセル高さの比は，カメラ高さと物体の実高さの比と近似できる**」を主張）

3. 異常とみなした物体については，カメラ高さの計算時・粗い幾何制約式の損失計算時に利用しない
</details>

上記の方法だと，物体上部だけ見えているが道路平面に接地している物体を取り除いてしまうというデメリットがある．これらの物体は本来スケールの決定に用いることができるので取り除くべきではない．

**※一旦塩漬け**

とりあえずは以下の方法を試してみる．同時にもっと良い方法がないかを考え続ける．

1. 推定したDepth mapを前エポックに計算したカメラ高さでスケール
2. Depth mapから構成したpoint cloudの物体上部に対応する点から道路平面へ鉛直方向に伸ばした直線の高さが，事前情報よりもかなり大き/小さかったら，その物体は接地していないとみなす

デメリットとしては，前エポックに計算したカメラ高さに依存しており，この値に最適化されてしまう可能性が高い．


# O-3: 他の先行研究とresolutionを揃えて精度で勝つ

今まではMonodepth2に則って1024x320の解像度で実験していたが，比較手法が割と640x192で行っていたので，そちらに合わせて実験する．
## _KR-3: とりあえず640x192の解像度で学習・評価_
...


# メモ
- 以前考えてた異常物体除去手法のための案．何かに使えるかもしれないので一応おいとく

    - **物体の全貌が見えていた場合のDepth推定モデルをCarlaで学習**

        - 遮蔽されている物体なら「仮に物体の全貌が見えていた場合このようなDepth Mapになるだろう」，画像外に見切れた物体なら「視野角がもっと広かったらこのような深度マップが推定できただろう」というのを推定するモデル．
            - 見切れた物体のDepthはoutpaintingするイメージ
        - 物体の全貌が見えていたら，カメラ高さの計算時などに深度の代表値を取り出すべき点が決定できる（カメラ座標の原点の最近傍点）
            - 見切れた物体がz<0の領域まで入ってしまうとかなりめんどくさそう
        - Domain Gapを生みたくないのでセグメントマスクのみを入力とする．出力は物体全貌のセグメントマスク（確率マップ）と物体領域の深度マップ
            - 学習済みMonodepth2が出力する深度マップでは，物体形状があまり正確ではないのでここからGTを作って学習するのは微妙？
            - マスクのみから深度を生成するできるのか...？ マスク形状から姿勢方向はわかりそうだが，appearanceがないと見切れ物体がどのくらい見切れているかを判断するのが難しい．
        - マスクをどんどん大きくしていくことを考えるとdiffusionも使えそう
- [X-Distill](https://www.bmvc2021-virtualconference.com/conference/papers/paper_0510.html)(BMVC2021)のように，高さ推定器の推定結果をpseudo labelとして，Depth推定器から生やした高さを推定するブランチを教師あり学習すると，物体の高さを理解した上で深度の推定を行うことが可能になるかも．

# 後回し

<details><summary>後回し</summary>

## _KR-2d: 道路平面の傾きを理解した推定が可能になるように，損失を加える or Data Augmentation手法を考える_


学習済みのオリジナルのmonodepth2が出力する深度マップから道路領域の法線ベクトルを計算し，そこから計算したhorizonとGTのhorizonを見比べると，特にGTの傾きが大きい時に差が大きくなる．つまりmonodepth2は**道路領域の法線ベクトルはほぼy軸方向であると推定しがち**．
    <img src="https://github.com/kyotovision/kinoshita_genki/assets/54442538/cb2c1749-a09b-4251-b6d8-b88f14bc285e" width=40%>
    これが原因となり，計算するカメラ高さの値が不正確になっている？

# O-3: 学習の高速化

## _KR-3: DDPにより複数GPUでの学習を可能にする_

元々のmonodepth2の実装がシングルGPUでの学習しか想定されておらず，学習速度が遅い．DDPを実装することで高速化する．
余裕があれば進めていく感じで．

# 一応書き残しておきたいメモ
- 修正する必要があるかもしれない点

    - **monodepth2では，softmax関数により値域が(0,1)に正規化された逆深度を出力し，逆深度の逆数（＝深度）の値域が(min-depth, max-depth)になるようスケールしたものを最終的な深度マップとしている．min-depth, max-depthは予め定義しておく．スケールフリーな手法では評価時にmedian scalingされるので問題ないが，直接実スケールの深度マップを出力する場合は，問題に成りうる．**
        - max-depthよりも遠い地点が推定できなくなる
        - [Insta-DM](https://sites.google.com/site/seokjucv/home/instadm)では出力層にsoftplus関数を用いて直接的に値を推定している
</details>